[{"content":" ðŸš§ Project Abundance ðŸš§ Researching next-gen blockchain architecture (as of 2026) to achieve ultimate scalability in permissionless setting and fully resolve Blockchain Trilemma. May or may not succeed but must be fun.\nGitHub repo Book Rust docs Zulip Chat ","date":"24 January 2026","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"24 January 2026","externalUrl":null,"permalink":"/authors/","section":"authors","summary":"","title":"authors","type":"authors"},{"content":"","date":"24 January 2026","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":"","title":"Blog","type":"blog"},{"content":"In the last update I introduced a basic RISC-V interpreter, but its performance was underwhelming, which was to be expected, but still was something that I\u0026rsquo;d like to improve. So since the last update I have implemented infrastructure for measuring performance, did a bunch of refactorings to hopefully make my work reusable by other projects in the future, and even implemented some performance improvements with a solid idea of what to explore next.\nIn the process of doing it I ended up parsing and generating Rust code in both build.rs and procedural macros, something I have never thought I\u0026rsquo;d end up doing, but let\u0026rsquo;s start from the beginning.\nBaseline performance # As mentioned last time, there was a basic CLI for building a contract. It was possible to run it, but there was no actual test that runs anything from a contract, and there was no nice programmatic way to even do that.\nThe first step was to extract contract building logic from CLI into a library in PR 497, which with some follow-up fixes in PR 499 and PR 509 allowed me to implement building, testing, and even benchmarking a basic contract in PR 510. It is a contract that does BLAKE3 hashing and Ed25519 signature verification.\nWhy those? I expect them to be used often and, as a reminder, I plan to not have custom domain-specific precompiles/instructions, so it is important for generic and popular algorithms to perform well, which it clearly didn\u0026rsquo;t at first:\nfile/parse-only time: [115.34 Âµs 117.01 Âµs 120.17 Âµs] thrpt: [8.3215 Kelem/s 8.5463 Kelem/s 8.6702 Kelem/s] file/parse-with-methods time: [109.65 Âµs 109.84 Âµs 109.99 Âµs] thrpt: [9.0920 Kelem/s 9.1046 Kelem/s 9.1198 Kelem/s] file/iterate-methods time: [78.396 ns 78.753 ns 79.699 ns] thrpt: [12.547 Melem/s 12.698 Melem/s 12.756 Melem/s] blake3_hash_chunk/native time: [769.04 ns 769.74 ns 770.31 ns] thrpt: [1.2380 GiB/s 1.2390 GiB/s 1.2401 GiB/s] blake3_hash_chunk/interpreter time: [229.14 Âµs 230.88 Âµs 232.84 Âµs] thrpt: [4.1942 MiB/s 4.2297 MiB/s 4.2618 MiB/s] ed25519_verify/native/valid time: [16.618 Âµs 16.641 Âµs 16.692 Âµs] thrpt: [59.908 Kelem/s 60.094 Kelem/s 60.176 Kelem/s] ed25519_verify/native/invalid time: [16.525 Âµs 16.554 Âµs 16.599 Âµs] thrpt: [60.245 Kelem/s 60.407 Kelem/s 60.516 Kelem/s] ed25519_verify/interpreter/valid time: [6.3310 ms 6.3605 ms 6.4322 ms] thrpt: [155.47 elem/s 157.22 elem/s 157.95 elem/s] ed25519_verify/interpreter/invalid time: [6.0230 ms 6.0487 ms 6.0823 ms] thrpt: [164.41 elem/s 165.32 elem/s 166.03 elem/s] This is slightly apples to oranges comparing AVX512 assembly BLAKE3 implementation on Zen 4 against a basic interpreter for RV64EM target, but that is how I measure a success: the ceiling is what the hardware is capable of, and we need to get close to that with whatever VM ends up being used. I do plan to add vector and crypto extensions in the future, though.\nSo the initial performance gap is around three orders of magnitude.\nA bit about general approach # When writing code, I\u0026rsquo;m trying to make it look \u0026ldquo;obviously correct\u0026rdquo; whenever possible, especially when complex concepts are involved. Of course, the domain which I\u0026rsquo;m working in is inherently complex, so it may not be that obvious in absolute terms, but I hope you at least get the idea of what I\u0026rsquo;m trying to say here.\nGoing back to the topic of RISC-V interpreter, my initial approach was to have an instruction decoder and interpreter separately. Decoder outputs an enum, and then the interpreter matches on decoded instructions and executes them. I even extracted M extension from RV64 base in PR 511 and used composition with a base instruction set to allow more flexibility and being able to test various permutations quickly, which allowed me to add B extension (including optional Zbc) in PR 512 shortly after that. Zbc extension was further optimized with platform-specific intrinsics on x86-64/aarch64/RV64 in PR 518.\nThat simplicity is great for reviewing and understanding code. For example, here is what decoding and execution of Zbs instructions looks like in the current version of the codebase:\n#[derive(Debug, Clone, Copy, PartialEq, Eq)] pub enum Rv64ZbsInstruction\u0026lt;Reg\u0026gt; { // Single-Bit Set Bset { rd: Reg, rs1: Reg, rs2: Reg }, Bseti { rd: Reg, rs1: Reg, shamt: u8 }, // Single-Bit Clear Bclr { rd: Reg, rs1: Reg, rs2: Reg }, Bclri { rd: Reg, rs1: Reg, shamt: u8 }, // Single-Bit Invert Binv { rd: Reg, rs1: Reg, rs2: Reg }, Binvi { rd: Reg, rs1: Reg, shamt: u8 }, // Single-Bit Extract Bext { rd: Reg, rs1: Reg, rs2: Reg }, Bexti { rd: Reg, rs1: Reg, shamt: u8 }, } impl\u0026lt;Reg\u0026gt; const Instruction for Rv64ZbsInstruction\u0026lt;Reg\u0026gt; where Reg: [ const ] Register\u0026lt;Type=u64\u0026gt;, { type Reg = Reg; #[inline(always)] fn try_decode(instruction: u32) -\u0026gt; Option\u0026lt;Self\u0026gt; { let opcode = (instruction \u0026amp; 0b111_1111) as u8; let rd_bits = ((instruction \u0026gt;\u0026gt; 7) \u0026amp; 0x1f) as u8; let funct3 = ((instruction \u0026gt;\u0026gt; 12) \u0026amp; 0b111) as u8; let rs1_bits = ((instruction \u0026gt;\u0026gt; 15) \u0026amp; 0x1f) as u8; let rs2_bits = ((instruction \u0026gt;\u0026gt; 20) \u0026amp; 0x1f) as u8; let shamt = ((instruction \u0026gt;\u0026gt; 20) \u0026amp; 0x3f) as u8; let funct7 = ((instruction \u0026gt;\u0026gt; 25) \u0026amp; 0b111_1111) as u8; let funct6 = ((instruction \u0026gt;\u0026gt; 26) \u0026amp; 0b11_1111) as u8; match opcode { // R-type instructions 0b0110011 =\u0026gt; { let rd = Reg::from_bits(rd_bits)?; let rs1 = Reg::from_bits(rs1_bits)?; let rs2 = Reg::from_bits(rs2_bits)?; match (funct3, funct7) { (0b001, 0b0010100) =\u0026gt; Some(Self::Bset { rd, rs1, rs2 }), (0b001, 0b0100100) =\u0026gt; Some(Self::Bclr { rd, rs1, rs2 }), (0b001, 0b0110100) =\u0026gt; Some(Self::Binv { rd, rs1, rs2 }), (0b101, 0b0100100) =\u0026gt; Some(Self::Bext { rd, rs1, rs2 }), _ =\u0026gt; None, } } // I-type instructions 0b0010011 =\u0026gt; { let rd = Reg::from_bits(rd_bits)?; let rs1 = Reg::from_bits(rs1_bits)?; match (funct3, funct6) { (0b001, 0b001010) =\u0026gt; Some(Self::Bseti { rd, rs1, shamt }), (0b001, 0b010010) =\u0026gt; Some(Self::Bclri { rd, rs1, shamt }), (0b001, 0b011010) =\u0026gt; Some(Self::Binvi { rd, rs1, shamt }), (0b101, 0b010010) =\u0026gt; Some(Self::Bexti { rd, rs1, shamt }), _ =\u0026gt; None, } } _ =\u0026gt; None, } } #[inline(always)] fn alignment() -\u0026gt; u8 { size_of::\u0026lt;u32\u0026gt;() as u8 } #[inline(always)] fn size(\u0026amp;self) -\u0026gt; u8 { size_of::\u0026lt;u32\u0026gt;() as u8 } } impl\u0026lt;Reg, Memory, PC, InstructionHandler, CustomError\u0026gt; ExecutableInstruction\u0026lt; Rv64InterpreterState\u0026lt;Reg, Memory, PC, InstructionHandler, CustomError\u0026gt;, CustomError, \u0026gt; for Rv64ZbsInstruction\u0026lt;Reg\u0026gt; where Reg: Register\u0026lt;Type=u64\u0026gt;, [(); Reg::N]:, { #[inline(always)] fn execute( self, state: \u0026amp;mut Rv64InterpreterState\u0026lt;Reg, Memory, PC, InstructionHandler, CustomError\u0026gt;, ) -\u0026gt; Result\u0026lt;ControlFlow\u0026lt;()\u0026gt;, ExecutionError\u0026lt;Reg::Type, Self, CustomError\u0026gt;\u0026gt; { match self { Self::Bset { rd, rs1, rs2 } =\u0026gt; { // Only the bottom 6 bits for RV64 let index = state.regs.read(rs2) \u0026amp; 0x3f; let result = state.regs.read(rs1) | (1u64 \u0026lt;\u0026lt; index); state.regs.write(rd, result); } Self::Bseti { rd, rs1, shamt } =\u0026gt; { let index = shamt; let result = state.regs.read(rs1) | (1u64 \u0026lt;\u0026lt; index); state.regs.write(rd, result); } Self::Bclr { rd, rs1, rs2 } =\u0026gt; { let index = state.regs.read(rs2) \u0026amp; 0x3f; let result = state.regs.read(rs1) \u0026amp; !(1u64 \u0026lt;\u0026lt; index); state.regs.write(rd, result); } Self::Bclri { rd, rs1, shamt } =\u0026gt; { let index = shamt; let result = state.regs.read(rs1) \u0026amp; !(1u64 \u0026lt;\u0026lt; index); state.regs.write(rd, result); } Self::Binv { rd, rs1, rs2 } =\u0026gt; { let index = state.regs.read(rs2) \u0026amp; 0x3f; let result = state.regs.read(rs1) ^ (1u64 \u0026lt;\u0026lt; index); state.regs.write(rd, result); } Self::Binvi { rd, rs1, shamt } =\u0026gt; { let index = shamt; let result = state.regs.read(rs1) ^ (1u64 \u0026lt;\u0026lt; index); state.regs.write(rd, result); } Self::Bext { rd, rs1, rs2 } =\u0026gt; { let index = state.regs.read(rs2) \u0026amp; 0x3f; let result = (state.regs.read(rs1) \u0026gt;\u0026gt; index) \u0026amp; 1; state.regs.write(rd, result); } Self::Bexti { rd, rs1, shamt } =\u0026gt; { let index = shamt; let result = (state.regs.read(rs1) \u0026gt;\u0026gt; index) \u0026amp; 1; state.regs.write(rd, result); } } Ok(ControlFlow::Continue(())) } } But that was not so great for performance since in addition to branches during instruction decoding, another match (or several when composition of multiple instructions is used) needs to be done on something that was just decoded.\nYou may also notice quite a few generics, that is because interpreter and register/instruction abstractions are generic over both the base instruction set (RV32/RV64) and the number of registers (E extension, so RV32I/RV64I vs. RV32E/RV64E). While there is no instruction decoding or execution implemented for RV32 base yet, it should technically work. Interpreter is also generic over memory, instruction decoder, and handling of syscalls, which is why I\u0026rsquo;m hoping the work I\u0026rsquo;m doing will be useful for the broader Rust ecosystem since I have not seen something that looks exactly like this yet. Also, neither instruction decoder nor interpreter are in any way specific or dependent on anything Abundance-specific, only generic reusable code.\nOf course, I\u0026rsquo;m heavily using Rust Nightly features, so it only runs on Nightly and will stay that way for some time.\nEager instruction decoding # The first obvious thing was when I compared the number of instructions in the contract with the number of instructions executed when doing BLAKE3 hashing and Ed25519 signature verification. Turns out, due to loops in the code, the number of instructions executed is far larger, so it makes sense to decode the whole contract upfront and then simply fetch pre-decoded instructions. That was immediately much better, as can be seen in benchmark results:\nfile/decode-instructions time: [112.00 Âµs 113.21 Âµs 115.70 Âµs] thrpt: [8.6431 Kelem/s 8.8333 Kelem/s 8.9288 Kelem/s] blake3_hash_chunk/interpreter/lazy time: [200.31 Âµs 211.64 Âµs 217.33 Âµs] thrpt: [4.4935 MiB/s 4.6143 MiB/s 4.8753 MiB/s] blake3_hash_chunk/interpreter/eager time: [81.048 Âµs 86.829 Âµs 91.780 Âµs] thrpt: [10.640 MiB/s 11.247 MiB/s 12.049 MiB/s] ed25519_verify/interpreter/lazy time: [6.7690 ms 6.8320 ms 6.8638 ms] thrpt: [145.69 elem/s 146.37 elem/s 147.73 elem/s] ed25519_verify/interpreter/eager time: [2.3935 ms 2.4013 ms 2.4121 ms] thrpt: [414.58 elem/s 416.44 elem/s 417.80 elem/s] Though it was still very far from native, despite however flawed that comparison is to begin with.\nSpatial locality of popular instructions # While I\u0026rsquo;ve done some more improvements like skipping bounds checks during instruction fetching in most cases in PR 517 and combined some shared fields in the same data structure in PR 520, none of it allowed closing the gap substantially.\nWhat I explored then was which instructions are even included in the contract\u0026rsquo;s binary. Turned out, very few relatively to the available set and with a heavy skew towards a small minority:\n2637 ld 80 andn 3 bseti 2086 sd 59 sh3add 3 slti 1983 add 46 andi 3 addiw 1266 addi 26 rev8 2 sextb 974 xor 26 bne 2 slt 736 rori 23 lui 2 sh 689 srli 18 lb 2 lh 593 or 17 beq 2 blt 561 and 15 srai 1 bexti 548 slli 15 jal 1 bclri 526 lbu 14 bltu 1 unimp 393 auipc 11 bgeu 318 jalr 10 sw 260 sb 10 lw 224 roriw 9 sltiu 169 sub 7 subw 127 sltu 5 xori 121 mulhu 4 sh2add 120 mul 4 srl 106 sh1add 4 sll And surely enough, after doing some crude refactoring locally and moving enum variants and interpreter implementation of popular instructions closer together, I was able to observe double-digit percentage performance improvement.\nProcedural macros alone are not enough # The problem is, I\u0026rsquo;d really like to keep instruction decoding and execution the way it is. It is very readable and not tied to any project, it is pretty much what you\u0026rsquo;d write if you read the spec. But it is bad for performance, so what do we do? The answer, often, is procedural macros.\nMy intial plan was to parse enum with instructions and then generate a macro_rules for each variant, so I can recombine them something like this:\n#[instruction] enum Custom { rv64_add!(), rv64_sub!(), } I tried a few variations of this, but ultimately due to the order of macro expansion, those rv64_add!() end up being expanded last and Rust doesn\u0026rsquo;t allow them to be placed in that position. I\u0026rsquo;ve spent some time thinking about something that looks like Rust and concluded that it can\u0026rsquo;t be done with procedural macro alone.\nThankfully, David Tolnay comes to the rescue again with syn and quote that do not require to be used within procedural macros. So the new plan was to parse all files from build.rs, generate necessary replacements bits of code in separate files, which procedural macro will then simply replace original definitions with. And since I want to be able to still do composition of various extensions, build.rs can emit metadata with dependencies and locations of generated files that dependant crates can consume and/or re-export further.\nIn the simplest case, slapping #[instruction] on instruction definition and decoding implementation and #[instruction_execution] on execution implementation is enough to parse and generate necessary metadata:\n/// RISC-V RV64 instruction #[instruction] #[derive(Debug, Clone, Copy, PartialEq, Eq)] pub enum Rv64Instruction\u0026lt;Reg\u0026gt; { Add { rd: Reg, rs1: Reg, rs2: Reg }, // ,,, } #[instruction] impl\u0026lt;Reg\u0026gt; const Instruction for Rv64Instruction\u0026lt;Reg\u0026gt; where Reg: [ const ] Register\u0026lt;Type=u64\u0026gt;, { type Reg = Reg; #[inline(always)] fn try_decode(instruction: u32) -\u0026gt; Option\u0026lt;Self\u0026gt; { // ... } } #[instruction_execution] impl\u0026lt;Reg, Memory, PC, InstructionHandler, CustomError\u0026gt; ExecutableInstruction\u0026lt; Rv64InterpreterState\u0026lt;Reg, Memory, PC, InstructionHandler, CustomError\u0026gt;, CustomError, \u0026gt; for Rv64Instruction\u0026lt;Reg\u0026gt; where Reg: Register\u0026lt;Type=u64\u0026gt;, [(); Reg::N]:, Memory: VirtualMemory, PC: ProgramCounter\u0026lt;Reg::Type, Memory, CustomError\u0026gt;, InstructionHandler: Rv64SystemInstructionHandler\u0026lt;Reg, Memory, PC, CustomError\u0026gt;, { #[inline(always)] fn execute( self, state: \u0026amp;mut Rv64InterpreterState\u0026lt;Reg, Memory, PC, InstructionHandler, CustomError\u0026gt;, ) -\u0026gt; Result\u0026lt;ControlFlow\u0026lt;()\u0026gt;, ExecutionError\u0026lt;Reg::Type, Self, CustomError\u0026gt;\u0026gt; { match self { Self::Add { rd, rs1, rs2 } =\u0026gt; { let value = state.regs.read(rs1).wrapping_add(state.regs.read(rs2)); state.regs.write(rd, value); } // ... } } } And then, of course, additional call needs to be made in build.rs:\nuse ab_riscv_macros::process_instruction_macros; use std::error::Error; fn main() -\u0026gt; Result\u0026lt;(), Box\u0026lt;dyn Error\u0026gt;\u0026gt; { process_instruction_macros()?; Ok(()) } And for all this to be reusable by dependencies it, package.links needs to be added to Cargo.toml:\n[package] name = \u0026#34;ab-contract-file\u0026#34; # ... links = \u0026#34;ab-contract-file\u0026#34; But the most interesting thing is that this makes it possible to split, merge, reorder, or partially skipping instructions. As I mentioned earlier, some instructions are more popular than others:\n/// Instructions that are the most popular among contracts #[instruction( reorder = [ Ld, Sd, Add, Addi, Xor, Rori, Srli, Or, And, Slli, Lbu, Auipc, Jalr, Sb, Roriw, Sub, Sltu, Mulhu, Mul, Sh1add, ], ignore = [ Rv64Instruction, Rv64MInstruction, Rv64BInstruction, Rv64ZbcInstruction, ], inherit = [ Rv64Instruction, Rv64MInstruction, Rv64BInstruction, Rv64ZbcInstruction, ], )] #[derive(Debug, Clone, Copy, PartialEq, Eq)] pub enum PopularInstruction\u0026lt;Reg\u0026gt; {} /// Instructions that are less popular among contracts #[instruction( ignore = [PopularInstruction, Fence, Ecall], inherit = [ Rv64Instruction, Rv64MInstruction, Rv64BInstruction, Rv64ZbcInstruction, ], )] #[derive(Debug, Clone, Copy, PartialEq, Eq)] pub enum NotPopularInstruction\u0026lt;Reg\u0026gt; {} With this, we get two enums, where all instructions are reordered and flattened in both definition and implementation. While at it, I also ignored some instructions that contracts are not supposed to contain like Fence and Ecall. And surely enough, performance improved again significantly; even instruction decoding became much faster:\nfile/parse-only time: [83.585 Âµs 85.928 Âµs 88.935 Âµs] thrpt: [11.244 Kelem/s 11.638 Kelem/s 11.964 Kelem/s] change: time: [âˆ’28.603% âˆ’27.030% âˆ’25.232%] (p = 0.00 \u0026lt; 0.05) thrpt: [+33.747% +37.042% +40.061%] file/parse-with-methods time: [83.389 Âµs 84.704 Âµs 86.508 Âµs] thrpt: [11.560 Kelem/s 11.806 Kelem/s 11.992 Kelem/s] change: time: [âˆ’29.627% âˆ’28.854% âˆ’27.861%] (p = 0.00 \u0026lt; 0.05) thrpt: [+38.621% +40.555% +42.100%] file/decode-instructions time: [91.852 Âµs 92.057 Âµs 92.373 Âµs] thrpt: [10.826 Kelem/s 10.863 Kelem/s 10.887 Kelem/s] change: time: [âˆ’11.233% âˆ’9.3143% âˆ’7.4060%] (p = 0.00 \u0026lt; 0.05) thrpt: [+7.9983% +10.271% +12.655%] blake3_hash_chunk/native time: [769.88 ns 770.34 ns 770.86 ns] thrpt: [1.2372 GiB/s 1.2380 GiB/s 1.2387 GiB/s] blake3_hash_chunk/interpreter/lazy time: [123.31 Âµs 125.63 Âµs 131.19 Âµs] thrpt: [7.4437 MiB/s 7.7734 MiB/s 7.9194 MiB/s] change: time: [âˆ’40.658% âˆ’38.304% âˆ’35.395%] (p = 0.00 \u0026lt; 0.05) thrpt: [+54.786% +62.086% +68.513%] blake3_hash_chunk/interpreter/eager time: [58.122 Âµs 60.115 Âµs 63.763 Âµs] thrpt: [15.316 MiB/s 16.245 MiB/s 16.802 MiB/s] change: time: [âˆ’10.095% âˆ’5.8940% âˆ’2.0291%] (p = 0.01 \u0026lt; 0.05) thrpt: [+2.0711% +6.2632% +11.229%] ed25519_verify/native time: [16.581 Âµs 16.625 Âµs 16.743 Âµs] thrpt: [59.727 Kelem/s 60.150 Kelem/s 60.310 Kelem/s] ed25519_verify/interpreter/lazy time: [4.4544 ms 4.5550 ms 4.7086 ms] thrpt: [212.38 elem/s 219.54 elem/s 224.50 elem/s] change: time: [âˆ’35.284% âˆ’33.827% âˆ’32.133%] (p = 0.00 \u0026lt; 0.05) thrpt: [+47.347% +51.119% +54.522%] ed25519_verify/interpreter/eager time: [2.1032 ms 2.1154 ms 2.1251 ms] thrpt: [470.56 elem/s 472.71 elem/s 475.48 elem/s] change: time: [âˆ’10.465% âˆ’8.8065% âˆ’7.3122%] (p = 0.00 \u0026lt; 0.05) thrpt: [+7.8891% +9.6570% +11.688%] Still about two orders of magnitude slower, but it is not three, progress!\nOf course, above enums are defined downstream of generic crates since they are specific to contracts, but now anyone can do similar things without massive copy-pasting and with quick iteration speed.\nAll this was implemented in PR 531. As a bonus, it also has code pre- and post-processors because it turns out syn only supports stable Rust syntax and I used very much unstable and even incomplete Nightly features, so I had to replace things like impl const Instruction with impl cnst::Instruction and Reg: [const] Register with Reg: BRCONST+Register just to keep it syntaxically valid in stable Rust for syn to parse successfully and then reverting it back before writing to files. You can find this hack here.\nOverall, I\u0026rsquo;m quite happy with the way it turned out, the biggest drawback so far is developer experience since going to definition of these enums in IntelliJ now jumps to generated files, and due to the order of macro expansion and other compile-time limitations, I don\u0026rsquo;t think there is much that can be done with it for now.\nNext steps for improving performance # The workflow still follows the pattern of separate instruction decoding and execution. Now that I have the machinery for parsing and generating code while resolving dependencies, it should be possible to combine instruction decoding and execution in lazy case to skip the extra match before execution. It should also be possible to extract execution of each enum variant into a separate generated function and then store pointers to those functions alongside enum payload, which opens the possibility for jump-dispatch or indirect threading.\nIt is still early for me, but I\u0026rsquo;m optimistic it\u0026rsquo;ll be possible to close the gap of another magnitude order with that, some instruction fusion and adding support for vector extension. While it wouldn\u0026rsquo;t be possible to take advantage of vector extension very easily in Nightly Rust directly until scalable vectors for RISC-V are supported (thankfully, there was a push last year for SVE/SME on aarch64, which RISC-V will benefit from too), LLVM auto-vectorization is still helpful. I tried compiling contracts with V extension and can confirm the code is significantly more compact, meaning LLVM is doing something implicitly.\nIf you have other ideas or would like to work on it, please reach out.\nInfrastructure improvements # In PR 528 and PR 529 I introduced CI checks for RISC-V targets (primarily Clippy and Miri).\nIn PR 534 I did a substantial amount of work on RISC-V compliance testing by parsing test cases from files in included in riscv-non-isa/riscv-arch-test. While I didn\u0026rsquo;t discover any implementation issues so far, I sleep better knowing that test coverage has improved. It is only for interpreter, though. I\u0026rsquo;m still looking for reasonably usable test vectors for instruction decoding that I can take advantage of. If you know about any, please share with me. In the process of working on this, it turned out the whole B extension had broken data for many years in that repo, so only RV64I (RV64E is missing in the repo) and M extension for RV64 is currently being tested.\nImprovements to contracts ABI # Well, that was a long update already, but it turns out working with contracts was a bit more unpleasant than expected, so I\u0026rsquo;ve done some work there as well to improve the status quo.\nFirst, with PR 498 it is now possible to not only write, but also read #[output] values. There were a few bugs found as well, which I fixed with additional test cases added accordingly. This made calling RISC-V contracts for benchmarking purposes directly much more pleasant. In fact, I\u0026rsquo;m now on track to removing the need to specify #[input] and #[output] on contract method arguments, which I think will be a positive change, especially for the simplest cases. The other arguments like slots will still need annotations since there is no way to infer them.\nAnd I changed ABI in PR 508, which I think makes things simpler to explain and simpler in the implementation and removes the need to use null pointers in some cases, which bothered me for a while. I do not see the need to change ABI any further from now on, but we\u0026rsquo;ll see if something else gets discovered to change my mind.\nUpcoming plans # With that unexpectedly lengthy update out of the way, I think I\u0026rsquo;ll pause with RISC-V changes for a bit. It feels more and more like a grind rather than something exciting after working with it for a few weeks. While I didn\u0026rsquo;t wrap the interpreter into the execution environment yet, it shouldn\u0026rsquo;t take too long once I actually need it with the refactoring I have done recently.\nI\u0026rsquo;ll probably focus on sharding for a bit again, after all, the global history and shard confirmation still lacks implementation and has some open design questions I need to address before intermediate and leaf shards can be introduced and for the whole thing to look like something more than another typical basic blockchain.\nI\u0026rsquo;ll be happy to chat about anything you\u0026rsquo;ve read here on Zulip and will be back with another update once I have more to share about the progress.\n","date":"24 January 2026","externalUrl":null,"permalink":"/blog/2026-01-24-making-risc-v-interpreter-faster/","section":"Blog","summary":"\u003cp\u003eIn the \u003ca\n  href=\"../2025-12-29-contracts-cli-and-risc-v-interpreter\"\u003elast update\u003c/a\u003e I introduced a basic RISC-V interpreter, but its performance was underwhelming, which was to be\nexpected, but still was something that I\u0026rsquo;d like to improve. So since the last update I have implemented infrastructure\nfor measuring performance, did a bunch of refactorings to hopefully make my work reusable by other projects in the\nfuture, and even implemented some performance improvements with a solid idea of what to explore next.\u003c/p\u003e\n\u003cp\u003eIn the process of doing it I ended up parsing and generating Rust code in \u003cem\u003eboth\u003c/em\u003e \u003ccode\u003ebuild.rs\u003c/code\u003e and procedural macros,\nsomething I have never thought I\u0026rsquo;d end up doing, but let\u0026rsquo;s start from the beginning.\u003c/p\u003e","title":"Making RISC-V interpreter faster","type":"blog"},{"content":"","date":"24 January 2026","externalUrl":null,"permalink":"/authors/nazar-pc/","section":"authors","summary":"","title":"nazar-pc","type":"authors"},{"content":"","date":"24 January 2026","externalUrl":null,"permalink":"/tags/status-update/","section":"tags","summary":"","title":"status-update","type":"tags"},{"content":"","date":"24 January 2026","externalUrl":null,"permalink":"/tags/","section":"tags","summary":"","title":"tags","type":"tags"},{"content":"It has been around a year since the project started, and I would like to share a brief summary of what has happened so far, where the project is at and what\u0026rsquo;s coming next. I\u0026rsquo;ll only capture the highlights and key outcomes here, presented in rough chronological order.\nDecember 2024 # In December 2024 I have officially transitioned out of Autonomys Labs (previously known as Subspace Labs) and started working on \u0026ldquo;Project Abundance\u0026rdquo; as an independent R\u0026amp;D project with the sponsorship from Subspace Foundation (for which I\u0026rsquo;m very grateful).\nThe first commit in the repository was made on December 27, 2024.\nJanuary 2025 # I had a conviction that the way smart contracts development is done in existing blockchain ecosystems is suboptimal and is not designed for performance. So my first goal was to think from the first principles what a good, high-performance design could look like.\nI researched existing blockchains and did a bunch of prototyping and experimentation. By the end of the month, contracts were almost running with a lot of initial infrastructure and some early documentation available. There was no execution environment available yet, there was definitely no blockchain, but it was sufficient to get an idea of what it would look like to write contracts for a future system, what benefits, limitations and challenges it would have.\nFebruary 2025 # At the beginning of February I finally implemented the first version of the native execution environment, which finally allowed running basic contracts with some basic documentation for the #[contract] macro.\nSame month I was talking to some developers to collect initial feedback about the contract design, which helped with the APIs, documentation improvements and generally was a great opportunity to get some external feedback.\nI also spent some time tuning the performance of the core data structures related to execution of contracts to get an idea of the low-level overhead required before any contract logic even begins executing, which turned out to be tiny, especially for the native execution environment.\nMarch 2025 # In March, with contracts being somewhat usable, I went to the next layer in the stack: transactions. I spent a lot of time thinking about how transactions should work in a sharded blockchain and came up with a generic design, where the verification of transactions is done by a \u0026ldquo;wallet\u0026rdquo; contract. This way, there is a huge design space for implementation of contracts, with the blockchain itself not being aware of the signature verification logic or similar concepts, usually hardcoded into the core protocol of other blockchains.\nThis was also the month when Subspace Foundation sponsored Alfonso to help me with research of the sharding design.\nBy the end of the month there was a way to build contract files in ELF format. We also had productive discussions about the way hierarchical sharding might work with Proof-of-Archival-Storage consensus.\nApril 2025 # April continued with the sharding design research and preparations towards having an actual blockchain one day. Eventually I started seeing trees everywhere, and I must admit, I continue seeing them everywhere to this day.\nBy the end of the month I have refactored and massively improved the performance of some components forked from Subspace.\nMay 2025 # In May, I continued tinkering with forked Subspace components with the goal of shaping them closer to what I wanted and not depending on Substrate since I was not planning to use it.\nA big and nice accomplishment of this month was the user-readable formatting for contract addresses. It was a lot of research and is inherently linked to the sharding design, I\u0026rsquo;m quite happy with the way it turned out.\nBy the end of the month, Alfonso also started seeing trees everywhere in the design. We were also working on a way to organize the hierarchical shards in a way that allows having a reasonable confirmation of blocks and archived history, such that we can actually have efficient cross-shard communication.\nAs a result of a lot of brainstorming and all the discussions, I finally designed a shard-native block structure for the beacon chain, intermediate shards and leaf shards, such that they form a hierarchical 3-level tree. It was a long and tedious process, but crucial to any progress down the line.\nJune 2025 # In June, we discussed how to assign farmers to shards, although the decision and especially implementation materialized much later.\nI started making steps towards having initial block production and stumbled upon many opportunities for performance optimizations, which was quite rewarding to explore and continues to be the trend to this day. Modern hardware can do so much when software is designed with performance in mind!\nWe explored the topic of sector expiration extensively, but it has proven to be hard to come up with something robust that is also reasonably simple to understand and implement.\nJuly 2025 # July marked the start of a lengthy process of tinkering with an awesome rust-gpu after exploring a few alternatives that ended up being much less awesome. Performance improvements continued to pour in from various fronts, making everything much faster still.\nExploration of sector expiration and a way to commit farmers to shards continued, though nothing better than the previous design was ultimately found.\nI continued building foundational components like Sparse Merkle Tree and client database.\nUnfortunately, this was the month when Alfonso\u0026rsquo;s contract was terminated by Subspace Foundation, so I was the only one working from this month onwards.\nAugust 2025 # In August, I finally managed to put together a very custom client database prototype and worked on transaction processing design. I even created a node prototype, finally, even though it wasn\u0026rsquo;t capable of doing much just yet.\nAs I was tinkering with rust-gpu all this time and working on implementing sector encoding on the GPU, I kept discovering ways to make it faster on the CPU too. In the end, CPU plotting is more than 2x faster than Subspace repo at the time and even today.\nSeptember-October 2025 # I think it is safe to say that I spent most of September and October on GPU programming. I learned a lot more ways still to accelerate Proof-of-Space both on CPU and GPU. Ultimately, I was able to get GPU plotting to work.\nNovember 2025 # Then there was almost a month of work gap as I was trying to get GPU plotting to work at a reasonable speed since the initial version wasn\u0026rsquo;t very fast. I didn\u0026rsquo;t post any updates during this time as it was probably not as interesting to anyone following.\nWhat was interesting is that by the end of November I reached a key milestone, the node prototype mentioned before started producing beacon chain blocks!\nUnfortunately, this was also the month when Subspace Foundation sponsorship ended for me too, so at this point I\u0026rsquo;m alone and living off my savings.\nDecember 2025 # The first produced block achieved in November was nice. However, there were plenty of errors shortly after that, so my December started with fixing a bunch of things to the point that I was able to produce 10k blocks, which involved more consensus processes like solution range adjustment, Proof-of-Time entropy injection and block archiving.\nI then started taking steps towards multi-shard farming and implemented a permissionless shard assignment designed back in June.\nBy the end of the month I decided to work on something more rewarding again related to contracts. After some research and experimentation I landed the initial version of RISC-V interpreter, CLI for building contracts and defined a contract file format no longer based on ELF, but one that can be converted from (and eventually to) ELF.\nToday # And today is January 1st 2026, so I can\u0026rsquo;t tell you how this year ends until we\u0026rsquo;re through. But I am sure there will be key developments with contract execution, I expect block production of intermediate shards and leaf shards at some point later this year, and maybe we\u0026rsquo;ll even get some networking between the nodes to sync from each other in a small devnet.\nDefinitely a lot of exciting stuff ahead, can\u0026rsquo;t wait to see where we end up one more year from today.\nAcknowledgements # I\u0026rsquo;d like to thank again the Subspace Foundation for their financial support, Alfonso for helping with research, Shamil and Liu-Cheng for fruitful developer feedback and everyone else I had a chance to discuss things with in some capacity.\nI also want to thank Zulip for providing free access to \u0026ldquo;Zulip Cloud Standard\u0026rdquo; and Graphite for free access to their AI code review service.\nAnd certainly to Subspace Labs (now Autonomys Labs) for original research and implementation of the Subspace protocol, none of this would have been possible otherwise, especially to Jeremiah and Dariia.\nIf you want to find me, Zulip Chat is the best way to start a conversation on topics related to \u0026ldquo;Project Abundance.\u0026rdquo;\n","date":"1 January 2026","externalUrl":null,"permalink":"/blog/2026-01-01-a-year-of-abundance/","section":"Blog","summary":"\u003cp\u003eIt has been around a year since the project started, and I would like to share a brief summary of what has happened so\nfar, where the project is at and what\u0026rsquo;s coming next. I\u0026rsquo;ll only capture the highlights and key outcomes here, presented\nin rough chronological order.\u003c/p\u003e","title":"A year of Abundance","type":"blog"},{"content":"","date":"1 January 2026","externalUrl":null,"permalink":"/tags/announcement/","section":"tags","summary":"","title":"announcement","type":"tags"},{"content":"Switching to something that I thought would be more fun, I decided to look into ELF and RISC-V last week or so. I learned more than I wanted and managed to achieve a few key deliverables:\ndesign initial CLI for building/converting/verifying contract files define and implement a contract file format implement a simple RISC-V interpreter Let\u0026rsquo;s look into each of those in more detail.\nInitial CLI for building/converting/verifying contract files # I spent quite some time in the past thinking about how the contracts should be built and what the developer experience about that would look like. Ideally, the developer would just call cargo build and get a file they can upload to the blockchain. Unfortunately, things are not quite that simple, at least for now, and there are multiple reasons for that.\nThe first reason is that there isn\u0026rsquo;t an official target that would fit the use case for contracts perfectly, which means a custom target specification is needed, which involves custom CLI options and requirement of a nightly Rust toolchain. Not only that, the standard library is not available for custom targets, and there is no way to specify that it needs to be built on demand, which requires more CLI options. And on top of everything I really wanted the developer experience writing and interacting with contracts look as \u0026ldquo;normal\u0026rdquo; as possible, meaning not requiring special crate type definitions in the Cargo.toml and having extra build artifacts when a contract is used as a dependency, which leads to even more CLI options and the need to build with cargo rustc instead of cargo build.\nFor my experiments I used a command that looked something like this:\ncargo rustc \\ --crate-type cdylib \\ -Z build-std=core \\ --package ab-example-contract-flipper \\ --features ab-example-contract-flipper/guest \\ --profile production \\ --target crates/contracts/riscv64em-unknown-none-abundance.json I\u0026rsquo;m sure it is easy to see that it is not fun to deal with all the time. Not to mention that anyone using it would have to have an up-to-date version if the target specification file somewhere nearby at all times.\nI decided to implement a cargo extension CLI cargo ab-contract that would simplify the process of building contracts and provide a more convenient experience for developers. The first step taken in PR 483 was to implement convert command that takes ELF cdylib as an input and produces a contract file as an output, which was extended with verify command in PR 493 and build command that replaces that messy cargo rustc command followed by cargo ab-contract convert with a simple cargo ab-contract build in PR 494.\nEventually I plan to implement recover command that would take a contract file as an input and produce an ELF cdylib that would behave similarly to the original one, which may be useful for debugging purposes with traditional tooling, but it is not 100% clear if that would ever be actually necessary. We\u0026rsquo;ll see.\nContract file format # Since I mentioned multiple times in the past that the plan was to use ELF files, you might be wondering why the conversion step is necessary? Well, I spent a lot of time researching and experimenting with ELF RISC-V cdylib files and what they look like. Turns out there is A LOT to it, and it is quite non-trivial to process them correctly, not to mention that they support so many potential things that will never be needed. On top of that, there is a size concern. For simple contracts various sections and headers ELF files typically contain are just dead weight, and with contract metadata included in statics, some of it ends up being duplicated in the output file.\nI also spent some time looking into what PolkaVM does and some other projects and concluded that it would be beneficial to have something way simpler that would be trivial to parse and would only contain the bare minimum necessary.\nWhile looking at the file format, I was also thinking about interpreter/static binary recompiler implementation and the way memory is supposed to be organized during execution. PolkaVM has an interesting WASM-like design where address space for code and data are separate and code can\u0026rsquo;t be read, only executed. I decided to not go that way to make the contract file as close to the original ELF cdylib as possible and to make the conversion process dead simple.\nThis also raises the question about what features should even be supported from the developer perspective. For example, thread-local storage (TLS) is not useful in a single-threaded environment, I decided early on to not have syscalls and dynamic memory allocation, but should global mutable statics be available? I ultimately decided that no global mutable statics should be supported. Rather, a generous amount of stack space (by blockchain standards) will be given during contract execution, and that is all code will have to work with (in addition to input/output arguments of the contract call). I learned to dislike global mutable state of all kinds over the years, so this is in-line with my evolved personal preferences.\nSo what we\u0026rsquo;re left with in terms of what the file should contain is read-only data (constants, read-only statics, contract metadata) and code. Code can remain mutable, but nothing outside the code section will be executable.\nThe conversion process then consists of a bunch of checks to ensure the ELF file doesn\u0026rsquo;t contain what can\u0026rsquo;t be supported by the contract, then a small header is written first followed by a read-only section and code section from the ELF file. I made sure to preserve relative offsets between read-only data and code sections such that no fixups are needed for execution and reversing is still possible.\nThe header right now looks like this:\n/// Header of the contract file #[derive(Debug, Clone, Copy, PartialEq, Eq, TrivialType)] #[repr(C)] pub struct ContractFileHeader { /// Always [`CONTRACT_FILE_MAGIC`] pub magic: [u8; 4], /// Size of the read-only section in bytes as stored in the file pub read_only_section_file_size: u32, /// Size of the read-only section in bytes as will be written to memory during execution. /// /// If larger than `read_only_section_file_size`, then zeroed padding needs to be added. pub read_only_section_memory_size: u32, /// Offset of the metadata section in bytes relative to the start of the file pub metadata_offset: u32, /// Size of the metadata section in bytes pub metadata_size: u16, /// Number of methods in the contract pub num_methods: u16, /// Host call function offset in bytes relative to the start of the file. /// /// `0` means no host call. pub host_call_fn_offset: u32, } It is then followed by a bunch of pointers to exported contract methods:\n/// Metadata about each method of the contract that can be called from the outside #[derive(Debug, Clone, Copy, PartialEq, Eq, TrivialType)] #[repr(C)] pub struct ContractFileMethodMetadata { /// Offset of the method code in bytes relative to the start of the file pub offset: u32, /// Size of the method code in bytes pub size: u32, } This is all that is needed to load read-only data and code into memory, find/decode contract metadata (which tells us about all available methods) and where to find them in memory for execution. I even wrote a simple CLI to call such contracts.\nhost_call_fn_offset is a special case, it tells us where the host call function is located in memory. Since I didn\u0026rsquo;t want to use syscalls, the host call is simply an external import in the ELF file, which is proxied through the exported function. The exported function has a distinct assembly that can be rewritten during execution into something that acts as a host call, while loading ELF cdylib in a regular RISC-V process allows making host calls through a regular mechanism shared libraries normally use. Feels a bit hacky, so let me know if you know something better.\nThe code section is parsed as a series of RISC-V instructions and if unexpected instructions are encountered, like ecall used for making syscalls, the whole contract is rejected as invalid.\nI\u0026rsquo;m still thinking whether I should make it a requirement for contracts to be compressed with Zstandard. If so, it would be possible to include the padding that sometimes appears between read-only data and code sections in the file size, such that when the contract is decompressed, it doesn\u0026rsquo;t need any post-processing to be loaded into memory, and the decompressed data would already have the correct memory layout. From what I found, it should be fine security-wise, but I am still hesitant for some reason. Thankfully, there is a rust decompression crate, so I wouldn\u0026rsquo;t have to mess with bindings if I go that route.\nInitial file format definition landed together with cargo ab-contract in PR 483 with some fixes in PR 485, parsing of the new file was implemented in PR 487 and that was used for verification post-conversion from ELF cdylib in PR 493 along with instruction parsing and verification.\nRISC-V interpreter # To test all of the above, I needed a way to run RISC-V code. An interpreter is a great way to start, and I was pleasantly surprised by both how simple RISC-V really is compared to something like x86-64 and how capable LLMs are generating initial mostly working prototype and lengthy tests for it.\nI started with a new target specification in PR 481 since I needed to be able to produce the files first. I was actually running ELF files directly at first, which helped me to explore and design my own file format. Initial definition of registers and instructions landed in PR 483 with the file format introduction. To make it potentially more usable outside the project, I refactored registers in PR 486 so not just RV64E, but also RV64I base ISA is supported. After all, the only difference between the two is the number of general purpose registers and literally nothing else.\nCalling some methods incorrectly, I quickly discovered magic instruction 0xc0001073 that objdump read as unimp, which various compilers canonically use as invalid instruction for panics, so I added support for that in PR 488 too. Encouraged by how helpful LLMs were in generating instruction parser, I used them to generate almost two thousand lines of tests in PR 489. While I didn\u0026rsquo;t audit the 100% of tests, they look good enough for now.\nWith all those preparations I was finally able to land an interpreter I was toying with for a few days in a local branch in PR 490. It is a very basic textbook interpreter that decodes and executes one instruction at a time. Low performance, no gas metering, etc., but it works and can be built upon or used as a reference.\nShard allocation follow-up # The previous update was about the initial shard allocation implementation, which I briefly continued before switching to RISC-V-related business.\nAs mentioned there, I wasn\u0026rsquo;t quite happy with a shard rotation interval measured in slots, so I switched it to beacon chain blocks in PR 480, which was a big simplification, especially to intermediate and leaf shards implementation when time comes. I missed tying allocation to the public key hash initially and fixed it in PR 482.\nThe last improvement is a very basic algorithm for reducing the total set of unique history sizes used per plot in PR 479. It is functional but has a high chance of resulting in too much replotting and also might cause recent history being significantly underrepresented compared to Subspace. I decided to not overengineer it for now, but it\u0026rsquo;d be nice to calculate the probabilities and decide what the algorithm should look like long-term and how the reference implementation should behave when it comes to replotting.\nUpcoming plans # With a basic interpreter in place, it is still not enough to run contracts. An execution environment needs to be implemented around it similarly to the already existing native execution environment. It\u0026rsquo;ll probably take a couple of weeks before I have that, but there are zero unknowns about its feasibility at this point.\nI\u0026rsquo;d like to write some benchmarks and see what kind of performance I get and how difficult it would be to make it half-decent with minimal effort.\nThis will keep me occupied for a while, I\u0026rsquo;ll decide when to tackle gas metering and other things later, though I have been doing some research on that already.\nThis week felt more lively than usual, which I\u0026rsquo;m quite happy about. We\u0026rsquo;ll see how the next one goes.\nIt has actually been almost exactly one year since the project formally started. I\u0026rsquo;ll be writing a post summarizing the progress so far and the road ahead, which with some luck will become a yearly occurrence for many years to come.\nZulip is where you can find me to chat about anything related to this update or the project in general.\n","date":"29 December 2025","externalUrl":null,"permalink":"/blog/2025-12-29-contracts-cli-and-risc-v-interpreter/","section":"Blog","summary":"\u003cp\u003eSwitching to something that I thought would be more fun, I decided to look into ELF and RISC-V last week or so. I\nlearned more than I wanted and managed to achieve a few key deliverables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edesign initial CLI for building/converting/verifying contract files\u003c/li\u003e\n\u003cli\u003edefine and implement a contract file format\u003c/li\u003e\n\u003cli\u003eimplement a simple RISC-V interpreter\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet\u0026rsquo;s look into each of those in more detail.\u003c/p\u003e","title":"Contracts CLI and RISC-V interpreter","type":"blog"},{"content":"Last time I shared challenges and next steps in making consensus shard-aware. I\u0026rsquo;m happy to report that an initial version of the shard assignment algorithm is now implemented and working.\nTo the best of my knowledge, this is the first shard assignment design of this kind in blockchain space. All other designs are permissioned and require on-chain registration with some tokens, which hinders both participation and scalability.\nThe majority of changes are in PR 476, you can read the description there and higher-level conceptual description in the previous blog post.\nThe major outcome is that the farmer is mostly prepared to support multiple shards now. I will have to add notifications about what permutations of public key hash/shard commitments roots/history sizes that connected farmers are currently using so that node can sync onto corresponding shards. However, since there are no actual shards yet and sync implementation is still far away, the implementation of it is missing right now, though the RPC interface is already in place.\nFrom here the next major thing will be to add super segments and figure out all the details about creating the global history. I explained a high-level idea last time, and I don\u0026rsquo;t really have anything to add to that just yet.\nThere was a bunch of various fixes throughout last week, but nothing interesting enough to mention here.\nUpcoming plans # The fact that shard allocation is based on slot interval and not beacon chain block interval annoys me quite a bit, so I\u0026rsquo;ll probably change it soon. It is especially annoying since non-beacon chain blocks do not carry PoT checkpoints in them, which will make verification even trickier than it already is. Tricky consensus logic is something I\u0026rsquo;d prefer to avoid, though.\nAnd I\u0026rsquo;m kind of itching to experiment with RISC-V interpreter/VM to try to confirm some of my hypotheses.\nThis update is very short because I had something important to share, but kind of explained it already in the previous blog post. Ping me on Zulip if you have any questions.\n","date":"19 December 2025","externalUrl":null,"permalink":"/blog/2025-12-19-permissionless-assignments-of-farmers-to-shards/","section":"Blog","summary":"\u003cp\u003e\u003ca\n  href=\"../2025-12-11-steps-towards-multi-shard-farming\"\u003eLast time\u003c/a\u003e I shared challenges and next steps in making consensus shard-aware. I\u0026rsquo;m happy to report that an initial\nversion of the shard assignment algorithm is now implemented and working.\u003c/p\u003e\n\u003cp\u003eTo the best of my knowledge, this is the first shard assignment design of this kind in blockchain space. All other\ndesigns are permissioned and require on-chain registration with some tokens, which hinders both participation and\nscalability.\u003c/p\u003e","title":"Permissionless assignments of farmers to shards","type":"blog"},{"content":"So far the block production was basic and very similar to Subspace, despite some key underlying differences. But that is just an early prototype to get beacon chain running, but intermediate and leaf shards must be added into the mix, which substantially complicate things.\nI\u0026rsquo;ve spent a lot of time thinking about various moving pieces and merged a few PRs with various refactorings that will be useful down the road.\nIn short, there are two key things that are completely unimplemented and not even described accurately yet, which I focused on:\nshard allocation algorithm global archival history Shard allocation algorithm # The closest to public description of the algorithm is by Alfonso in PR 277. The PR is not 100% accurate and was supposed to be updated, but that never happened, so I\u0026rsquo;ll describe the gist of the problem and solution here.\nIn short, the goal is to have a completely permissionless algorithm, according to which farmers will be assigned to shards, while ensuring uniform distribution of both space pledged and security (honest/adversarial farmers ratio).\nThe permissionless part is what causes the most trouble here: farmers should not register or explicitly announce themselves on the network, yet they should be assigned to a shard verifiably. Moreover, farmers can\u0026rsquo;t be allowed to just pick the shard at will because attacker would easily occupy and compromise a shard, wreaking havoc since all shards are considered to be trusted, but not fully verified by everyone.\nLuckily, to participate in the consensus, farmers have to do plotting, which is a compute-intensive process. So their plot just needs to be assigned to a shard unpredictably, in a way that forces them to replot from scratch when a specific shard destination is desired.\nWhile at it, the solution also considers a few hardening factors: post-quantum security (so not VRFs with elite-curve cryptography) and some DoS protection (so it is not possible to guess which farmer is assigned to which shard externally).\nHere is the solution sketch:\nfarmer derives a set of random private values (will likely be derived from the keypair), something like (2^20) of them, and builds a Merkle Tree from it the plotting process requires a farmer to include Merkle Tree root in the PoSpace seed when encoding sectors, tying sectors to the set of values on-chain randomness is taken at an interval (like 1 hour, depends on how fast and expensive shard sync is) to update shard assignment, ensuring the assignment is not static on-chain together with farmer\u0026rsquo;s public key hash indicates which value in the above Merkle Tree is used for shard assignment when winning the puzzle, a farmer reveals the value in the set that proves shard assignment This way we have a completely permissionless algorithm for shard assignment, which keeps shard assignment private until a farmer won a chance to produce a block, while ensuring farmers are assigned to shards randomly and uniformly and are forced to rotate according to a tight schedule.\nThe size of the plot is already capped to 65 TiB by the SectorIndex data type and on a large scale it does not matter, but there is actually a way to inflate the effective plot size if a farmer commits the same sector to different history sizes (and the number of options increases as blockchain gets bigger). This is a big problem if Subspace construction is used without extra modifications. To solve this, I came up with a simple solution, which essentially mixes in the history size into the shard allocation algorithm. This way creating a sector for the same public key hash, but different history size would result in a different shard allocation.\nThis is not difficult to implement and would work as intended, but it causes some inconveniences due to a farmer needing to follow more than one shard per plot as they incrementally replot as well as being potentially forced to use outdated history size just to avoid following the ever-growing number of shards. I had some conceptual ideas on how to improve that, and Alfonso came up with a proposal, but it is complex and unfinished. I\u0026rsquo;ll run with the simple solution for now, but if someone is interested in researching improvements, please ping me.\nGlobal archival history # The global archival history is even more complicated than that. I do not have a complete solution for it yet, so I\u0026rsquo;ll share the high-level problem and solution.\nThe goal is to have each shard archive their local history independently as blocks are produced, yet to have an eventual ordering of all segments into a single linear global history. Just like in Subspace, to verify any piece of the history, it should be sufficient to have a compact list of roots that pieces can be verified against. The list should be so small that it can be kept in memory even on resource-constrained clients, despite the massive global history size.\nConceptually, this means that the segments that each shard produces initially are basically incomplete. Segment roots will be propagated up the hierarchy into the beacon chain. Once considered to be confirmed (I\u0026rsquo;ll describe how exactly later once I have the whole design worked out), a set of segment roots will form a super segment. In Subspace a piece already includes a proof of that its record belongs to the segment root, so here we\u0026rsquo;ll need to add yet another proof that segment root belongs to a super segment.\nOnce a super segment is created, it defines the ordering of its segments, which assigns unique global index to corresponding pieces that various clients can then use for piece retrieval and verification.\nThere are various edge-cases and annoying complications, but high-level this is the idea. One super segment root is just 32 bytes, but it commits to a bunch of individual segments, which have ~256 MB worth of blockchain history. This is a pretty high compression ratio if there are hundreds or even thousands of segments in each super segment. This means that even when the global history is measured in exabytes, it will be easy to store al super segment roots in memory even in browser extensions and mobile apps.\nProgress so far # As mentioned at the beginning, a lot of time was spent recalling this information since I was last actively working on it a few months ago and thinking about the exact steps. That said, I did some preparation already.\nIn PR 465 I introduced BlockProducer trait, which will allow implementing multiple variants of it for different kinds of shards. Previously the implementation was designed around producing beacon chain blocks. With shard assignment there will be a single shared slot worker that issues challenges to the farmer. However, which shard the solution lands on will vary on the solution, thus block producer needs to be customizable.\nIn PR 466 I removed separate ephemeral SegmentHeadersStore and merged its functionality into the database implementation, so each shard can persist its segment headers independently across restarts.\nUnrelated, but something that bothered me for some time was PR 461 where I removed one of the last remnants of Substrate-specific code related to reward addresses. It now uses the format I described in the Address formatting article, but even that is still a placeholder. I think eventually rewards will be slightly delayed with the \u0026ldquo;claim\u0026rdquo; mechanism, so the implementation will most likely probably change, but at least there is no dependency on Substrate-specific crates anymore.\nUpcoming plans # The immediate next step is to wire the initial version of the shard assignment logic into both farmer and solution verification on the client side. From there I\u0026rsquo;ll probably focus on the global archival history design.\nSince these are something that I\u0026rsquo;d call more \u0026ldquo;boring,\u0026rdquo; I\u0026rsquo;ll probably start working on RISC-V VM/interpreter thingy in the meantime too, which I expect to be more fun.\nThat is it for today, I\u0026rsquo;ll be back with more updates in not-so-distant future. Until then Zulip is where you can find me any day of the week.\n","date":"11 December 2025","externalUrl":null,"permalink":"/blog/2025-12-11-steps-towards-multi-shard-farming/","section":"Blog","summary":"\u003cp\u003eSo far the block production was basic and very similar to \u003ca\n  href=\"https://subspace.github.io/protocol-specs/docs/category/consensus\"\n    target=\"_blank\"\n  \u003eSubspace\u003c/a\u003e, despite some key underlying differences. But that\nis just an early prototype to get beacon chain running, but intermediate and leaf shards must be added into the mix,\nwhich substantially complicate things.\u003c/p\u003e","title":"Steps towards multi-shard farming","type":"blog"},{"content":"In the previous update I shared that the block production started to work on the beacon chain. Well, it did produce the first block, but not that many more, for a variety of reasons. But majority of those reasons are not fixed, so I\u0026rsquo;m happy to share what they were and where things are at today.\n2025-12-02T18:30:30.293062Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=80592 number=13621 root=96fd67e51a475dd17e8d52f192ab77716f4cf11fe8f10e013541c9637ffb9498 pre_seal_hash=65d47b1db13230898027763f09f941795c2005c76acaf84eafec8aa3f3275d6b 2025-12-02T18:30:30.301322Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=13621 root=96fd67e51a475dd17e8d52f192ab77716f4cf11fe8f10e013541c9637ffb9498 2025-12-02T18:30:30.795821Z INFO ab_client_informer: ðŸ’¤ shard=BeaconChain best_number=13621 best_root=96fd67e51a475dd17e8d52f192ab77716f4cf11fe8f10e013541c9637ffb9498 2025-12-02T18:30:31.293669Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=80597 number=13622 root=f7f8f5fd64c433be9954e3e8f7d40c1c54e8517d0fa7a56b14e5fda46f1a56be pre_seal_hash=7fbd8c6eff83cc1fcc4ec368366ad3988fedd2e6b525b939ed49b64cd15e5211 2025-12-02T18:30:31.301710Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=13622 root=f7f8f5fd64c433be9954e3e8f7d40c1c54e8517d0fa7a56b14e5fda46f1a56be 2025-12-02T18:30:31.493792Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=80598 number=13623 root=0313cd2d5d70c7ae299bdadaf595e66fbfaa945cf6f8826fe5f6bb233e579289 pre_seal_hash=d8cf050a2229b56a303f0cb4ffa3e6a04b6bef904ac85179d7769f4f32ca9fb3 2025-12-02T18:30:31.501569Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=13623 root=0313cd2d5d70c7ae299bdadaf595e66fbfaa945cf6f8826fe5f6bb233e579289 2025-12-02T18:30:32.694473Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=80604 number=13624 root=3d80b9ecf7a5f2897cb08e1c64bc29508fbfed02d5112fc0601a37e38222624f pre_seal_hash=1eac2eb7389a64c607cab88f8ce53cf8ff8edeea9e7921e8fb71d0b805b3f6e1 2025-12-02T18:30:32.702264Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=13624 root=3d80b9ecf7a5f2897cb08e1c64bc29508fbfed02d5112fc0601a37e38222624f 2025-12-02T18:30:34.095277Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=80611 number=13625 root=8b87d0e0e41c97ec7818d5d7394061d360c6396915a93949b94c73e0ef547794 pre_seal_hash=427625877e63ed57c09c52a9927e8f8d7be32abc127b655ed1503b4968c56fe3 2025-12-02T18:30:34.103259Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=13625 root=8b87d0e0e41c97ec7818d5d7394061d360c6396915a93949b94c73e0ef547794 2025-12-02T18:30:35.797046Z INFO ab_client_informer: ðŸ’¤ shard=BeaconChain best_number=13625 best_root=8b87d0e0e41c97ec7818d5d7394061d360c6396915a93949b94c73e0ef547794 2025-12-02T18:30:37.297141Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=80627 number=13626 root=6942ff43d98f35dad0ff1668cb9068b03599aa14f3fbebc2d6a276ce4aa05756 pre_seal_hash=6ec425df17e120f46aa888575cd59af1fc04b9724110122b935f63d6dbc5bb73 2025-12-02T18:30:37.305484Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=13626 root=6942ff43d98f35dad0ff1668cb9068b03599aa14f3fbebc2d6a276ce4aa05756 CPU/GPU plotting mismatch # The first issue was that occasionally I was getting strange issues about either insufficient quality or other consensus errors, but only when plotting with GPU. It took me a few days to track this down due to this being a very rare issue that was not reproducible with generated inputs in integration tests. Eventually I fixed it in PR 452, it turned out that the CPU version was relying on some assumptions that were no longer true and that the GPU version correctly took into consideration. With that I did not get those strange errors anymore, but there were plenty of others.\nArchiving issues # When 100 blocks were produced, archiver attempted to archive the block by reading it from the database, but the database had a TODO instead of block reading implementation. I initially thought the block would be in RAM and that it would be able to proceed anyway. However, I recalled that wrong. The block body was only present on the disk at that point, so I had to implement block reading.\nI changed the block reading API to async and implemented it in PR 457. In the process of doing so, I had to fix a few more issues, notably offsets when it comes to reading from disk were in pages instead of bytes, which made writes messing the data and reading it back resulted in garbage. There was also a block confirmation issue. And while at it, I improved internals a bit with some refactoring. It is all still quite unpleasant to work with, but I don\u0026rsquo;t have a good idea how to improve it drastically yet.\nPoT issues # With those out of the way, I was able to produce more blocks until the point where entropy was supposed to be injected into PoT, at which there were a bunch of issues found. From minor block decoding error to wrong sources of PoT parameter changes. The big item, though, was TODO in place of the notification from the block import to the PoT source worker, which caused PoT entropy to not be injected at all, but block verification was still expecting it.\nAll those and some more were fixed in PR 458 and PR 459. With these fixes the beacon chain is able to produce over 10k blocks with no issues, going through PoT entropy injection, solution range adjustments, etc. Very similarly to how Subspace works, though a lot of things are reimplemented from scratch slightly differently.\nOther nice to have things # Feature-wise, I have implemented an \u0026ldquo;informer\u0026rdquo; in PR 454, which just like in Substrate prints a message every five seconds with the current state of the blockchain, you can see it in the logs above.\nTo make debugging more pleasant, I added a tiny benchmark in the timekeeper in PR 455. Now, when that benchmark detects that the slot was produced way too quickly, it will switch to a different mode where it sleeps after creation of PoT checkpoints to maintain expected slot time. This way everything progresses as expected, while the actual CPU usage for PoT proving is essentially zero instead of burning a full CPU core at all times.\nAnd lastly, in PR 456 I went through all unsafe code, ensured all usages have safety comments, and enabled Clippy lint to check that in CI.\nUpcoming plans # That is it for now, the implementation is much less buggy now than at the time of the previous update, but I\u0026rsquo;m sure there are still plenty of things left to fix as well.\nI\u0026rsquo;ll be going through various TODOs. The database pruning is still not implemented, and I\u0026rsquo;ll probably postpone it because I\u0026rsquo;m not going to run the node for days any time soon anyway. I\u0026rsquo;ll need to implement bounded block size, though, so the database size can be estimated and pre-allocated, instead of guessing. Once that is done, it\u0026rsquo;ll be possible to know how many shards a node can run concurrently within allocated disk space (there will be at least the beacon chain, one intermediate shard and one leaf shard, possibly more). Still not sure what to do about contracts caching, but I guess I\u0026rsquo;ll figure it out once I get there.\nThe networking stack is still an open question, but I\u0026rsquo;ll probably not need/be blocked by it for some time still. I\u0026rsquo;d really like something relatively simple and photobuf-free, and libp2p is neither at the moment, especially Rust version.\nRISC-V VM is also in the background, but I might start some experimentation to get something functional off the ground myself. That will be a big and probably both fun and frustrating project.\nAs always, if you have any questions, you can find me on Zulip.\n","date":"2 December 2025","externalUrl":null,"permalink":"/blog/2025-12-02-the-first-10k-blocks/","section":"Blog","summary":"\u003cp\u003eIn the previous update I shared that the block production started to work on the beacon chain. Well, it did produce the\nfirst block, but not that many more, for a variety of reasons. But majority of those reasons are not fixed, so I\u0026rsquo;m happy\nto share what they were and where things are at today.\u003c/p\u003e","title":"The first 10k blocks","type":"blog"},{"content":"It has been a month since the last update, and I finally have more exciting news to share here. I received feedback previously that grinding on the same topic is not particularly interesting, so I decided to wait for something different to happen, and it finally did, we\u0026rsquo;ve got the first block on the beacon chain!\nDependencies # It has been a relatively long process to figure out all the data structures and components before the node can be put together and a farmer can connect to it. On the node side, both block production and import pipelines need to exist, as well as RPC server for a farmer to connect to.\nBasic block data structures and consensus pieces (although subtly broken) were in place for some time already. The bigger problem was farmer, which both depended on Subspace networking stack and CUDA/ROCm-based GPU plotting. That GPU-based plotting is what I spent the bulk of my time on last month.\nGPU plotting performance # While I shared that things worked during the last update, performance was not great. I\u0026rsquo;m happy to report that with Mesa RADV I was finally able to capture some profiling traces and figure out some bottlenecks. Now the performance matches the CUDA/ROCm version in Subspace with lower VRAM and shared memory usage and support for a wide range of GPUs.\nThe biggest one turned out to be vector register pressure with almost all shaders and looked something like this:\nAs you can see, the shader which generates the second table used 96 vector registers on AMD RX 7600 XT GPU, and that was limiting GPU occupancy to 5/16 available wavefronts. While not always a problem, it definitely was in this particular case.\nAfter tinkering with algorithms and rewriting the shaders I ended up with a profile that looks substantially different:\nAs you can see, both relative time improved and shaders are no longer limited by vector registers pressure. There are likely other constraints still present, but I am not advanced enough in GPU programming to deal with that yet, and performance is satisfactory as is.\nLet me briefly share some optimizations I have done to achieve that.\nRmap # Rmap, as a reminder, is the data structure that allows to quickly find matches into the right table. It was a source of optimizations before, and turned out not the last time. The major problems with it were that it was constructed sequentially and was large, but frequently used data structure. The sequential nature was caused by the fact that there are duplicated Y values in tables, and exactly first two should be used in the sorted order.\nI tried solving the sequential nature of its of it in a few different ways. Each attempt was more successful than the next, but also substantially different, and the ultimate solution is unlike anything before it. And I\u0026rsquo;m skipping complete, functional, but ultimately not fruitful results, which I had a few of.\nThe first attempt was in PR 430, which added additional sorting steps before matching tables, such that it is relatively easy to handle duplicates with subgroup operations. Then a piece of additional information is attached to each R value, such that Rmap construction can be reduced to a bunch of concurrent atomic \u0026ldquo;or\u0026rdquo; operations. I used R instead of Y since we know what bucket the value belongs to and with R value being in the range of 0..15113 there were unused bits to store extra information \u0026ldquo;for free\u0026rdquo; (in terms of memory usage).\nThis worked better than the sequential version, but there were two additional sorts involved, which takes time, and the preparation overall was still sequential, even though threads within subgroup cooperated to share values with each other.\nI then looked into parallel preparation. In PR 431 I parallelized the preparation step, such that all threads are doing something useful at every step. I had a few more variations of this approach before and after, but they were not better than this one.\nThis worked slightly better, but the complexity of the preparation step was quite high. Not to mention dreaded register pressure. And something I kind of accepted at that point was that Rmap was still too large to fit into shared memory on small iGPUs like one found on Raspberry PI 5, so it had to use global memory there. I don\u0026rsquo;t think it\u0026rsquo;d be too fast there. So overall I was still unhappy with the result and was looking for alternatives.\nAnd I did implement a drastically different alternative in PR 435. I removed extra sorting steps and stopped storing positions of R/Y values in Rmap. Instead, for each R I only reserve two bits, which indicate whether there was a value present and if so, whether there was a second duplicate found or not. This is the kind of information that can be efficiently constructed fully in parallel using atomic operations. This is then used to find matches. And only when doing compute_fn step for the matches that were found, a full scan of the right bucket is performed to find the actual position of R value we\u0026rsquo;re dealing with.\nThis feels quite brilliant to me, and I\u0026rsquo;m wondering why it took me so long to arrive with this simple and elegant design. Not only it is much simpler, the fact that match doesn\u0026rsquo;t store position of R values allowed me to smash all the necessary information about the match into a single u32 value: 9 bits for offset of the left value within a bucket, 6 bits for m value, 14 bits for R target into the right table, and 1 bit to indicate whether the first or second duplicate was used. The offset in the left table was not needed strictly speaking, but having it allowed to further parallelize search for matches with quick sorting of matches afterward (initially implemented in [PR 433]).\nWith that my fighting with Rmap was over (at least for now) and it became so small (2 bits per R value instead of 9), it now fits nicely on smallest iGPUs in shared memory too!\nSorting # The bitonic sorting I originally implemented within subgroup registers seemed really nice and elegant at first, I thought it\u0026rsquo;d perform really well, but nope! Turns out it was using way too many registers, especially on GPUs with smaller subgroup sizes. Modern AMD GPUs have a subgroup size of 64, but Nvidia and many other GPUs have 32, which doubles the number of registers needed to store the whole bucket.\nEquipped with Radeon GPU Profiler I was finally able to see that. In PR 432 I refactored storting to not only sort the values within shared memory, but also to make the whole workgroup sort the values, rather than subgroups, which made the problem \u0026ldquo;wider,\u0026rdquo; and GPUs liked that. The algorithm became much more compact and easy to reason about too. This is not the first time clever GPU code ended up being slower.\nReducing memory and register usage # A pattern that I became annoyingly familiar with on GPUs is that the memory is scarce (fast memory, like vector registers and shared memory) and I ended up smashing multiple values into the same u32 over and over again. For example, for example, in [PR 433] I was able to combine position and bucket offset to reduce Match data structure from 12 to 8 bytes, which was further reduced to 4 bytes in PR 435 by storing 4 separate values in one u32. The ALU cost to extract those values paid off every single time!\nNoticing that trend, I looked at some of the larger types and noticed metadata. On CPU u128 Rust type is used for metadata, which works nicely, but this type is not supported on GPUs. I had to implement polyfills with u32 and u64 to get something that resembles u128, but turned out I didn\u0026rsquo;t need all 128 bits, at least not yet. Moreover, u64 while is supported by modern GPUs, it required compiling two versions of each shader and after experiments was also using more vector registers than polyfills that use u32.\nWith that knowledge, I removed the second version of the shader in PR 436, implemented generic U32N polyfll instead of U128 in PR 438 and finally switched to U32N\u0026lt;3\u0026gt; for metadata in PR 439, which both helped to reduce register usage and reduced memory usage by metadata by 25%. Hypothetically, it is possible to take advantage of it even more since some tables do not even need three words to store metadata. However, implementing that in a generic way was too much for the current state of const generics in Rust and I abandoned the idea after a few failed attempts.\nProbably one of the most confusing and counter-intuitive sources of register usage today with rust-gpu is loops. It is extremely common to use the to process batches of elements, yet it causes so much trouble!\nFor example, originally the shaders were prepared for odd numbers of workgroups to be dispatched (both larger and smaller than necessary). However, removing that in PR 432 from all shaders made a substantial improvement in register usage, despite it being a single loop.\nA more extreme example of the same was in PR 440, where find_proofs shader was efficiently loading data from global memory using 5 nested loops that each were doing exactly two iterations. Simply having those loops meant that to track their progress, at least 5 u32 registers were needed, despite only storing two bits of information in each. So I smashed them all into a single u32 with a sprinkle of bit shifts. It is not pretty but allowed to achieve full occupancy and improve performance in that particular shader. I wish there was a more readable way to do it, though.\n--- a/crates/farmer/ab-proof-of-space-gpu/src/shader/find_proofs.rs +++ b/crates/farmer/ab-proof-of-space-gpu/src/shader/find_proofs.rs @@ -280,12 +280,14 @@ fn find_proofs_impl\u0026lt;const SUBGROUP_SIZE: u32\u0026gt;( let mut group_left_x_index = subgroup_local_invocation_id * 2; - // TODO: This uses a lot of registers for all the loops and expressions, optimize it further + // `chunk_index` is used to emulate `for _ in 0..2` loops, while using a single variable for + // tracking the progress instead of a separate variable for each loop + let mut chunk_index = 0u32; // Reading positions from table 6 - for table_6_chunk in 0..2 { + loop { let table_6_proof_targets = subgroup_shuffle( table_6_proof_targets, - SUBGROUP_SIZE / 2 * table_6_chunk + subgroup_local_invocation_id / 2, + SUBGROUP_SIZE / 2 * (chunk_index \u0026amp; 1) + subgroup_local_invocation_id / 2, ); @@ -297,10 +299,11 @@ fn find_proofs_impl\u0026lt;const SUBGROUP_SIZE: u32\u0026gt;( // Reading positions from table 5 - for table_5_chunk in 0..2 { + chunk_index \u0026lt;\u0026lt;= 1; + loop { let table_5_proof_targets = subgroup_shuffle( table_5_proof_targets, - SUBGROUP_SIZE / 2 * table_5_chunk + subgroup_local_invocation_id / 2, + SUBGROUP_SIZE / 2 * (chunk_index \u0026amp; 1) + subgroup_local_invocation_id / 2, ); @@ -312,10 +315,11 @@ fn find_proofs_impl\u0026lt;const SUBGROUP_SIZE: u32\u0026gt;( // Reading positions from table 4 - for table_4_chunk in 0..2 { + chunk_index \u0026lt;\u0026lt;= 1; + loop { let table_4_proof_targets = subgroup_shuffle( table_4_proof_targets, - SUBGROUP_SIZE / 2 * table_4_chunk + subgroup_local_invocation_id / 2, + SUBGROUP_SIZE / 2 * (chunk_index \u0026amp; 1) + subgroup_local_invocation_id / 2, ); @@ -327,10 +331,11 @@ fn find_proofs_impl\u0026lt;const SUBGROUP_SIZE: u32\u0026gt;( // Reading positions from table 3 - for table_3_chunk in 0..2 { + chunk_index \u0026lt;\u0026lt;= 1; + loop { let table_3_proof_targets = subgroup_shuffle( table_3_proof_targets, - SUBGROUP_SIZE / 2 * table_3_chunk + subgroup_local_invocation_id / 2, + SUBGROUP_SIZE / 2 * (chunk_index \u0026amp; 1) + subgroup_local_invocation_id / 2, ); @@ -342,10 +347,12 @@ fn find_proofs_impl\u0026lt;const SUBGROUP_SIZE: u32\u0026gt;( // Reading positions from table 2 - for table_2_chunk in 0..2 { + chunk_index \u0026lt;\u0026lt;= 1; + loop { let table_2_proof_targets = subgroup_shuffle( table_2_proof_targets, - SUBGROUP_SIZE / 2 * table_2_chunk + subgroup_local_invocation_id / 2, + SUBGROUP_SIZE / 2 * (chunk_index \u0026amp; 1) + + subgroup_local_invocation_id / 2, ); @@ -439,10 +446,39 @@ fn find_proofs_impl\u0026lt;const SUBGROUP_SIZE: u32\u0026gt;( ); } } + + if chunk_index \u0026amp; 1 == 1 { + break; + } + chunk_index += 1; + } + chunk_index \u0026gt;\u0026gt;= 1; + + if chunk_index \u0026amp; 1 == 1 { + break; } + chunk_index += 1; + } + chunk_index \u0026gt;\u0026gt;= 1; + + if chunk_index \u0026amp; 1 == 1 { + break; } + chunk_index += 1; } + chunk_index \u0026gt;\u0026gt;= 1; + + if chunk_index \u0026amp; 1 == 1 { + break; + } + chunk_index += 1; + } + chunk_index \u0026gt;\u0026gt;= 1; + + if chunk_index \u0026amp; 1 == 1 { + break; } + chunk_index += 1; } } Concurrent GPU plotting # One thing I disliked a lot about Subspace\u0026rsquo;s CUDA/ROCm implementation is that it was using sppark that is written with some bad (IMO) architectural decisions. Not only, it doesn\u0026rsquo;t allow to this day to support CUDA and ROCm in the same process (either CUDA or ROCm needs to be selected at compile time, but not both), it uses global singletons for \u0026ldquo;GPUs.\u0026rdquo; What that means is that it is not possible to instantiate the same GPU multiple times and make it run multiple independent computations concurrently. However, as we discovered experimentally over the years, that is very beneficial for performance. So users had to run multiple instances of the farmer even with a single GPU just to utilize it fully.\nNot being tied to that library and relying on Vulkan instead, make it quite straightforward to implement. In PR 441 I implemented support for multiple concurrent record encodings on the same GPU with the default being 4 for dGPU and 2 for iGPU (seems about right from my experiments). The only complication is that wgpu\u0026rsquo;s APIs currently do not support multiple dispatch queues per GPU, so I had to instantiate multiple \u0026ldquo;devices\u0026rdquo; instead, which is probably slightly less optimal than it would be otherwise, but it works. Finally, no need to open multiple farmers, everything \u0026ldquo;just works\u0026rdquo; out of the box, like it does with CPU plotting and NUMA support there.\nMoving farmer and first block production # Now that GPU plotting is no longer a blocker, I did some preparation by moving crates in PR 437 and PR 444 (mostly networking stack and some primitives). Then in PR 445 ab-farmer was finally created from subspace-farmer.\nHaving a farmer is nice, but the node was not complete, the biggest missing piece on the surface was the lack of RPC server to connect to. I added one in PR 448 alongside a bunch of fixes (some of which were extracted into PR 446) and previously lacking consensus archiving implementation.\nWith that, the first blocks were produced:\n2025-11-23T19:49:38.853648Z INFO ab_node::cli::run: âœŒï¸ Abundance 0.0.1 2025-11-23T19:49:38.853674Z INFO ab_node::cli::run: ðŸ“‹ Chain specification: dev 2025-11-23T19:49:38.853680Z INFO ab_node::cli::run: ðŸ’¾ Database path: /tmp/ab-node-HctAVm 2025-11-23T19:49:38.853961Z INFO ab_node::cli::run: Started farmer RPC server address=127.0.0.1:9944 2025-11-23T19:49:38.853978Z INFO ab_client_archiving::archiving: Not creating object mappings 2025-11-23T19:49:38.853989Z INFO ab_client_archiving::archiving: Starting archiving from genesis 2025-11-23T19:49:38.853995Z INFO ab_client_archiving::archiving: Archiving already produced blocks 0..=0 2025-11-23T19:50:31.609669Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=27 number=1 root=d5217acac57cf25f598630b87ad9b4c4bfdc9b3f41c5d51badf1fed3a052375f pre_seal_hash=ad3d9fbddcbb178ace0f2f3d1a0d1b23dc54deeea726d9cae29e82a20025ee18 2025-11-23T19:50:31.613833Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=1 root=d5217acac57cf25f598630b87ad9b4c4bfdc9b3f41c5d51badf1fed3a052375f 2025-11-23T19:50:46.991342Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=36 number=2 root=37e75ce4794c99d6ede6ae1dbc14fb3ce3fbeaff144b3a2971c6cffb80771d74 pre_seal_hash=f492d7264ca3b05aae49f1a88107b48c6bd9c1ff6ee6a352add7bbd9758fa6fc 2025-11-23T19:50:46.994170Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=2 root=37e75ce4794c99d6ede6ae1dbc14fb3ce3fbeaff144b3a2971c6cffb80771d74 2025-11-23T19:50:50.425401Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=38 number=3 root=5dbbbf2468e60e52edf57ddbe4ced4cefe35b1754cafdc139abba1edd86de973 pre_seal_hash=099d9d2e79bf854d720181482317bbf44920c43ce9e63303128b4b64a16c2a22 2025-11-23T19:50:50.428264Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=3 root=5dbbbf2468e60e52edf57ddbe4ced4cefe35b1754cafdc139abba1edd86de973 2025-11-23T19:50:52.120060Z INFO ab_client_block_authoring::slot_worker: ðŸ”– Built new block slot=39 number=4 root=a3b7e8fa84a33996e80c57bfeff7c9cf4187203f488a6534a611a8ce0ec23d0d pre_seal_hash=f21c74285fa56ffb1c18b5d90e08e87be929bb590107c61cafbce80b5ab32571 2025-11-23T19:50:52.122686Z INFO ab_client_block_import::beacon_chain: ðŸ† Imported block number=4 root=a3b7e8fa84a33996e80c57bfeff7c9cf4187203f488a6534a611a8ce0ec23d0d There is a lot of work ahead now, but having something like this working is an important milestone nevertheless.\nThis is a blockchain node built from scratch, no Substrate or anything like that.\nUpcoming plans # I am aware of some mismatch between GPU and CPU plotter, so I\u0026rsquo;ll have to look into that soonish (GPU is used automatically, so I\u0026rsquo;m now testing it all the time by simply starting a farmer).\nI\u0026rsquo;ll probably do some updates to the node and start tackling countless TODOs everywhere before considering adding support for intermediate and leaf shards into the mix.\nThe major items will be working on the database again, there are too many TODOs there, and there are some architectural changes needed to support different kinds of shards.\nThe networking stack is something I\u0026rsquo;m not particularly happy with. I\u0026rsquo;m not even 100% sure about going with libp2p anymore, its APIs are quite cumbersome to use, and I have a persistent feeling that there must be a better way.\nAnd I\u0026rsquo;ve been doing occasional research about RISC-V interpreter/VM design. I have more answers for myself now, but ideally, I\u0026rsquo;m looking for someone with experience to work on this for not a lot of money, so if you know someone who might be interested, please let me know.\nWith that, the post is long enough as is. I\u0026rsquo;ll plan for more frequent updates again in the future, a month was the longest gap between posts so far. In case you are curious about something sooner, Zulip is a good place to ping me.\n","date":"24 November 2025","externalUrl":null,"permalink":"/blog/2025-11-24-the-first-block/","section":"Blog","summary":"\u003cp\u003eIt has been a month since the last update, and I finally have more exciting news to share here. I received feedback\npreviously that grinding on the same topic is not particularly interesting, so I decided to wait for something different\nto happen, and it finally did, we\u0026rsquo;ve got the first block on the beacon chain!\u003c/p\u003e","title":"The first block","type":"blog"},{"content":"All the updates in recent weeks were about Proof-of-Space performance improvements, but what was really driving it is my exploration into how to efficiently implement it for GPU. Today I\u0026rsquo;m happy to announce that an initial version of that implementation is integrated into the farmer.\nI\u0026rsquo;ve tested it on both AMD and Nvidia GPUs, but in principle it should work on any Vulkan 1.2-capable GPU, which includes both discrete and integrated graphics from something like the last decade or so. It will also run on Apple Silicon Macs (actually tested in CI) and likely older Macs with Intel/AMD GPUs as well, though I didn\u0026rsquo;t bother verifying it myself.\nThe fact that it runs doesn\u0026rsquo;t necessarily mean it is fast, though, so the bulk of this post will be about that.\nCurrent performance level # I did some quick testing, and performance is not great right now, CUDA/ROCm version that Supranational engineers wrote for Subspace is currently much faster. I did not do any performance benchmarking or investigation yet, and honestly, it is not a very high priority, but I\u0026rsquo;ll likely experiment with it some soon. Overall, I\u0026rsquo;d say it is probably ~5x slower than the Subspace version today.\nI know of a bunch of reasons for it to be the case already, and there are likely many more that I do not know about yet.\nOne thing that the Subspace version does is erasure coding on the GPU. While erasure coding is way faster without KZG, it still takes time and is currently implemented sequentially in between creation of proofs on the GPU. This both means that GPU is idling in the meantime and CPU cores are not utilized properly. Erasure coding could have happened on the GPU, though, but someone (likely me) needs to write the code for that.\nAnother thing is that the proofs search is currently implemented in a way that processed all s-buckets regardless of whether there are proofs at a particular s-bucket or not, although only half of them are needed. Not only that, the proof as such is actually not needed for plotting either, only its hash is. But the hashes are generated on the GPU too, pausing GPU work and not even leveraging SIMD fully. In fact, the whole encoding could have happened on the GPU completely, transferring encoded records to the CPU instead of proofs themselves.\nAlso, the whole design is such that it tries to leverage the width of the GPU, but the workload is actually quite small, which probably doesn\u0026rsquo;t fully benefit from the available memory bandwidth. There are a few ways to address this.\nWhat users did with Subspace is simply running multiple instances of the plotter to provide GPU with more opportunities to hide memory latency. This was necessary because the API was written in a way that made it impossible to do within a single process. But with the new design it is possible, just not taken advantage of yet.\nAnother alternative is to fuse the whole plotting pipeline and let each group of 1024 threads (on modern GPUs) process a single record, with multiple records being processed concurrently. This will use substantially more memory but will likely be much faster as well since there will be a single fused shader with no global synchronization needed between processing stages.\nThere might be also opportunities to optimize the code by removing unnecessary bounds checks, which is one of many limitations with rust-gpu that prevents me from writing idiomatic Rust code.\nSo in general there are many already known ways to improve performance before even during any profiling, which I didn\u0026rsquo;t do either yet.\nThe key milestone # But the key milestone here is that the code is written, it does run on the GPU, and it does run correctly. I believe it is a correct design for performance from an architecture point of view, but more engineering is needed to get it to actually run fast. PR 425 is where integration into the farmer happened, I will not mention countless PRs that implemented various individual shaders before that.\nUpcoming plans # This work should unlock moving the farmer crate into the main codebase under crates.\nI\u0026rsquo;ll probably spend some more time profiling low-hanging fruits since GPU programming is new for me, but the next major step is to get a basic beacon chain block production locally as mentioned in the previous update.\nZulip has been quiet for a while, but I\u0026rsquo;m there if you have anything to discuss ðŸ˜‰\n","date":"23 October 2025","externalUrl":null,"permalink":"/blog/2025-10-23-gpu-plotting-works/","section":"Blog","summary":"\u003cp\u003eAll the updates in recent weeks were about Proof-of-Space performance improvements, but what was really driving it is my\nexploration into how to efficiently implement it for GPU. Today I\u0026rsquo;m happy to announce that an initial version of that\nimplementation is integrated into the farmer.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;ve tested it on both AMD and Nvidia GPUs, but in principle it should work on any Vulkan 1.2-capable GPU, which\nincludes both discrete and integrated graphics from something like the last decade or so. It will also run on Apple\nSilicon Macs (actually tested in CI) and likely older Macs with Intel/AMD GPUs as well, though I didn\u0026rsquo;t bother verifying\nit myself.\u003c/p\u003e\n\u003cp\u003eThe fact that it runs doesn\u0026rsquo;t necessarily mean it is fast, though, so the bulk of this post will be about that.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg alt=\"Screenshot of nvtop CLI with a farmer process in it\" src=\"nvtop-gpu-plotting.png\"\u003e\n\u003c/p\u003e","title":"GPU plotting works!","type":"blog"},{"content":"It has been a couple of weeks since the last status update about performance improvements in Proof-of-Space and I am finally at a decent stopping point where all architectural changes are done and I can share them with you.\nMatching performance optimizations # In a long series of observations, I noticed that when matching proofs, we always need both y values and its position, which were previously stored separately. Combining them into a tuple with some extra cache line alignment for some data structures in PR 393 resulted in yet another substantial performance improvement, both for table generation and even proof searching:\nBefore: chia/table/single/1x time: [702.17 ms 707.33 ms 712.59 ms] thrpt: [1.4033 elem/s 1.4138 elem/s 1.4242 elem/s] chia/table/parallel/8x time: [502.50 ms 505.77 ms 509.35 ms] thrpt: [15.706 elem/s 15.818 elem/s 15.920 elem/s] chia/proof/missing time: [107.29 ns 108.80 ns 111.58 ns] thrpt: [8.9618 Melem/s 9.1915 Melem/s 9.3207 Melem/s] chia/proof/present time: [374.03 ns 375.88 ns 377.16 ns] thrpt: [2.6514 Melem/s 2.6604 Melem/s 2.6736 Melem/s] After: chia/table/single/1x time: [638.21 ms 642.97 ms 649.77 ms] thrpt: [1.5390 elem/s 1.5553 elem/s 1.5669 elem/s] chia/table/parallel/8x time: [426.94 ms 431.49 ms 436.70 ms] thrpt: [18.319 elem/s 18.540 elem/s 18.738 elem/s] chia/proof/missing time: [76.806 ns 76.913 ns 77.038 ns] thrpt: [12.981 Melem/s 13.002 Melem/s 13.020 Melem/s] chia/proof/present time: [351.19 ns 352.22 ns 354.05 ns] thrpt: [2.8245 Melem/s 2.8391 Melem/s 2.8474 Melem/s] Proof searching and verification improvements # In PR 394 I then further reduced memory usage and simplified public API that allowed to reuse LeftTargets across all table generation instances, which helps with higher-level CPU cache utilization. As a bonus this PR removes heap usage from proof verification and fixes (previously broken) alloc feature for a measurable verification performance improvement:\nBefore: chia/verification time: [7.0819 Âµs 7.0908 Âµs 7.0961 Âµs] thrpt: [140.92 Kelem/s 141.03 Kelem/s 141.21 Kelem/s] After: chia/verification time: [6.4624 Âµs 6.4689 Âµs 6.4758 Âµs] thrpt: [154.42 Kelem/s 154.59 Kelem/s 154.74 Kelem/s] Proofs API simplification # I mentioned in one of the previous updates that number of matches for a pair of buckets was limited to 288 (a multiple of 32, important for GPUs), and the number of elements per bucket of y values was limited to 272 (a multiple of 16, important for CPU). The idea behind those numbers is that they were supposed to be the smallest numbers that still provide enough proofs for sector encoding. I came up with those numbers after a series of empirical experiments, but didn\u0026rsquo;t have any guarantees that the assumption will hold.\nTurns out, it was crucial for further simplification. Farmer since at that time the farmer code was ready for an insufficient number of proofs for a sector, which complicates the logic in multiple places, especially when GPU implementation is involved. In PR 408 I added a test case (it was much harder to write a const fn function that prevents compilation) to ensure that the assumption always holds and removed support for unencoded sector chunks from the farmer.\nWith that, it was possible to simplify proof handling. PR 410 introduced and started using a new API that instead of generating tables that can be queried later, generates a sufficient number of proofs and a bitmap with which proofs are present. The bitmap is exactly the same as the one farmer was already generating internally, so a bit of compute is saved there. The proofs themselves are also easier to handle, especially during piece decoding.\nAfter looking for so long at the way proofs are found, I also noted that the current design is somewhat inspired by the older architecture. In the older architecture the tables were sorted by y values. In Chia this is important. However, we\u0026rsquo;re only interested in a single proof per challenge, so PR 406 changed the way s-buckets are converted into the challenges that Chia deals with to optimize the search (breaking change compared to Subspace implementation), and then PR 411 refactored internals to only retain a single proof target per s-bucket instead of the complete last seventh table. With the above changes, we can generate the tables and find the proofs after than the earlier split process of generating full tables and then finding proofs one by one:\nBefore: chia/proofs/single/1x time: [728.10 ms 735.58 ms 744.74 ms] thrpt: [1.3427 elem/s 1.3595 elem/s 1.3734 elem/s] chia/proofs/parallel/8x time: [600.14 ms 604.93 ms 609.18 ms] thrpt: [13.132 elem/s 13.225 elem/s 13.330 elem/s] After: chia/proofs/single/1x time: [710.02 ms 713.94 ms 718.19 ms] thrpt: [1.3924 elem/s 1.4007 elem/s 1.4084 elem/s] chia/proofs/parallel/8x time: [567.42 ms 574.66 ms 581.51 ms] thrpt: [13.757 elem/s 13.921 elem/s 14.099 elem/s] Note that while the numbers are a bit higher than before, this is not just table generation anymore, this is the time to find all the proofs the farmer will need for record encoding. And now since the API is reduced in scope, more optimizations are possible (though not implemented yet). For example, sector encoding is actually done with a hash of the proof. Since we know we\u0026rsquo;ll have exactly 2^15 proofs, we can use BLAKE3 SIMD to hash them much faster and return hashed proofs to the caller, which is even less RAM and much faster. Overall, there are several new optimization opportunities remaining unimplemented. A substantial chunk of the logic is actually sequential there for now, and yet it is much faster than before I started all these optimizations.\nGPU implementation # A lot of the changes above were driven by observations of what is actually needed for the protocol and what is an implementation design decision and can be changed. Some things were more efficiently implementable on GPU when design is changed slightly, and it turns out most of those changes benefit CPU as well. It is really beneficial to know both the theoretical needs of the protocol and the nuances of what can be efficiently implemented for CPU/GPU at the same time.\nOverall, I have managed to implement all the pieces needed for plotting on the GPU.\nPR 395 implemented a shader for sorting individual buckets. CPU produces deterministic order due to single-thread implementation (parallelization is not worth it there), but it is too costly on the GPU, so everything is placed in arbitrary order and sorted after the fact. Since the number of matches is small and has a hard upper-bound of less than 512, I ended up using bitonic sort and modified it in PR 396 to only use registers, which I think is a fairly neat implementation and should shine on a GPU.\nTo optimize the memory bandwidth, I fused matching shader and compute_fn shaders together in PR 401, then similarly fused chacha8 and compute_f1 shaders into one as well in PR 402.\nI then introduced find_matches_and_compute_last variant in PR 414, which instead of grouping entries by buckets, stores proof targets like I described above. However, in contrast to the CPU version that is sequential and picks the first entry per s-bucket, this one had to also estimate an upper-bound for number of elements per s-buckets and reduce them to a single one at a later stage.\nAnd with all of those, PR 415 finally implemented find_proofs shader, which looks at those s-bucket elements, reduces each to a single one and finally generates proofs. It doesn\u0026rsquo;t hash the proofs yet, which, as I mentioned above, would be a nice performance improvement. It also processes even entries that do not have proofs, which wastes some amount of compute that could have been used better. But those are optimizations for another time.\nUpcoming plans # Now all the primitives necessary for GPU plotting are present, what\u0026rsquo;s left is to combine those shaders into a single pipeline. Should not be too hard and GPU plotting will be ready.\nWith that I should be able to move the farmer and implement a version of local beacon chain block production shortly afterward. And after that I\u0026rsquo;ll probably get back to the database, it needs more work with upper-bound estimation to be able to run multiple shards in the same physical file/disk, which is crucial for the introduction of intermediate and leaf shards.\nHopefully future updates will be more entertaining, in the meantime you can find me on Zulip.\n","date":"14 October 2025","externalUrl":null,"permalink":"/blog/2025-10-14-faster-proof-of-space-part-4/","section":"Blog","summary":"\u003cp\u003eIt has been a couple of weeks since the last status update about performance improvements in Proof-of-Space and I am\nfinally at a decent stopping point where all architectural changes are done and I can share them with you.\u003c/p\u003e","title":"Faster Proof-of-Space (part 4)","type":"blog"},{"content":"This is just a short note about something that was not possible to do as cleanly before.\nIf you worked with GitHub Actions for a meaningful period of time, and especially for testing Rust code, you will know that Windows runners are really slow. They are easily the bottleneck in many workflows and until recently there was no good way around it without turning the whole workflow into a mess. Thankfully, GitHub Actions recently introduced support for Yaml anchors in workflow files, which allows to reuse parts of the workflow instead of copy-pasting them.\nFor context, I had a job definition that looked something like this:\ncargo-test: strategy: matrix: os: - ubuntu-24.04 - ubuntu-24.04-arm - macos-15 - windows-2025 miri: - true - false type: - together - features - guest-feature exclude: - os: macos-15 type: guest-feature - os: windows-2025 type: guest-feature runs-on: ${{ matrix.os }} steps: # Many steps here It is testing code on several operating systems, with/without Miri and already split into several parts to parallelize the tests, but that was still kind of slow.\nThe slowest permutation out of the above are together type that compiles all tests with default features together and runs them all. Other variants test certain things more selectively and are significantly shorter. On Windows specifically, it was usually taking over 17 minutes, but it is relatively slow on other operating systems as well.\nSo we can\u0026rsquo;t run CI faster than 17 minutes, but the whole workflow actually took 20 to 21 minutes in practice. This is because there are limits on the number of free runners given, and it just so happened that the slow job on Windows was starting after some of the faster runs were complete (completely undeterministic process though).\nThe way to fix it would be to make sure these slower jobs start first, and then faster jobs start later in whatever order they like since they are likely to complete long before the slowest job anyway. GitHub Actions supports job dependencies, so I came up with this small helper job:\ncargo-test-slow-head-start: runs-on: ubuntu-24.04 steps: - name: Artificial delay run: sleep 5 And then wanted to add it to dependencies for non-Windows jobs to give the Windows job a head start:\nruns-on: ${{ matrix.os }} needs: ${{ contains(matrix.os, \u0026#39;windows\u0026#39;) \u0026amp;\u0026amp; fromJSON(\u0026#39;[]\u0026#39;) || \u0026#39;cargo-test-slow-head-start\u0026#39; }} And\u0026hellip; that didn\u0026rsquo;t work.\nThis would have been a clean solution, but unfortunately, it is not supported yet. Since I was not looking forward to duplicating the large steps section into a separate Windows-specific job, I shelved the idea for a few months.\nSolution # But last week GitHub Actions introduced support for Yaml anchors, so I could annotate anything in the workflow file and reference it later without copying. Here is how it looks now:\ncargo-test: strategy: matrix: os: - ubuntu-24.04 - ubuntu-24.04-arm - macos-15 - windows-2025 miri: - true - false type: # `together` variant is running in a separate job, see `cargo-test-slow` # - together - features - guest-feature exclude: - os: macos-15 type: guest-feature - os: windows-2025 type: guest-feature runs-on: ${{ matrix.os }} # Gives the slow cargo test jobs a head start needs: cargo-test-slow-head-start env: command: ${{ matrix.miri == true \u0026amp;\u0026amp; \u0026#39;miri nextest run\u0026#39; || \u0026#39;nextest run\u0026#39; }} steps: \u0026amp;test-steps # Many steps here cargo-test-slow: strategy: matrix: os: - ubuntu-24.04 - ubuntu-24.04-arm - macos-15 - windows-2025 miri: - true - false type: - together # - features # - guest-feature exclude: - os: macos-15 type: guest-feature - os: windows-2025 type: guest-feature runs-on: ${{ matrix.os }} env: command: ${{ matrix.miri == true \u0026amp;\u0026amp; \u0026#39;miri nextest run\u0026#39; || \u0026#39;nextest run\u0026#39; }} steps: *test-steps I copied the matrix definition with everything except together in the original job and just together in the new cargo-test-slow job, while reusing the same exact steps, which can be maintained like before. Note \u0026amp;test-steps anchor and *test-steps reference to it.\nNow my slower test jobs start 5+ seconds earlier, which is short enough to not delay the rest of the CI too much and long enough to give the slow job a head start in scheduling.\nResults # The results are awesome! With Windows together job taking 17m10s, the whole CI run took 17m20s. So basically I get the whole CI run for the time of the slowest one:\nI hope you find this trick useful, PR 397 is where this was done and where you can find the final diff with the changes described here.\nBonus content # You might be wondering: how the heck did I get that full-screen CI visualization? Well, since my feature request wasn\u0026rsquo;t implemented yet, I added a small user style to my browser to fix the annoyance and take advantage of the screen space in my possession. With it, the visualization will occupy the full vertical space available in the browser window instead of being limited to the miserable 600 pixels.\n","date":"25 September 2025","externalUrl":null,"permalink":"/blog/2025-09-25-shorter-github-actions-runs/","section":"Blog","summary":"\u003cp\u003eThis is just a short note about something that was not possible to do as cleanly before.\u003c/p\u003e\n\u003cp\u003eIf you worked with GitHub Actions for a meaningful period of time, and especially for testing Rust code, you will know\nthat Windows runners are really slow. They are easily the bottleneck in many workflows and until recently there was no\ngood way around it without turning the whole workflow into a mess. Thankfully, GitHub Actions recently introduced\nsupport for \u003ca\n  href=\"https://github.blog/changelog/2025-09-18-actions-yaml-anchors-and-non-public-workflow-templates/\"\n    target=\"_blank\"\n  \u003eYaml anchors\u003c/a\u003e in workflow files, which allows to reuse parts of the workflow instead of copy-pasting them.\u003c/p\u003e","title":"Shorter GitHub Actions runs","type":"blog"},{"content":"","date":"25 September 2025","externalUrl":null,"permalink":"/tags/tips-and-tricks/","section":"tags","summary":"","title":"tips-and-tricks","type":"tags"},{"content":"This third part has fewer improvements and could have been called \u0026ldquo;Adventures with rust-gpu part 2\u0026rdquo; given how much time I spent wrestling with it.\nRmap optimizations # Since the last update I was mostly focusing on GPU implementation for matching logic. As it often happens, I\u0026rsquo;m re-reading and re-auditing related CPU code in the process, trying to figure out what would be an efficient way to implement it. And I discovered one more optimization that turned out to be applicable to both CPU and GPU.\nI really want to support as many usable GPUs as possible, which in turn means thinking about their constraints. One of the constraints is the amount of shared memory used. Turned out that Rmap that was essentially [[u32; 2]; 15113] data structure was quite big, almost 121 kiB big! It doesn\u0026rsquo;t fit into the 32 kiB limit that I was targeting, it doesn\u0026rsquo;t even fit into many modern consumer GPUs with 48-64 kiB of shared memory! BTW, it turned out the baseline for Vulkan is actually 16 kiB and that is the amount Raspberry PI 5s iGPU has. Using global memory means a massive performance hit, and even if it could work, if I want to maximize GPU utilization by running more workgroups, I better use less shared memory than more.\nThe observation with Rmap data structure is that it is sparse. Not only it has most of the slots empty (remember, in the previous post I mentioned that the bucket size is now limited to just 272 elements), even those that are occupied usually have 1 element, not two. So a lot of space is wasted. What can be done about that is to create an indirection: Rmap table will store pointers into a different table, which only needs to be [[u32; 2] 272] (again, at least half of it will be empty, but dealing with that is not worth it). Since the second table is much smaller, pointers don\u0026rsquo;t need to occupy 8 bytes, in fact, 9 bits is sufficient, and that is what GPU implementation uses. For CPU additional arithmetic operations were not worth it, so it uses u16 for pointers instead, while still enjoying massive data structure size reduction.\nAs a result, we get the following change in CPU performance, which landed in PR 385:\nBefore: chia/table/parallel/8x time: [529.15 ms 534.42 ms 540.15 ms] thrpt: [14.811 elem/s 14.969 elem/s 15.119 elem/s] After: chia/table/parallel/8x time: [518.48 ms 521.57 ms 525.13 ms] thrpt: [15.234 elem/s 15.338 elem/s 15.430 elem/s] It is not particularly huge difference, but it is certainly consistently faster and has a much higher chance of remaining in L1 cache during processing.\nOn GPU using 9 bits for pointers plus an additional table with actual values occupies ~19 kiB of shared memory vs 121 kiB before. This still doesn\u0026rsquo;t fit into 16 kiB on Raspberry PI 5, so the Rmap table there will have to be moved to global memory and hoping that it is cached, but at least everything else fits nicely with some room to spare.\nOther CPU improvements # I was also exploring some other things and noticed that manually unrolling in finding matches was actually worse in the current state of the library, so I removed that in PR 388, which further improved performance substantially:\nBefore: chia/table/single/1x time: [747.82 ms 756.94 ms 768.09 ms] thrpt: [1.3019 elem/s 1.3211 elem/s 1.3372 elem/s] chia/table/parallel/8x time: [518.48 ms 521.57 ms 525.13 ms] thrpt: [15.234 elem/s 15.338 elem/s 15.430 elem/s] After: chia/table/single/1x time: [697.77 ms 707.91 ms 723.34 ms] thrpt: [1.3825 elem/s 1.4126 elem/s 1.4331 elem/s] chia/table/parallel/8x time: [500.34 ms 506.86 ms 513.82 ms] thrpt: [15.570 elem/s 15.783 elem/s 15.989 elem/s] Finding matches on GPU # Finding matches on GPU was the first kernel that was not embarrassingly parallel and required some level of synchronization. The reason for it is that most of the candidate pairs do not have matches and those that do need to be compressed and be in the same deterministic order as on the CPU. Not only that, I later discovered that the whole code needs to progress in phases or else the results differ in unexplainable ways and depend on GPU vendor/implementation.\nIt does still waste a bit more time on divergent control flow than I\u0026rsquo;d like, but we\u0026rsquo;ll see how it performs once the whole workflow is complete.\nSince Raspberry PI 5 and baseline Vulkan requirements more generally are lower than modern-ish consumer dGPUs, I decided to evolve existing compilation of two kernels with and without Int64 (64-bit integer) support. Since PR 389 the shader variants are \u0026ldquo;modern\u0026rdquo; and \u0026ldquo;fallback.\u0026rdquo; Modern kernel supports Int64 and 32 kiB+ of shared memory, while fallback will theoretically run on anything compliant with Vulkan 1.2+, which is most dGPUs in the last decade or so and even iGPUs. I checked one of the laptops I have with Intel HD Graphics 620 iGPU and even that one should work.\nMaking it compile with rust-gpu # While I was writing the code, I used cargo clippy and cargo build for verification, but since I did not have the shader definition initially, I didn\u0026rsquo;t yet know if it would compile for rust-gpu. And it did not compile in a big way with over 60 errors.\nThe root of all evil, or at least the most of it, was rust-gpu issue 241. The thing is that I used arrays for many data structures since I know the upper bound on everything and want to store things compactly. However, I most often have dynamic indices into those arrays. The way a Rust standard library implements both checked and unchecked indexing is using Deref of an array to a slice, but rust-gpu doesn\u0026rsquo;t allow casing *[u32; N] into *[u32], which effectively means I can\u0026rsquo;t use most of the methods, I can\u0026rsquo;t even use iterators.\nSo for iterators I had to do manual indexing. For unchecked access just use [] without the ability to explain to the compiler that it doesn\u0026rsquo;t need to do bounds checks. And for checked access I have to first check the index against length of the array explicitly and then use []. As you can imagine, this resulted in a lot of ugly boilerplate that is much harder to read. Not only that, I had to give up some of the new types because I couldn\u0026rsquo;t cast new types to/from their inner types (for those that were backed by reusable scratch space in the shared memory).\nAll in all, that wasn\u0026rsquo;t a great experience at all, and I hope it\u0026rsquo;ll improve soon.\nMaking it run properly # Once the code was compiled, it didn\u0026rsquo;t run properly (surprise!), and I had to figure out why. This turned out to be much more challenging than I expected. Turns out there is no direct way to know if shared executed successfully or not. If code hits a panic, it just exits execution early, leaving the memory with whatever garbage it may have had during allocation or with whatever incomplete results it managed to produce so far.\nThe only real way to know if it completed successfully is to write some value at the very end explicitly and check that manually. Not only that, once you know that the code has likely panicked, it is surprisingly challenging to figure out both where and why. No step by step debugger like on CPU for us, not even println!() is available easily unless you implement it yourself. And when you try to implement it yourself, it is most likely that you\u0026rsquo;ll hit the mentioned pointer casting issue again. Quite a frustrating experience overall, there must be a better way eventually if we want people to write shaders in Rust.\nThe complete version of find_matches_in_buckets shader with tests landed in PR 391.\nIt does run # In the end I managed to make it work, tested both on AMD GPU and with llvmpipe on CPU (which is the one used in CI). Both produce results that are identical to CPU.\nUpcoming plans # With matching logic implemented, the only remaining difficult thing is bucketing. I think I\u0026rsquo;ll be changing the logic a bit again, and I think it\u0026rsquo;ll improve performance on CPU too. Once bucketing is implemented, the remaining thing will be to do proof searching (mostly embarrassingly parallel task) and applying them to record chunks in a sector. Neither of these two is remotely as complex as bucketing, though.\nIt\u0026rsquo;ll be a bonus to implement erasure coding on GPU, which will help with both plotting and archiving (which is already very fast, especially compared to the original KZG-based version in Subspace). But that is an optional thing for now. If you\u0026rsquo;re reading this and would like to give it a try, let me know!\nI\u0026rsquo;ll write once I have more updates to share. In the meantime you can always find me on Zulip.\n","date":"18 September 2025","externalUrl":null,"permalink":"/blog/2025-09-18-faster-proof-of-space-part-3/","section":"Blog","summary":"\u003cp\u003eThis third part has fewer improvements and could have been called \u0026ldquo;\u003ca\n  href=\"../2025-07-02-adventures-with-rust-gpu\"\u003eAdventures with rust-gpu\u003c/a\u003e part 2\u0026rdquo; given how much\ntime I spent wrestling with it.\u003c/p\u003e","title":"Faster Proof-of-Space (part 3)","type":"blog"},{"content":"In the part 1 I shared some background information, performance improvements and future opportunities. Since then, I was pursuing various approaches. Some worked out nicely, others were not so fruitful. Overall, I have achieved a substantial performance improvement on CPU with a few more options still remaining on the table, all while becoming substantially more GPU-friendly.\nSpecification # Reading upstream Chia documentation to understand how things should be implemented is challenging, so there is a Subspace specification that describes things in a much clearer and more approachable way. Well, at least in theory it does that. In practice, though, it confuses the specification of what needs to be done with an efficient implementation strategy. And at least for our use cases, it just happened to be the case that the implementation strategy assumed there wasn\u0026rsquo;t the best one.\nWhat we\u0026rsquo;re essentially doing is creating a bunch of tables and searching for matches between them. Yes, we can sort the tables, but that is, strictly speaking, not necessary. The only thing we actually need is buckets that represent ranges of y values, but the order inside doesn\u0026rsquo;t really matter and never surfaces in the proof structure. The only thing affected is proof order, which has already been a bit of an annoyance before.\nIt is easier to create a sequential implementation that is deterministic, it is a bit harder to make one that is fast. And it is even more difficult to create a fast parallel one that behaves the same way on both CPU and GPU.\nNew high-level strategy # I started with the observation that only buckets are really necessary. I already did some statistical analysis mentioned in the previous post to discover that for any K that is relevant, the upper bound for the size of the bucket is 512. Coincidentally, the number of matches for a pair of buckets also has the same upper bound.\nWith this knowledge we can do two major implementation changes:\ninstead of sorting tables by y value, only assign y values to buckets (by dividing them by PARAM_BC = 15113) when searching for matches in pairs of buckets, store the results in the pre-allocated bucket-sized allocation This results in a few things that are good for performance:\nresults of matches no longer need to be concatenated and sorted, they can stay where they were written originally when the match was found and parallel version can easily achieve the same deterministic order as sequential one instead of doing a bunch of memory copies and potential allocations (depending on the sorting algorithm), we do a single scan to assign buckets to y values While bucketing requires a lot of random writes, the small total size of the data appears to be quite friendly for CPU caches and performs reasonably well.\nI even tried to use SIMD for bucketing, but faced a problem, which was one of the promising potential optimizations I mentioned last time.\nHandling of y duplicates and size bounds # One of the challenges with SIMD bucketing is the fact that y values can be duplicated. The behavior of the scatter operation is that only the last write will be observed. On the surface this is not a problem, but it does impact the number of matches found down the line and the number of proofs that can be found as the result. After doing experiments, it turned out that this decreases the number of proofs found too much. By too much I mean that it would not be possible to make plotted sector contain only the chunks which can be farmed, which is undesirable.\nIf duplicates are no-go, how many matches do we actually need and how small can we make the buckets? I did many more experiments and came up with numbers 288 (a multiple of 32) and 272 (a multiple of 8) for both. These appear to be large enough for plenty of proofs to be available, while also being substantially smaller than 512, which reduces memory usage and significantly increases performance. These smaller sizes should also make it more likely that the data will fit in shared memory on more (all?) GPUs.\nIt is really important to know what you\u0026rsquo;re doing and why to be able to make such decisions.\nWhile getting rid of duplicates results in not enough proofs, it doesn\u0026rsquo;t mean any number of duplicates needs to be supported. This is especially important for GPU implementation, where diverging control flow kills the performance. After extensive testing I concluded that supporting just a single duplicate is sufficient. This is also fast on CPU since handling of the second duplicate is only needed when the first duplicate exists. And most of the time there are no matches at all, so the CPU can have a decent chance at successfully predicting branches, and GPU threads will mostly progress without divergence.\nFinding proofs # One place where sorted y values are really beneficial is finding proofs. This process involves finding y matching the challenge and propagating back through the tables to generate a proof. But if there are only buckets, this doesn\u0026rsquo;t work. Another option is to do full scan and since there are buckets, only buckets that overlap with matching range need to be scanned.\nThis is substantially slower than binary search, especially when proof doesn\u0026rsquo;t exist. But on the flip side the number of proofs that need to be checked is upper bound by \\(2^{16}\\), significantly smaller in practice, and the case where proof isn\u0026rsquo;t found is the minority. So in the end for this number of searches, the increase in proof search time is overwhelmed with decrease in table construction time, and is a net positive change overall.\nScan is also very CPU-friendly due to predictable behavior (in terms of memory access pattern and branching), so the performance for when proof is found is actually not far off from the binary search in practice. One of many initially counter-intuitive cases where doing strictly more work might end up being faster due to how real-world hardware is designed.\nMemory optimizations # Something I carried in local branches, but didn\u0026rsquo;t really see a massive difference from was pruning the data from parent tables after they are used. More specifically, both metadata and y values are not needed after the next table is constructed since proof generation is only concerned with x values the final y was derived from.\nNow that there are fewer allocations and things fit better and better into CPU cache, pruning metadata and intermediate y values resulted in performance improvements. Moreover, since tables are no longer sorted by y, the position into the first table is the same as x value, so x values are not stored anymore and the whole first table is dropped as soon as the second table is constructed.\nAll these tricks shaved off about 40% of the memory usage!\nI\u0026rsquo;m very curious to see how this impacts the performance on CPUs with 3D V-cache, where the absolute majority if not all the data will stay in cache at all times ðŸ”¥\nResults # Not every hypothetical improvement results in actual performance improvement. Sometimes it really depends on what fits into the cache and what doesn\u0026rsquo;t, so it is not obvious when performance will improve and when it will decrease. So all this took quite a long time with countless benchmark runs and failed experiments.\nThe changes described above and then some were implemented in PR 380 and PR 381.\nThe results are still a bit variable due to my machine not staying idle, but when limited to a single CXX on AMD Threadripper 7970X CPU (roughly equivalent to 8C16T AMD Ryzen 7700X CPU), the results are as follows:\nBefore: chia/table/single/1x time: [920.37 ms 924.24 ms 929.24 ms] thrpt: [1.0762 elem/s 1.0820 elem/s 1.0865 elem/s] chia/table/parallel/8x time: [677.60 ms 684.25 ms 692.06 ms] thrpt: [11.560 elem/s 11.692 elem/s 11.806 elem/s] chia/proof/missing time: [20.764 ns 21.179 ns 21.459 ns] thrpt: [46.600 Melem/s 47.217 Melem/s 48.160 Melem/s] chia/proof/present time: [360.24 ns 360.65 ns 361.08 ns] thrpt: [2.7695 Melem/s 2.7727 Melem/s 2.7760 Melem/s] After: chia/table/single/1x time: [747.82 ms 756.94 ms 768.09 ms] thrpt: [1.3019 elem/s 1.3211 elem/s 1.3372 elem/s] chia/table/parallel/8x time: [529.15 ms 534.42 ms 540.15 ms] thrpt: [14.811 elem/s 14.969 elem/s 15.119 elem/s] chia/proof/missing time: [101.94 ns 102.22 ns 102.58 ns] thrpt: [9.7486 Melem/s 9.7824 Melem/s 9.8099 Melem/s] chia/proof/present time: [376.64 ns 377.52 ns 379.55 ns] thrpt: [2.6347 Melem/s 2.6489 Melem/s 2.6551 Melem/s] As mentioned before, proof searching performance decreased, especially for misses, but it is on average ~4 ms per table, which is more than compensated by the table construction time improvement.\nI still remember the time when we were struggling with table construction for proving. It was taking so long using reference Chia implementation that we had to introduce the delay into the protocol design to make sure farmers have a few seconds to generate a proof in time ðŸ¥¹. Now we casually create tables in 750 ms on a single CPU core and A LOT less than that when multithreaded (which is the default for proving BTW).\nOptimizations that didn\u0026rsquo;t work out # There were many things that didn\u0026rsquo;t work out, but most notably a version with binary search over sorted ys I mentioned last time. Not needing to sort at all was such a massive win and simplification for both CPU and GPU that I don\u0026rsquo;t think I will go back to it, but it took a few days to get working reasonably well. Still, it was an interesting experience I\u0026rsquo;m sure I\u0026rsquo;ll find applications for in the future.\nUpcoming plans # After implementation was pretty well tuned in Subspace I\u0026rsquo;m still finding ways to do major performance improvements, even if it means backwards incompatible changes for farmers. For now, though, I think this is mostly it for the CPU side, and I\u0026rsquo;ll be switching back to GPU where I\u0026rsquo;ll no longer need to implement custom sorting and other complicated things.\nFingers crossed for the next update in this series to talk about how this all works nicely on GPU with rust-gpu.\nIf you have any feedback about this or anything else related to the project, I\u0026rsquo;m not difficult to find on Zulip.\n","date":"7 September 2025","externalUrl":null,"permalink":"/blog/2025-09-07-faster-proof-of-space-part-2/","section":"Blog","summary":"\u003cp\u003eIn the \u003ca\n  href=\"../2025-08-26-faster-proof-of-space-part-1\"\u003epart 1\u003c/a\u003e I shared some background information, performance improvements and future opportunities. Since then, I\nwas pursuing various approaches. Some worked out nicely, others were not so fruitful. Overall, I have achieved a\nsubstantial performance improvement on CPU with a few more options still remaining on the table, all while becoming\nsubstantially more GPU-friendly.\u003c/p\u003e","title":"Faster Proof-of-Space (part 2)","type":"blog"},{"content":"In the last update I shared that I plan to work on GPU plotting some more, so that is what I did. The \u0026ldquo;easier\u0026rdquo; parts of it were done earlier. Now it was time for matching logic and that is more complex, so I decided to dedicate the whole blog post to it.\nHow is Chia PoSpace used in Subspace? # Chia Proof-of-Space is used in Subspace for plotting, but not in the same way as in Chia itself. Since Subspace is a Proof-of-Archival consensus, farmers fundamentally store the history of the blockchain itself, but how do we ensure each farmer stores a unique replica(s) of it? That is exactly where Chia PoSpace comes into play.\nWe generate Chia tables and use its proofs to encode pieces of the blockchain history. Chia has 4 phases in its construction, where the first is to create tables and then three more phases compact the tables to get rid of redundant data. In Subspace only the first phase is needed, which is interesting and has a slightly different set of tradeoffs.\nChia tables construction speedrun # As a quick summary for those who are not familiar with Chia, the first phase involves the creation of 7 tables. The first table is more or less takes a seed as an input and generates y values for each of \\(2^k\\) x value using compute_f1() function (mostly a ChaCha8 stream cipher). Each y value has k+PARAM_EXT (PARAM_EXT = 6) bits. That is the first table, a set of x and y values.\nThe other six tables essentially follow the following process:\ngroup y values into buckets each bucket spans a range of PARAM_BC = PARAM_B * PARAM_C (PARAM_B = 119, PARAM_C = 127, PARAM_BC = 15113) this means the first bucket is y in the range 0..PARAM_BC, the second bucket is PARAM_BC..2*PARAM_BC, etc. take a pair of adjacent buckets (called left and right) and match them for each left y derive the target using a special formula check if there is a matching y in the right table for each match found, compute new y and some additional metadata (x in the first table\u0026rsquo;s metadata) using a special compute_fn() function (includes some bit manipulations and BLAKE3 hashing) Challenges and optimizations # To make the above process faster and more efficient, there are various tricks one can use, which are different for CPUs and GPUs.\nThe targets into the right table are typically precomputed on the CPU, which allows avoiding recomputation of a bunch of multiplications and divisions all the time. To find matches and proofs faster, tables are typically sorted by y, so it is easier to find buckets and y in the ranges that relevant.\nThere are some tricky optimization tricks that are not always intuitive and need to be benchmarked. For example, for k=20 the size of the bucket is ~236 elements on average, despite PARAM_BC potential \u0026ldquo;slots\u0026rdquo; in it (also y values can be present more than once in a bucket). So it turns out, on CPU it is cheaper to copy y values into a PARAM_BC -sized array, so that matches can be found in O(1) time on CPU, but that is not necessarily faster on the GPU.\nThe way you sort things is also important and might be different on CPU and GPU due to inherent architecture differences and differences in optimal memory access and compute patterns.\nWhy now? # You might be wondering why I look into optimizations now? Well, turns out implementing the current design on GPU is quite difficult, and especially difficult (maybe impossible) to do so efficiently. So I was looking for potential changes that might help with that.\nAny proof is valid and can be verified from a consensus point of view. However, from an implementation perspective, we would really like to make sure both CPU and GPU plotter derive tables and proofs that are the same, so both are interchangeable. This allows creating plots and prove on CPU-only machine later. This is why efficient implementation on GPU may require changes to the CPU implementation as well.\nOptimization opportunities # I\u0026rsquo;ve been staring at the code and thinking about it for a better part of the last week. I implemented a version that is closer to the GPU pattern for the CPU, but it is almost twice as slow. It did tell me that it is possible (and will be relatively easy to port to rust-gpu). But it also made me look closer into the data structures and think about what are we actually doing in a lot of details.\nFor example, in PR 372 I separated the collection of matches and calculation of y values and metadata into separate phases, which I then leveraged in PR 373 to use SIMD for compute_fn(), though due to this upstream issue there is no SIMD acceleration for BLAKE3 hashing there yet.\nThe offsets for all supported k values (CPU implementation supports values between 15 and 25, GPU currently between 15 and 24) fit into u32, so that is what precomputed targets into the right table were using. However, by knowing the left table, we also know the range of y values for the right table too. I already mentioned that there are ~236 values in each bucket on average, and even the most conservative upper bound estimate is below 512. This means that there is no need to use u32 for the targets, we can get away with u16 and cut the size of the data structure 2x, which I did in PR 377.\nWhat else can be done? # One more bit of insight into how Subspace uses Chia PoSpace that leads to more optimizations is the fact that we only use a subset of proofs. In fact, ~2/3 of proofs exist for k=20 and since we don\u0026rsquo;t want to waste farmer space (have a portion of it that isn\u0026rsquo;t used for farming) we actually erasure code the records of the archival history before encoding with proofs from Chia PoSpace. What this means is that we have \\(2/3*2 = 4/3\\) of proofs necessary in total. So the encoded plot contains 2/3 of encoded source chunks and 1/3 of encoded parity chunks on average.\nWhat does this mean? Well, since any proof is verifiable, we don\u0026rsquo;t care which ones we use, as long as we\u0026rsquo;re consistent between CPU and GPU implementations. Or in other words, we can try to drop some information during table construction if it helps to save RAM/compute, while still having enough proofs to fully encode the plot. Technically, the farmer app is ready for not enough proofs to be found, but I\u0026rsquo;m thinking about removing that logic since in practice it never actually happens and would be detrimental to the farmer rewards.\nWhat can we optimize, you might ask?\nWe can probably just get rid of y duplicates, so when doing matches, we only need to handle present/missing options. I did some statistical analysis and found out that the majority of targets do not have matches, about 1.5% has one match and going to two, three and more matches decreases by two orders of magnitude with each step. So we can ignore anything beyond one match and probably not lose too much (though still need to verify the resulting number of proofs empirically).\nAlso note that on average there are ~236 y values in a bucket, we can truncate the actual number to 256 and then address it using just u8 in various places. This also happens to match the minimum work group size in Vulkan, which can lead to more efficient sorting implementation, etc. Similarly, the number of matches follows the same pattern. So it is possible to constrain the number of matches to 256 and preallocate the exact number of \u0026ldquo;buckets\u0026rdquo; for matches upfront, rather than dealing with dynamic allocation the way it is done right now.\nWith all buckets being the same (and small) size, which match Vulkan work group size, I believe it should be possible to have a massively better performance on GPU than the current Subspace implementation. Moreover, the current implementation requires 64 kiB of shared memory, which only the most recent GPUs have (Volta/Turning on the Nvidia side and similarly for AMD). Smaller data types should unlock support for smaller and iGPUs with 32 kiB of shared memory, making fast GPU plotting more accessible.\nIn fact, I already started preparing for changes to bucketing, and after introducing new APIs make proofs search substantially faster in PR 378 already:\nBefore: chia/proof/missing time: [22.638 ns 22.717 ns 22.808 ns] thrpt: [43.844 Melem/s 44.021 Melem/s 44.174 Melem/s] chia/proof/present time: [362.23 ns 363.37 ns 365.33 ns] thrpt: [2.7373 Melem/s 2.7520 Melem/s 2.7606 Melem/s] After: chia/proof/missing time: [19.397 ns 19.608 ns 19.845 ns] thrpt: [50.390 Melem/s 50.999 Melem/s 51.554 Melem/s] chia/proof/present time: [357.30 ns 358.78 ns 359.90 ns] thrpt: [2.7785 Melem/s 2.7872 Melem/s 2.7988 Melem/s] Conclusion # Not all attempts at optimization were successful, there are a lot of changes that never landed and some that are still floating in my local branches and may only end up in a GPU version in the end. I have done some optimizations already, here are the results before last week and now:\nBefore: chia/table/parallel/8x time: [767.76 ms 778.72 ms 790.06 ms] After: chia/table/parallel/8x time: [677.60 ms 684.25 ms 692.06 ms] thrpt: [11.560 elem/s 11.692 elem/s 11.806 elem/s] I\u0026rsquo;ll continue experimenting with it until the workflow is simple enough to implement efficiently on GPU. Supranational engineers who implemented the current GPU plotting implementation using CUDA/ROCm did a pretty good job IMO, but knowing how protocol works more intimately allows for an even more efficient implementation. And we do want the most efficient implementation possible because that impacts the security of the protocol. If Subspace\u0026rsquo;s k=20 can be increased to k=21 (each increase roughly doubles size/compute) while taking as much time as the previous k=20 implementation, that is a win!\nWhile k value change will be painful to implement in Subspace, it is possible to implement the change to the way the farmer stores plots to take advantage of performance improvements. In fact, I have already done it in the past. Originally, the algorithm for searching of proofs was implementation-defined, and Supranational engineers requested to change it, or else it would substantially hurt GPU plotting performance. So we introduced V1 plot version in addition to the original V0 to deal with this and supported both for the duration of the testnet. Same or similar thing (like incremental plot upgrade) could be done again for Autonomys mainnet.\nBonus content # Something that I implemented using and spent a lot of time trying to optimize during last week was a version of the matching logic that doesn\u0026rsquo;t create a PARAM_BC array. It instead does a branchless SIMD-accelerated binary search over the whole bucket (which as we remember is upper bound below 512 and practically is often way smaller than that). It uses the clever approach described in the article SIMD / GPU Friendly Branchless Binary Search. It is almost 2x slower than O(1) lookup on CPU, but will most likely shine brightly on GPU (though remains to be benchmarked), especially once the bucket size is reduced to 256.\nI really liked the approach! It is intellectually satisfying to see such tricks working well in practice, despite technically having worse theoretical algorithm complexity. Knowing how hardware works internally really matters for performance!\nUpcoming plans # I\u0026rsquo;ll continue experimenting with optimizations and hope to get to GPU side relatively soon. And I will share whatever results I achieve (or not) in the follow-up update.\nIn the meantime you can find me on Zulip, I\u0026rsquo;d be happy to read any other optimization ideas you might have. I know there was a lot of brain power poured into Chia optimizations and in fact there are third-party fully compatible (reportedly, I have not tried them myself) plotters that are substantially faster than the reference implementation already. Would be cool to learn from that experience.\n","date":"26 August 2025","externalUrl":null,"permalink":"/blog/2025-08-26-faster-proof-of-space-part-1/","section":"Blog","summary":"\u003cp\u003eIn the \u003ca\n  href=\"../2025-08-17-node-prototype\"\u003elast update\u003c/a\u003e I shared that I plan to work on GPU plotting some more, so that is what I did. The \u0026ldquo;easier\u0026rdquo; parts\nof it were \u003ca\n  href=\"../2025-07-02-adventures-with-rust-gpu\"\u003edone\u003c/a\u003e \u003ca\n  href=\"../2025-08-01-client-database-prototype/#gpu-plotting-implementation\"\u003eearlier\u003c/a\u003e. Now it was time for matching logic and that is more complex, so I decided to dedicate the\nwhole blog post to it.\u003c/p\u003e","title":"Faster Proof-of-Space (part 1)","type":"blog"},{"content":"The state of the codebase is slowing approaching the state in which block production might be finally possible. Exciting!\nNode prototype # I\u0026rsquo;ve spent a lot of time thinking about how to organize the state both in memory and on disk exactly, not just conceptually. That has proven to be challenging, so I shelved it for now and did the most basic thing possible in PR 362: store state in memory and hash everything on every block.\nInterestingly, this is not that far off from how the beacon chain and intermediate shards will work in the end, but for user transactions on leaf shards a completely different approach will be needed. But we can deal with that later and potentially in parallel with other issues.\nWith that I went on to create a prototype of the ab-node crate in PR 364 with implementation of the database formatting command (as I described in the earlier blog post). There were several bugs in the database implementation that even this basic functionality helped to uncover and fix.\nWith that I prototyped a basic beacon chain block production and import workflow in PR 367, which is hypothetically almost sufficient to produce blocks locally without any networking. We\u0026rsquo;ll know if it actually works once there is a farmer connected to it, which is currently not the case.\nThe things currently missing for that:\nfarmer RPC layer (based on JSON-RPC like it was in Substrate, but since this is an RPC dedicated just for the farmer, it might be replaced with something binary later) subspace-networking needs to be ported to ab-networking, likely more or less as it is right now GPU plotting implementation needs to be in a shape that kind of works, so subspace-farmer can migrate to it subspace-farmer needs to become ab-farmer With those completely feasible steps, it should be possible to produce blocks in a loop with a local farmer. Since the whole consensus workflow works outside the block runtime, the fact that block execution is not yet implemented should not cause issues for now.\nOnce that is done, the farmer will need to be upgraded with a notion of commitment to a leaf shard. Node will need to be extended to support intermediate and leaf shards, and a lot of parallel streams of work open up at that point.\nUpcoming plans # Those have been the key updates since the last blog post.\nI\u0026rsquo;ll focus on GPU some more during the upcoming week and will look into wiring a basic RPC layer for the farmer on the node side.\nI think in about two weeks there should be a way to run both node and farmer together, depending on how many hacks and placeholders I incorporate ðŸ˜…\nI\u0026rsquo;ll keep this update shorter. In case of questions or just feedback about these updates (what you find interesting/useful, what is less so), ping me on Zulip in one of the relevant channels.\n","date":"17 August 2025","externalUrl":null,"permalink":"/blog/2025-08-17-node-prototype/","section":"Blog","summary":"\u003cp\u003eThe state of the codebase is slowing approaching the state in which block production might be finally possible.\nExciting!\u003c/p\u003e","title":"Node prototype","type":"blog"},{"content":"This week I continued working with the client database and integrating it closer with the rest of the node. A key integration point that was missing completely and still not implemented was state management. The core parts of the consensus do not involve state management, but transaction processing will. So I was considering various ways to process transactions and came up with an idea I\u0026rsquo;ll be pursuing that should work nicely, but is also a bit unlike most of the blockchains out there.\nBlock processing # As a recap, the way blocks are processed in the current implementation is split into several parts, which can be done concurrently. More specifically, as long as parent block headers are available, child block consensus verification can already proceed in parallel, but results will be discarded if the parent block happens to be invalid. This does waste a bit of compute in the worst-case, but massively improves performance otherwise.\nThings are a bit different when it comes to state, though. The current implementation has \u0026ldquo;TODO\u0026rdquo; in place of implementation, so I had to think again how to approach it. State is much more involved than just block headers, so it is unrealistic to expect that a bunch of blocks worth of state will be downloaded during sync (though it is a possibility). At the same time, it would be really nice to have as much parallelism as possible here as well.\nAnother complication is that in most traditional blockchains block can only be propagated through the network once it was fully imported. This substantially increases the amount of time it takes to propagate a block and increases the probability of short-term forks.\nAsync transaction processing # I decided that a good compromise would be to split state update logically into two parts: system contracts are processed and transactions are checked for legality as part of block import, while transaction execution happens separately.\nThe key challenge and design constraint most blockchains face is that transactions are inherently untrusted and consume variable amounts of resources. Often, transactions specify much higher gas limit than they actually consume, which makes it harder to estimate how many of which transactions fit into the block. What makes things worse is that transactions often end up modifying the same contract and need to be executed sequentially. All this is not very friendly for parallelism and high performance.\nThe concurrent updates are addressed to a large degree by a novel state organization, but the rest of the issues remained unresolved until recently.\nI analyzed various common reasons for why transactions are consuming less gas than the specified limit and why it is a problem. It ends up being a complex mix of various factors like business logic, non-deterministic storage access, difficulty in gas estimation, etc. I came to the conclusion that with a more powerful RISC-V based VM and less expensive transactions it might actually be okay to always consume the gas limit, avoiding refunds completely (storage was non-refundable already). If we can do that, then it should be possible to pack a full block of transactions even without executing them!\nHere is the proposed workflow:\nBlock builder calls system contracts and updates their state Block builder runs stateless verification for all transactions and charges a full transaction fee from each that was verified successfully Note that this implies users should not send multiple transactions with e.g., the same nonce, which is a bit awkward design-wise, but might be acceptable since transactions are mortal and multiple nonces can be supported if necessary on the contract level This can be done 100% in parallel, especially with fee subtraction being a cumulative operation State root is computed with the result of those two operations Block is sent to the rest of the network Block import on any node (including block builder) imports the block and starts executing transactions in it, likely in parallel Creation or import of the next block must wait for results of the transaction execution from the parent block with another state root (post-transaction execution) included in it Essentially, transactions are executed while the block is being propagated through the network and while the solution for the next block is being computed. This allows for almost a complete block time worth of compute for transactions rather than a small fraction of it. This kind of pipelining should positively affect both throughput and latency.\nNote that the beacon chain and intermediate shards do not have user transactions, so the workflow there will be simplified.\nReducing MEV # MEV often comes to mind when talking about DeFi transactions, which is basically the ability to extract additional value by manipulating what transactions are included and in which order. I think PoT can come to the rescue here as well, significantly reducing the opportunity.\nPoT is already used in Subspace to delay a block proposal by a few seconds, ensuring all farmers, regardless of how fast or slow they are, have sufficient time to prove they have a solution to the consensus puzzle.\nA similar mechanism can be used to delay transaction execution. Transactions are included in the block in deterministically sorted order when the block is built and fees are charged. However, nodes will have to wait one more slot after that before they know the shuffling seed that determines the execution order of those transactions. Of course, some farmers may choose to wait and manipulate the order. However, delaying the announcement of the block means they will be at a disadvantage compared to potential blocks that were produced at around the same time (natural forks).\nRight now I think a single slot might suffice. However, it is possible to increase it to several slots with the tradeoff being that less time will remain before the next block is created, so the amount of compute (time) allowed will have to be reduced.\nOther updates # That was all I wanted to say about transactions for now, but there were other things I worked on this past week.\nPR 352 was a big cleanup for the client database to make it more extensible in the future. PR 354 made another step towards state management by offering API to persist state of system contracts (as described above), which nodes will have to store. This is in contrast to the state of other contracts, which they are not responsible to store.\nI also spent some more time on optimizations, this time again on balanced Merkle Tree. Last time I mentioned faster balanced Merkle Tree construction implemented in PR 345, this time the same kind of optimization was applied to the root-only computation in PR 350.\nBoth of these APIs are used in the archiving, so I decided to benchmark it:\nBefore: 65536/balanced/compute-root-only time: [4.0835 ms 4.0836 ms 4.0841 ms] After: 65536/balanced/compute-root-only time: [555.84 Âµs 556.90 Âµs 557.80 Âµs] Before: segment-archiving-whole-segment time: [1.8868 s 1.8892 s 1.8914 s] After: segment-archiving-whole-segment time: [950.64 ms 957.24 ms 962.52 ms] Very substantial improvement as expected, though there are still opportunities for better performance with both Merkle Tree and erasure coding.\nAs an interesting reference point, I re-benchmarked Subspace implementation that uses KZG:\nsegment-archiving-whole-segment time: [39.370 s 39.370 s 39.370 s] Wow, it is a whopping 41x slower and we\u0026rsquo;re not done yet! Both are single-threaded. This is why we had to parallelize it and implement an incremental archiving solution to smooth out the cost over a longer period of time. But it was still painful during sync from genesis or when a segment needed to be reconstructed, even on 32C64T CPU it was a noticeable slowdown when this happened.\nUpcoming plans # One of the key things to achieve in the next few weeks is to get state management to the point when we can execute system contracts during block building and import. I\u0026rsquo;m starting with the beacon chain, so transactions will come later. This is one of the key pieces needed for basic single-node node block production to work.\nWork on GPU-accelerated plotting, publishing of crates to crates.io and other miscellaneous things are also still on the table.\nI think this approach to transaction processing might be a bit controversial, so I\u0026rsquo;m open for any kind of feedback on Zulip, please join if you\u0026rsquo;re interested ðŸ™‚\n","date":"8 August 2025","externalUrl":null,"permalink":"/blog/2025-08-08-async-transaction-processing/","section":"Blog","summary":"\u003cp\u003eThis week I continued working with the client database and integrating it closer with the rest of the node. A key\nintegration point that was missing completely and still not implemented was state management. The core parts of the\nconsensus do not involve state management, but transaction processing will. So I was considering various ways to process\ntransactions and came up with an idea I\u0026rsquo;ll be pursuing that should work nicely, but is also a bit unlike most of the\nblockchains out there.\u003c/p\u003e","title":"Async transaction processing","type":"blog"},{"content":"The biggest update since the last blog post is that an initial prototype of the database was merged. It lays the foundation in terms of fundamental architecture and will now be extended to support more features. There were also updates in a few other areas.\nClient database # PR 348 finally landed an initial prototype of the client database that I described in the last update. It focuses on covering existing apis for storing blocks and querying block headers and related stuff. There will be more work needed to read full blocks from the database and a bunch of complexity around state handling, which I think I have mostly figured out, just not implemented yet.\nYou can see the last update and PR description for more details. Despite how incomplete it is, I\u0026rsquo;m happy it landed and can finally build on top of that foundation.\nGPU plotting implementation # After more upstream improvements in rust-gpu, most notably better const folding for int and bool, I was able to finish and merge implementation of compute_fn() shaders in PR 343. There are still a few key components missing like matching, sorting and hopefully erasure coding, before it can be assembled into the complete functional thing, but this is a good progress getting us closer to the goal.\nImproved Merkle Tree performance # I think I mentioned upstream BLAKE3 issues/feature requests a few times already, but this time I used blake3\u0026rsquo;s private/undocumented API in PR 344 to implement a version of SIMD-accelerated hashing of multiple values in ab-blake3, specifically block-sized values. I then used it to construct a balanced Merkle Tree in PR 345 and saw ~5x speedup, though it is still over 2x away from the theoretical performance of BLAKE3 on my hardware. I believe a large fraction of the gap is due to the private API, which was not designed for this use case.\nSo there is still a lot higher performance on the table, and only full construction of the balanced Merkle Tree was optimized for now, with more to come later with incremental updates.\nOther updates # In other news, Alfonso\u0026rsquo;s contract with Subspace Foundation has concluded at the end of July, so his time availability will likely decrease. We made decent progress on the consensus side from a theoretical perspective, and I should have most of the answers needed to work on implementation of the hierarchical sharded version of the protocol soon.\nI think the design is quite elegant overall, given the complexity of the design space. Once solidified, I\u0026rsquo;ll update the book with more details for others to read it. We initially focused on a version that is more for those who are already familiar with Subspace, but a standalone version from the first principles will be eventually needed as well.\nUpcoming plans # I\u0026rsquo;ll keep this update short and focused on key items.\nThe next steps include implementing more features in the client database, notably around state management. Probably after some code refactoring to make lib.rs a bit more manageable.\nThe GPU plotting implementation still needs more work, which I\u0026rsquo;ll probably interleave among other things. I\u0026rsquo;m also considering using CPU stubs for some components in the meantime now that I have confidence in the feasibility of the approach with rust-gpu, just to get the farmer migrated over sooner.\nWith more state-related updates, single-node block production should be getting closer to reality, but I think it\u0026rsquo;ll be at least several updates until then.\nI\u0026rsquo;m also considering publishing ab-blake3 and maybe other components to crates.io, so they can get attention and be useful to a wider community instead of being siloed in the abundance repo.\nThe development progresses with discussions on Zulip, feel free to join and ask any questions you might have.\n","date":"1 August 2025","externalUrl":null,"permalink":"/blog/2025-08-01-client-database-prototype/","section":"Blog","summary":"\u003cp\u003eThe biggest update since the last blog post is that an initial prototype of the database was merged. It lays the\nfoundation in terms of fundamental architecture and will now be extended to support more features. There were also\nupdates in a few other areas.\u003c/p\u003e","title":"Client database prototype","type":"blog"},{"content":"After adventures with rust-gpu, which I still monitor periodically, I moved on to the client database implementation, which is required for proper blockchain operation, and which is one of the bigger undertakings. Unfortunately, the database as such isn\u0026rsquo;t quite ready yet, but I did some preparation and would like to share some details about the database architecture.\nSparse Merkle Tree # The blockchain state is expected to be organized as a Sparse Merkle Tree, but just like with regular Merkle Tree, none of the implementations on crates.io looked like a good fit. With that, I decided to implement my own and merged it in PR 328.\nThe implementation is quite specific to the use case at hand. For example, it currently only supports up to \\(2^128\\) leaves since this is how many addresses the blockchain supports. It is not quite as efficient as it could be since it hashes one pair of leaves at a time, but it already includes an efficient handling of empty/missing leaves. The improvements in this department will come later, just like Merkle Tree will become much faster with SIMD-accelerated hashing of multiple values. In fact, unofficial blake3::platform APIs already make it possible, which I plan to take advantage of before something like that is upstreamed.\nArchitecturally, Sparse Merkle Tree implementation is very close to the Merkle Tree implementation that already existed, but it \u0026ldquo;hashes\u0026rdquo; two zero-filled nodes into a zero-filled node, which allows optimizing proof size. Other than that it is still a recursive data structure, and larger trees can be built from a set of smaller ones.\nImplementations I found on crates.io are tied to the storage backend, while what I implemented is basically ephemeral. It just takes an iterator with leaves and an input and produces a root as an output. It is expected that a bunch of smaller subtrees will be stored on disk and re-hashing will only need to be done on small parts of it, while higher-level nodes will be retained in memory. Since an optimized BLAKE3 implementation can hash ~9 GB of data on a single CPU core on my machine, a small in-memory cache will go a long way.\nDirect file I/O # I learned the hard way while working on the Subspace farmer that the way OSs handle file I/O is quite inefficient when you know exactly what you\u0026rsquo;re doing. Application-level caching is much more effective and efficient, that is assuming it is necessary at all. The code for direct I/O was hidden in the farmer implementation. However, now that I want to reuse it for database implementation, I extracted it into ab-direct-io-file crate in PR 332, then in PR 333 farmer started to use it instead of its own copy.\nThe implementation should work for now, but eventually I\u0026rsquo;d like to get back to experiments with async I/O using io_uring, which I tried in the past already, but it didn\u0026rsquo;t perform particularly well. With what I know now, I think it may have been caused by the fact that back then I was not doing direct I/O, so OS interference had a large impact.\nClient database # Those are both components that will be used for/in conjunction with the client database. The database is not in a state to open a PR and is not usable yet, but I can share some ideas and design decisions in the meantime.\nAs I mentioned in Blockchain as a library, the reference implementation of a node will only support block authoring by default, which opens a design space for optimizations.\nI\u0026rsquo;d really like to achieve constant-size disk usage just like on the farmer: node will pre-allocate all the necessary space upfront and only use that in the runtime.\nAnother observation is that while the blockchain does have short-term natural forks, most of them are short-lived, so those that didn\u0026rsquo;t survive for long don\u0026rsquo;t need to be written to disk at all.\nYet another important feature is that the blocks at 100 blocks deep are immutable, and even beyond that there is a limited number of blocks that need to be stored. So the blocks don\u0026rsquo;t need to be written into a fancy database, a flat file is just fine. Due to limited overall size, the file can be scanned on startup and mapping from blocks to offsets can be maintained in memory. In fact blocks in memory can also be stored in a flat list, only a single map from a block hash to an offset into an in-memory list needs to be stored. This all means very compact and efficient data structures both in memory and on disk.\nNow there is a bit of a challenge with state. It is both larger than blocks, and it is stored permanently. Thankfully, due to only storing roots of individual accounts rather than the contents of the state as such, the total amount of disk space used will be incomparably smaller than in more traditional blockchains. So much so that for early stages of the network, it\u0026rsquo;ll likely all easily fit into in-memory cache, but we still need to prepare for it to grow over time.\nThe way I\u0026rsquo;m thinking to approach it is to essentially start with an empty Sparse Merkle Tree, represented by a simple flat list of leaves, which can be hashed together to get a state root. On updates, a new list can be written to disk. Writes to SSDs happen in pages anyway, so anything smaller than 4 kiB (on modern SSDs more like 16 kiB) ends up consuming the whole page of actual disk write resource anyway. Once the list becomes too large, we split it in half by the key space until it fits into a single page again.\nThis can be done recursively over time as the state gets bigger, possibly with some \u0026ldquo;incremental diff\u0026rdquo; stored for most blocks on top of earlier checkpoint to trade disk space for computation. In-memory representation will likely store the most information about the state of the best block, since that is the one, which will be accessed most often. Remember, this is a block authoring node, not RPC, so we care very little about older state and don\u0026rsquo;t need it in most cases, and it is fine to pay a bit extra for its access.\nIn-memory cache will store references to these pages with some number of caches roots of subtrees of the larger state Sparse Merkle Tree, so the final root can be derived quickly without hashing the whole thing.\nBoth blocks and state subtrees and other elements are expected to be stored in a single flat file as \u0026ldquo;storage items.\u0026rdquo; Each storage item will be aligned to the disk page size for efficient access. Writes will be done in full pages, storage items will be cleared in bulk, and notification will be sent to the SSD that the pages are no longer used.\nTo handle interrupted disk writes, a few checksums will be added on storage item: one for header and one for the contents. The checksum for the header will be stored twice, once at the beginning of the storage item and once at the end. This should cover the most likely case of incomplete write where the tail of the storage item is missing.\nStorage items will also not be allowed to cross certain boundaries, like every multiple of 1 GiB. which will allow to quickly and efficiently scan the whole file and read its contents with high level of concurrency since read can start at any boundary and be guaranteed to hit the beginning of a storage item.\nOverall this should result in a flat file with a simple structure, next to no write amplification and O(1) reads for most practical purposes. This can only be done with a specific use case in mind and only due to architectural decisions around smart contracts design done earlier.\nUpcoming plans # I delayed this update a bit because I wanted to have some early version of the database working first. But it takes a substantial amount of time and I spent way too much time thinking and researching before I could even write anything. There was some amount of procrastination involved, of course. But I think the description should give you an idea of what it will look like, and hopefully it\u0026rsquo;ll not take too much time to have something to show as well.\nMy plan for the next few weeks is to continue working on the client database. I might make another detour to implement SIMD-accelerated BLAKE3 hashing for Merkle Tree (probably just balanced to start) and Sparse Merkle Tree to see how close it can get to the theoretical limits with current design that uses iterators, etc. Also once const folding PR is merged into rust-gpu, I\u0026rsquo;ll likely jump back and merge at least compute_fn() implementation, which I already implemented earlier (except tests).\nI was also thinking about publishing some of the libraries like ab-blake3 and ab-direct-io-file to crates.io since I do believe they might be useful to others and there isn\u0026rsquo;t really any library currently published that does what those libraries do.\nStill a lot of work in different areas, so I\u0026rsquo;ll certainly not being blocked completely by anything except myself. If any of this was interesting, and you\u0026rsquo;d like to discuss it, ping me on Zulip. Otherwise, I\u0026rsquo;ll have another update in a week or so.\n","date":"20 July 2025","externalUrl":null,"permalink":"/blog/2025-07-20-sparse-merkle-tree-and-client-database-preparation/","section":"Blog","summary":"\u003cp\u003eAfter adventures with rust-gpu, which I still monitor periodically, I moved on to the client database implementation,\nwhich is required for proper blockchain operation, and which is one of the bigger undertakings. Unfortunately, the\ndatabase as such isn\u0026rsquo;t quite ready yet, but I did some preparation and would like to share some details about the\ndatabase architecture.\u003c/p\u003e","title":"Sparse Merkle Tree and client database preparation","type":"blog"},{"content":"This week has been mainly focused on refining a bit the design for plot identification and sector expiration. I think that I finally have a model with which I am comfortable with, and that I think solves all of our previous problems. Nazar had this idea to drastically simplify how plot IDs were derived, and how sectors were linked to plots. The high-level idea made sense, but there were still some details that weren\u0026rsquo;t clear. This week I managed to come up with a design that I think satisfies all of our requirements.\nApart from all of this work around plot and sector lifecycle management, I also started working on a draft specification for the protocol that fits all the pieces together. Our protocol builds upon the Subspace protocol, so I am using the existing specification as a base and extending it with all the sharding specifics. I can\u0026rsquo;t wait to share the first draft with you all for feedback and suggestions.\nA Deterministic Mapping for Plot Lifecycle Management # The core of this proposal is based on the simplification idea for sector expiration and plot identification from this [zulip discussion]. The basic idea is to simplify plot identification by removing scheduled, network-wide epochs. Instead, we introduce a deterministic, cyclical mapping function. This approach decouples long-term shard assignment from the specific history_size by mapping any history_size to a fixed set of \u0026ldquo;history classes\u0026rdquo;. This allows a farmer to continuously update sectors with new history while having the flexibility to remain in the same shard, making a plot epoch parameter unnecessary. This also limits the number of sectors that can be created in parallel with the same sector index and different committed history sizes (artifically increasing the storage of a plot).\nThe operation of the protocol is based on the concept of a History Modulo (HISTORY_MODULO), a new global protocol parameter that defines the total number of unique history classes. For example, HISTORY_MODULO could be set to 2048. This parameter governs the trade-off between farmer flexibility and shard selection potential.\nA farmer\u0026rsquo;s assignment to a shard is determined not by the exact history_size they are plotting, but by the class that history maps to.\nHistory Class Calculation: When a farmer creates a sector, they commit to a committed_history_size. From this, a history_class is derived using a simple modulo operation: history_class = committed_history_size % HISTORY_MODULO Plot ID Derivation: The stable plot_id is then created by hashing the farmer\u0026rsquo;s public key with this class. This is the crucial step that provides stability. plot_id = hash(public_key_hash, history_class) Shard Allocation: This plot_id is then used as the input to the Verifiable Random Function (VRF) that determines the shard assignment. assigned_shard = vrf_output_to_shard(VRF(plot_id, randomness)) Because history_class is the result of a modulo operation, many different history_size values will map to the same class. This ensures the plot_id and the resulting shard assignment can remain stable even as the farmer plots new history.\nSector and Plot Identification # Sectors must still be uniquely and verifiably tied to the specific history they contain.\nSector ID: The sector_id is derived simply from this plot_id and the sector\u0026rsquo;s index within the plot. sector_id = hash(plot_id, sector_index) This hierarchical ID structure ensures that a farmer\u0026rsquo;s shard assignment can be stable, while still enforcing that each sector is uniquely tied to its specific committed_history_size for verification and expiration.\nExpiration and Re-Plotting Cadence # This model creates a flexible and predictable lifecycle for farmers, giving them direct control over their re-plotting strategy.\nFarmer Lifecycle Example:\nJoining: A farmer starts plotting when the current history size is H_1. They calculate history_class_1 = H_1 % HISTORY_MODULO. This determines a plot_id_1 and assigns them to a corresponding shard. They begin filling their drive with sectors committed to H_1.\nMaintenance: As time passes and the chain grows to a history size of H_2, their initial sectors from H_1 begin to expire. The farmer uses the newly freed space to plot new sectors. They now have a choice:\nRe-use Existing Plot: The farmer can choose to remain in the same plot (and thus the same shard). To do this, they find a committed_history_size (H_commit) that is less than or equal to H_2 and also satisfies H_commit % HISTORY_MODULO == history_class_1. A farmer can easily calculate the most recent history size that meets this criterion. Utilize a New Plot: Alternatively, the farmer can commit to the latest history, H_2. This will generate a new class, history_class_2 = H_2 % HISTORY_MODULO, creating a new plot_id and likely assigning them to a new shard. This model removes the concept of a mandatory, network-wide \u0026ldquo;Epoch Turnover.\u0026rdquo; Instead, re-shuffling becomes a strategic choice made by the farmer.\nBenefits and Parameter Tuning # This deterministic mapping provides significant advantages:\nSimplicity: The logic is extremely straightforward, relying on a single new parameter (HISTORY_MODULO) and simple modulo arithmetic. Farmer Quality of Life: Farmers are never forced into a disruptive, network-wide re-plotting event. They have fine-grained control over which plots they maintain, reducing operational overhead. Flexibility: Farmers can choose to commit to a slightly older history size to preserve their shard assignment or switch to a new one if it is more advantageous. The primary consideration is tuning the HISTORY_MODULO parameter, which involves a direct trade-off between farmer flexibility and preventing strategic shard selection.\nA model for parameter tuning # The key to this design is selecting an optimal value for \\(M\\), the HISTORY_MODULO. This choice involves balancing two competing factors: History Lag and Shard Selection Power.\nLet\u0026rsquo;s define the key parameters:\n\\(M\\): The HISTORY_MODULO, the number of distinct history classes. This is the value we want to determine. \\(S\\): The total number of shards in the network. \\(H_{current}\\): The current history_size of the blockchain. Modeling Farmer Flexibility (History Lag) # When a farmer wants to re-use an existing plot, they may not be able to use \\(H_{current}\\). This creates a \u0026ldquo;lag\u0026rdquo; between their plotted history and the most current state of the network. We can quantify this as the Average History Lag ((L_H)):\n$$ L_H = \\frac{M}{2} $$\nA larger \\(M\\) gives the farmer more classes to choose from, reducing the average lag for any given class.\nModeling Security (Shard Selection Power) # A farmer could try to gain an advantage by choosing which history class to plot. With \\(M\\) possible classes, they can generate \\(M\\) different plot_ids and calculate the resulting shard for each, choosing the one that is most favorable. We can model this as the Shard Selection Power (\\(P_{select}\\)), the probability of landing in one specific target shard:\n$$ P_{select}(M, S) = 1 - \\left(1 - \\frac{1}{S}\\right)^M $$\nA smaller \\(M\\) significantly reduces this probability, making it difficult for farmers to strategically choose their shard.\nThe Optimization Model # The goal is to choose a value for \\(M\\) that provides a good balance. For example, if a network has \\(S=1000\\) shards and we decide that no farmer should have more than a 5% chance of selecting a specific shard, we would solve for \\(M\\):\n$$ 0.05 = 1 - \\left(1 - \\frac{1}{1000}\\right)^M $$\n$$ \\ln(0.95) = M \\cdot \\ln(0.999) \\implies M \\approx 51.3 $$\nBased on this, a protocol designer might choose \\(M=50\\). This value would provide a low risk of shard selection while ensuring the average history lag is only 25 segmentsâ€”a very reasonable trade-off for farmer flexibility.\nAn opportunity to simplify sector expiration? # Sector expiration has been the main thing messing with me when thinking about the design of this part of the protocol. With this new approach to bind sectors into specific plots we may be able to conceptually simplify the existing formula for sector expiration to make it more intuitive. The logic is fundamentally sound.\nEssentially, the current model can be thought of as:\nExpiration Point = Base Lifetime + Random Additional Lifetime\nWhere:\nBase Lifetime = history_size + MIN_SECTOR_LIFETIME Random Additional Lifetime = A random value between 0 and 3 * history_size This conceptual model preserves all the benefits of the original design while being easier to reason about. The current implementation, while verbose, appears to be a necessary complexity to ensure long-term network health and fairness. What I want to do next is to figure out how to simplify this conceptual model into a more intuitive formula that can be easily understood and reasoned about on top of the current lifecycle management design. (Worst case, we can keep the current sector expiration, as it wouldn\u0026rsquo;t break in any way with this new design.)\nSpec in progress and next steps # The \u0026ldquo;protocol overview\u0026rdquo; section of a specification is always the harder for me. I try to give a high-level overview of the operation of the protocl that gives readers a core intuition of how all the piece fit together. This way, one can have a clear mental model for when we jump into the low-level details (where is easy to get lost into the weeds).\nAs mentioned above, this week I started working on the specification for the protocol, and I already managed to have a protocol overview section that I am happy with. I hope to make more progress on it in the coming days, and to figure a way to start sharing small pieces of it so I can start getting feedback and improvement suggestions.\nThroughout all of my more theoretical work from the past few weeks, I have been sharing theoretical models for different parts of the protocol, but I haven\u0026rsquo;t shared the specific protocol parameters for the various components (e.g. the optimal HISTORY_MODULO value). As part of my spec\u0026rsquo;ing efforts, I am hoping to also recommend specific values for these parameters based on the theoretical models and target system requirements. Hopefully, this will provide a more concrete foundation for the protocol and help guide implementation efforts.\nOn another personal note, next week I will be giving a talk at the Web3Summit in Berlin, so if you are around and you want to chat about this project, blockchain scalability, or honestly anything web3-related, feel free to reach out to me. I will be around the event and would love to meet you in person.\n","date":"14 July 2025","externalUrl":null,"permalink":"/blog/2025-07-14-a-deterministic-mapping-for-plot-lifecycle-management/","section":"Blog","summary":"\u003cp\u003eThis week has been mainly focused on refining a bit the design for plot identification and sector\nexpiration. I think that I finally have a model with which I am comfortable with, and that I think\nsolves all of our previous problems. Nazar had this idea to drastically simplify how plot IDs were\nderived, and how sectors were linked to plots. The high-level idea made sense, but there were still\nsome details that weren\u0026rsquo;t clear. This week I managed to come up with a design that I think satisfies\nall of our requirements.\u003c/p\u003e","title":"A Deterministic Mapping for Plot Lifecycle Management","type":"blog"},{"content":"","date":"14 July 2025","externalUrl":null,"permalink":"/authors/adlrocha/","section":"authors","summary":"","title":"adlrocha","type":"authors"},{"content":"","date":"14 July 2025","externalUrl":null,"permalink":"/tags/consensus/","section":"tags","summary":"","title":"consensus","type":"tags"},{"content":"There are buzzwords in any industry that are thrown around easily, and blockchains are no exception. In this post, I want to focus on \u0026ldquo;scalability\u0026rdquo;. Turns out when you say \u0026ldquo;blockchain scalability\u0026rdquo; different people hear different things. The prevalent opinion seems to be that scalable blockchains are able to process more transactions than non-scalable ones or something along those lines. Essentially making the ability to scale equivalent to peak performance.\nSure, peak performance is an important metric, although it is often a theoretical one. But I don\u0026rsquo;t think that is the most useful property, especially without clarifying the conditions under which it can be achieved.\n\u0026ldquo;Scalable blockchain\u0026rdquo; # When looking around searching for scalable blockchains, Solana often appears as one of the most scalable options. What that means in the context of the meaning at the beginning of this post is that it can handle a lot of transactions.\nThe catch? Solana validators have quite high hardware requirements. Processing more transactions is possible by increasing those requirements further over time, which is exactly the plan for Solana.\n\u0026ldquo;Infinitely scalable blockchain\u0026rdquo; # There are blockchains that go even further than that, claiming \u0026ldquo;infinite scalability\u0026rdquo;, NEAR does that in articles and on landing pages. NEAR\u0026rsquo;s hardware requirements for validators are substantially lower though, so how is it possible that a chain with lower hardware requirements for validators achieves infinitely higher scalability? Turns out, they mean something completely different!\nWhat the team behind NEAR is actually trying to say is that while the requirements of individual validators are modest, as more validators join the network, the network as a whole can process more transactions. And they seem to believe it is possible to just keep adding infinite number of validators to achieve infinite transaction throughput.\nWho said rollups? # There is another quirky case I\u0026rsquo;d like to mention before giving my own definition. There are various flavors of heterogeneous systems that are also called \u0026ldquo;scalable\u0026rdquo;. For example, Polkadot has parachains, Ethereum community believes they are scaling it through rollups.\nIn this case, neither the blockchain itself can process a particularly large number of transactions nor can it do so by attracting more validators. What happens instead is that this blockchain (layer 1 or L1) is used as a settlement layer for essentially other blockchains (layer 2 or L2), and since there could be multiple of those L2s, the whole construction in its entirety can process more transactions.\nThe definition # My definition is:\nScalable blockchain is such that can process more transactions as more consensus participants (think physical machines) join the network\nIn fact, it is important to stress that most blockchains have inverse scalability!\n100% of blockchains mentioned so far are based on Proof-of-Stake consensus, which has practical scalability issues around the number of distinct consensus participants they could have. From registering on chain to network communication complexity, it is basically guaranteed that a ceiling will be hit sooner or later despite all the excellent research done with clever subcommittee sampling, validator pools and optimizations that avoid quadratic network complexity.\nReally scalable blockchain # I believe a really actually scalable blockchain must be able to scale the number of participants and increase its throughput while doing so.\nThis is why this research project starts with a permissionless consensus design, which, just like Bitcoin, can support an enormous number of distinct consensus participants. Then the consensus is made hierarchical through sharding, such that all those participants no longer execute the same exact thing over and over again (which wouldn\u0026rsquo;t be scalable at all).\nNow a bit about \u0026ldquo;infinite scalability\u0026rdquo;. Of course NEAR doesn\u0026rsquo;t have it. They will not be able to add an infinite (or even extremely large, like 1B) number of consensus participants, nor would their validators be able to physically receive erasure coding pieces from an infinite number of other shards. So calling it \u0026ldquo;infinite scalability\u0026rdquo; is just a marketing fluff.\nIn fact, it does help to have practical hard limits for various parts of the system. This allows to optimize and simplify data structures, for example, by avoiding variable-length encodings.\nIn current sharding design, we have several of these hard limits. One is the solution range, which is represented by u64 and 64 bits is sufficient to represent all the disk space that currently exists in the world many times over and for practical purposes is unlimited, but in principle it is not. If there is ever a sudden exponential advancement in SSD capacity, additional sampling rules or other changes might be needed in consensus to scale the capacity further and it wouldn\u0026rsquo;t be that difficult.\nAnother hard limit is the number of shards. Shard index is an important parameter and its size has a wide range of implications on things ranging from contract addresses to various inclusion proofs. We\u0026rsquo;re using 20 bits for shard index, which gives us ~1M shards in total, which feels like a really high number that the blockchain will likely never reach, but it is a hard limit nonetheless.\nEven if each shard processes a single user transaction, with 1M of them, it\u0026rsquo;ll likely crush any public blockchain that currently exists in terms of throughput already.\nConclusion # I have never seen a comparable design to one that me and Alfonso are working on, that (under the above definition) is actually scalable to Internet-scale performance.\nOf course, there are many other important aspects of the blockchain beyond scalability. I believe there are good solutions to many of them. Some of them are important enough on their own to justify the existence of a blockchain that doesn\u0026rsquo;t scale. However, true scalability remains one of those illusory goals that doesn\u0026rsquo;t have a great solution yet. But humanity deserves it!\n","date":"13 July 2025","externalUrl":null,"permalink":"/blog/2025-07-13-what-is-blockchain-scalability/","section":"Blog","summary":"\u003cp\u003eThere are buzzwords in any industry that are thrown around easily, and blockchains are no exception. In this post, I\nwant to focus on \u0026ldquo;scalability\u0026rdquo;. Turns out when you say \u0026ldquo;blockchain scalability\u0026rdquo; different people hear different things.\nThe prevalent opinion seems to be that scalable blockchains are able to process more transactions than non-scalable ones\nor something along those lines. Essentially making the ability to scale equivalent to peak performance.\u003c/p\u003e\n\u003cp\u003eSure, peak performance is an important metric, although it is often a theoretical one. But I don\u0026rsquo;t think that is the\nmost useful property, especially without clarifying the conditions under which it can be achieved.\u003c/p\u003e","title":"What is blockchain scalability?","type":"blog"},{"content":"I have to admit that I am a bit disappointed with my progress this week. If you recall from last week\u0026rsquo;s update, I started the week with a base proposal to handle the linking of sectors to plots based on history window ranges. The idea was to limit the number of parallel sectors that could be created in parallel, linked to the same plot, and hence allocated to the same shard. While the approach seemed quite elegant because it didn\u0026rsquo;t require any changes to how piece selection and expiration currently works in Subspace, it turned out to be pretty complicated (even to explain) and not the most effective solution to prevent the attack I was trying to mitigate.\nA formal model to convince myself # I still thought that the idea of window ranges was a good one, so I tried to come up with a model to understand the optimal window sizes to minimise the number of parallel sectors that a farmer could have in parallel for the same plot. The result wasn\u0026rsquo;t very encouraging, as the history size increased, the number of sectors that could be created in parallel also increased significantly as farmer windows grow. The situation was such that even with windows we ended up in a similar situation to the one where we didn\u0026rsquo;t have history windows and every sector was committed to a different history size (like in the original Subspace protocol).\nThis was more apparent when Nazar pointed this same thing out to me on one of our syncs. For small history sizes the solution could make sense, but as the history size we just add complexity to the protocol without any real benefit.\nEverything should be made as simple as possible but not simpler # Let\u0026rsquo;s take a step back to understand why we were doing this in the first place.\nWe needed a way to link sectors to plots so that we could use plots as core unit for the allocation of storage space in shards. We wanted to prevent farmers from being able to bias or game shard membership by creating multiple sectors linked to the same plot committed to different history sizes but with the same sector ID. Sector expiration and piece selection should be designed in a way that ascribes with the above requirements while maintaining the basic properties (in terms of sector lifetime and history load balancing, respectively) of current Subspace protocol. Fortunately, Nazar to the rescue once again! He introduced in this Zulip discussion a really simple solution that solves all of the above with the only drawback that it may require farmers to keep at least to active plots in parallel, what would require them to sync with at least two more shards. But this may end up becoming more of a feature than a problem.\nThe Simple Solution # The key idea is that we derive a plot_id from the combination of public_key_hash and history_size, and then commit individual sectors to this derived plot identity (uniquely linking them to the plot).\nHere\u0026rsquo;s how it works:\nPlot Identity: plot_id = hash(public_key_hash, history_size) Sector Identity: sector_id = hash(plot_id, sector_index) This approach maintains the essential properties we need:\nEach plot is effectively bound to a unique history size through the plot_id derivation Sectors within a plot share the same history size commitment The gaming attack is prevented because changing the history size results in a completely different plot_id What happens when sectors expire? # One of the key concerns I initially had about this approach was whether all sectors within a plot would expire simultaneously, creating a massive re-plotting burden for farmers. However, as Nazar clarified, this isn\u0026rsquo;t the case at all.\nThe expiration mechanism remains exactly the same as in the current Subspace protocol: probabilistic expiration with the committed history size as an anchor. Here\u0026rsquo;s how it works:\nWe wait for a certain number of segments to pass We determine expiration for each sector individually using the same probabilistic mechanism Sectors expire gradually over time, not all at once This means that even though all sectors in a plot are committed to the same history size, they don\u0026rsquo;t expire simultaneously. The expiration is still spread out probabilistically, maintaining the same load balancing properties we have today.\nThe Multi-Plot Reality # One interesting consequence of this approach is that farmers will likely end up maintaining at least two active plots in parallel. Initially, I was concerned this would be a drawback, but I\u0026rsquo;ve come to realize it might actually be a feature rather than a bug.\nHere\u0026rsquo;s the typical farmer lifecycle under this model:\nInitial State: Farmer creates a plot with plot_id_A = hash(public_key_hash, history_size_A) and fills it with sectors Gradual Expiration: Over time, sectors in the plot begin to expire probabilistically New Plot Creation: When the first sector expires, the farmer picks a new history size (history_size_B) and creates a new plot (plot_id_B) for subsequent sectors Parallel Operation: For a period, the farmer maintains both plots, syncing with multiple shards Transition: Eventually, all sectors in the old plot expire, and the farmer can focus solely on the new plot This creates an interesting economic decision point for farmers: Should they dedicate the storage space of the expiring plot to continue syncing with the allocated shard, or is it more economical to re-plot those sectors so they only need to sync with one shard?\nBenefits of the Simple Approach # The more I\u0026rsquo;ve thought about this solution, the more elegant it becomes:\nSimplicity: No complex window ranges or intricate sector management logic Security: Prevents the gaming attack by binding plots to specific history sizes Flexibility: Farmers can optimize their shard participation based on economic incentives Compatibility: Maintains the core properties of the existing Subspace protocol Natural Load Balancing: Forces farmers to distribute across multiple shards over time The requirement to sync with multiple shards might actually improve network resilience by ensuring farmers maintain connections to different parts of the network.\nNext Steps: Formal Specification # Now that we\u0026rsquo;ve settled on this approach, I think I have everything that I need to start writing an end-to-end spec with all the ideas and protocol mechanisms that we\u0026rsquo;ve come up with in the past months.\nHaving all these protocol mechanisms I\u0026rsquo;ve been describing consolidated into a single specification will provide a clear roadmap for implementation while having a single source to start gathering feedback from the broader community. I am still figuring out the best format for this spec. I am thinking about forking the Subspace protocol spec and including all the sharding mechanisms in it, or to create a separate one. So far I\u0026rsquo;ve started writing it in a separate file for now, but I may also try the fork approach to keep everything in one place and avoid having to re-write some things. But for any of you that have been following this updates actively, feel free to let me know your thoughts on this, or if you already have feedback or concerns about any of the ideas presented so far.\nUntil next week! I hope this one is a more productive one, as I am really excited to start gathering feedback for all the work we\u0026rsquo;ve done so far.\n","date":"7 July 2025","externalUrl":null,"permalink":"/blog/2025-07-07-simplifying-plot-expiration/","section":"Blog","summary":"\u003cp\u003eI have to admit that I am a bit disappointed with my progress this week. If you recall from last\nweek\u0026rsquo;s update, I started the week with a base proposal to handle the linking of sectors to plots\nbased on history window ranges. The idea was to limit the number of parallel sectors that could be\ncreated in parallel, linked to the same plot, and hence allocated to the same shard. While the\napproach seemed quite elegant because it didn\u0026rsquo;t require any changes to how piece selection and\nexpiration currently works in Subspace, it turned out to be pretty complicated (even to explain) and\nnot the most effective solution to prevent the attack I was trying to mitigate.\u003c/p\u003e","title":"Simplifying Plot Expiration","type":"blog"},{"content":"GPU plotting was one of the items on the roadmap last week and that turned into a week+ long side quest, so let me share some details about that.\nBackground # For a little bit of background, Subspace protocol has a compute-intensive plotting component, where the majority of the cost is Chia-based Proof-of-Space that is used to encode plots (it is used differently than in Chia, but that is not very important here).\nThis is something that can be accelerated with GPU and, in fact, is extremely desirable to make plotting more energy efficient and less time-consuming. In Subspace GPU plotting was implemented for CUDA/ROCm by very skilled folks from Supranational. Unfortunately, it broke after switching from KZG and some other changes, which left it in unusable state.\nNow I\u0026rsquo;m trying to avoid C++ when I can, let alone CUDA C++, so I was looking for ways to rewrite it in Rust instead if possible, which I shared in the past updates. On top of that, previous implementation was picky on AMD side, it only really worked with RX 6000/7000 GPUs (on the consumer side) and only on Linux. According to AMD developers it is unlikely that Windows support will come any time soon, there were also annoyances caused by dynamic linking required by ROCm to a specific release of their libraries.\nAnd forget about Intel or Apple, iGPUs, etc.\nSo as you can imagine, I wasn\u0026rsquo;t particularly thrilled with the status quo to begin with. The better situation would be to target Vulkan/Metal, which is exactly what wgpu allows, but we need shaders and I want them in Rust, not yet another obscure language to suffer with.\nCubeCL # I looked at CubeCL a few times, tried to write some kernels, but in the end I gave up on it, for now at least. The basic issue I have with it is that it is one of those obscure shader languages. Yes, it looks like Rust, but it only supports a subset of language features and standard library, doesn\u0026rsquo;t allow to freely pull no_std crates from crates.io and as the result ends up not being a real Rust.\nI strongly considered this path, but ultimately I do want to be able to use Rust with any of its features and have the fullest control over the compilation output of the code.\nrust-gpu # This brings be back to rust-gpu, which I initially dismissed due to it requiring old nightly compiler, but with cargo-gpu as a library it became not great, but at least usable. It is in fact actual Rust, more specifically a codegen backend that rustc calls to produce regular SPIR-V binary, which can be executed on a GPU using wgpu or other libraries. Using wgpu for this purpose is nice because it\u0026rsquo;ll recompile SPIR-V, which is Vulkan-specific, into shader that runs on Apple\u0026rsquo;s Metal API, which means support for quite powerful Apple Silicon iGPUs.\nWith that, I started prototyping and learned a lot in progress about rust-gpu, Vulkan, SPIR-V and GPU programming in general. One interesting property that surfaced fairly quickly was that GPUs really like 32-bit integers and seriously dislike smaller and larger ones, something like u128 is not generally available at all, even as an optional capability.\nNot going to lie, it was a struggle. There are countless limitations in rust-gpu as it stands today, and eventually I hit a wall that I\u0026rsquo;ll probably stop at for now. But I have made progress. When you look at Proof-of-Space spec, I have ChaCha8 keystream derivation and compute_f1() fully implemented as shaders, and even tested with LLVMpipe in CI. compute_fn() is technically implemented, but unfortunately doesn\u0026rsquo;t quite compile despite my best efforts to work around every last limitation. Since u64 is not supported on all GPUs and u128 is not supported anywhere (at least until rust-gpu learns to add polyfills for them), I had to write polyfills for both and test them against native types, ensuring they have the same exact binary representation in memory.\nThat still leaves matching logic and sorting. For sorting, there are some libraries on crates.io, hopefully something that will work and for matching it shouldn\u0026rsquo;t be terribly difficult to implement directly. CUDA C++ implementation also did erasure coding, but I didn\u0026rsquo;t look into how difficult it\u0026rsquo;ll be to make reed-solomon-simd work with rust-gpu yet. Worst case we\u0026rsquo;ll do it on CPU for now, erasure coding is A LOT faster now than it was with KZG stuff involved.\nIf you\u0026rsquo;re interested in what I have so far, PR 313 was the very first piece of code with GPU with a few follow-ups in PR 314, PR 315 and PR 320.\nOther work # Learning more about GPUs makes one re-think some of the existing approaches. As I mentioned earlier, GPUs really do not like working with individual bytes, but strongly prefer u32, so as I was writing GPU code I though if CPU can benefit from similar changes, which led to PR 312 with substantial PoSpace verification performance improvement:\nBefore: chia/verification time: [9.3419 Âµs 9.4671 Âµs 9.7223 Âµs] After: chia/verification time: [7.4406 Âµs 7.4539 Âµs 7.4651 Âµs] I was also increasingly frustrated with how difficult it is to land any changes to upstream BLAKE3 crate and since I needed BLAKE3 for GPU plotting as well, I ended up creating ab-blake3 crate. It currently has more exotic and special-purpose APIs. For example there are const fn methods that were under review upstream since January, there are also more compact (and thus easier for compiler to optimize) methods that handle up to one chunk and up to one block worth of data only. For single block version, I also created a portable variant that works with u32 words instead of individual bytes, which as you may have guessed is necessary for GPU. In the future there I\u0026rsquo;d like to have single-block variants that can process multiple independent blocks with SIMD.\nInitial implementation landed in PR 316 with further extension in PR 318. Using it in the repo immediately yielded further small performance improvements:\nMerkle Tree before: 65536/balanced/new time: [4.1899 ms 4.1903 ms 4.1919 ms] 65536/balanced/compute-root-only time: [4.2740 ms 4.2743 ms 4.2754 ms] 65536/balanced/all-proofs time: [1.4789 ns 1.4796 ns 1.4824 ns] 65536/balanced/verify time: [70.688 ms 70.696 ms 70.728 ms] Merkle Tree after: 65536/balanced/new time: [3.8788 ms 3.8789 ms 3.8790 ms] 65536/balanced/compute-root-only time: [3.6719 ms 3.6851 ms 3.6884 ms] 65536/balanced/all-proofs time: [1.4212 ns 1.4222 ns 1.4225 ns] 65536/balanced/verify time: [65.727 ms 65.792 ms 66.050 ms] PoSpace before: chia/table/single time: [1.0683 s 1.0908 s 1.1156 s] chia/table/parallel/1x time: [158.26 ms 160.78 ms 163.98 ms] chia/table/parallel/8x time: [860.76 ms 873.21 ms 885.64 ms] chia/verification time: [7.4406 Âµs 7.4539 Âµs 7.4651 Âµs] PoSpace after: chia/table/single time: [977.21 ms 985.74 ms 996.67 ms] chia/table/parallel/1x time: [160.87 ms 162.47 ms 163.84 ms] chia/table/parallel/8x time: [821.04 ms 833.45 ms 850.07 ms] chia/verification time: [6.8722 Âµs 6.9056 Âµs 6.9300 Âµs] Upcoming plans # With that, I\u0026rsquo;ll probably take a pause with GPU programming and wait for one of many issues/discussions I opened/commented on in rust-gpu repository to be resolved, so that there isn\u0026rsquo;t as much friction. Still, I think rust-gpu has a future, I learned a lot about GPUs during the last week and looking forward to returning to this some time soon. In fact, now that I can write shaders for GPUs, I think there are more protocol components that could be accelerated, for example, plot auditing might be a good candidate for this.\nNow I\u0026rsquo;ll probably be going back to thinking and hopefully prototyping the state management nad Sparse Merkle Tree. I did some research earlier, but didn\u0026rsquo;t write anything specific in code yet.\nAlfonso made some good progress on sharded consensus, so I might go back to that and make plotting/auditing/verification shard-aware, though it\u0026rsquo;d be nice to have a farmer and node in runnable shape first.\nBasically as much work ahead as ever, but I\u0026rsquo;m not hard to find on Zulip in case you have any thoughts about this update or the whole project in general.\n","date":"2 July 2025","externalUrl":null,"permalink":"/blog/2025-07-02-adventures-with-rust-gpu/","section":"Blog","summary":"\u003cp\u003eGPU plotting was one of the items on the roadmap last week and that turned into a week+ long side quest, so let me share\nsome details about that.\u003c/p\u003e","title":"Adventures with rust-gpu","type":"blog"},{"content":"As mentioned on last week\u0026rsquo;s status update, one of the key pieces that I was missing to have the detailed operation of plot membership allocation was the impact of sector expiration on the protocol. By having a unique plot identifier, we are able to uniquely link sectors to plots, but these sectors need to expire in a way that does not require farmers to re-plot while archiving the most recent history. Fortunately, we can leverage the current expiration mechanism of the Subspace protocol, and build a layer on top of it to adapt it to the sharded version while maintaining the original guarantees in terms of plot expiration, sector re-plotting, and history archiving.\nSharded Subspace Plot Expiration Protocol # Overview # Before I start with the description of the protocol, let me remind you what are the core properties that we wanted the protocol to have:\nUnique Plot IDs: plot_id is unique and stable. This is key for the protocol, as this ID will be used to allocate space into shards. No Bias through history_size: Farmers must not be able to bias the shard allocation for their convenience. They may be able to achieve this by creating sectors with the same sector ID that can be committed to any history_size. This can be prevented by limiting the range of history sizes that plot sectors can be committed to. Thus, farmers are not allowed to manipulate shard allocation by assigning to a plot the most convenient sector at each time. Load-Balanced Archiving (New History): As new history is archived, it gets plotted shortly after by sufficient number of farmers without large re-plotting events. Detect Expiration and Validity: Plots, sectors and blocks include all the information needed to verify their validity, plot assignment to a shard, sector belonging to the plot, and expiration. No Excessive Re-plotting: Keep re-plotting burden similar to original Subspace. Committing sectors to history ranges # The core idea for sector expiration is the following: instead of letting farmers commit their sectors to any history size, they are bound to commit them to a range of history sizes determined by the plot the sector is linked to. This way, we limit the range of history sizes that a sector can be committed to, and hence limited the number of parallel sectors with the same plot_id and different committed history sizes that can be created. This is implemented as a \u0026ldquo;self-evolving plot commitment range\u0026rdquo; that grows as the blockchain grows on top of the existing mechanism for sector expiration. Thus, we maintain all the properties of the original Subspace protocol, while adapting it to the needs of the sharded version. This allows us to also learn an apply the heuristics from the live Subspace network.\nThe high-level operation of the protocol is as follows:\nWhen creating a new plot, farmers need to choose a random nonce for their plot. This nonce determines the specific ID of the plot and will influence the history range that the plot\u0026rsquo;s sectors can commit to. As a reminder, plots identities are inferred from the public key hash of the farmer, and the random nonce they choose: plot_id = hash(public_key_hash || nonce) When creating new sectors for a plot, farmers will choose any valid history size within the plot\u0026rsquo;s current range. Piece selection is based on the sector\u0026rsquo;s specific history size, and expiration follows the same logic as the original Subspace protocol. Finally, the sector ID is derived from the plot ID, sector index, and committed history size: sector_id = keyed_hash(plot_id, sector_index || committed_history_size) We determine a protocol parameter BASE_WINDOW_SIZE that determines the size of the BASE_RANGE. This BASE_RANGE determines the baseline history range for all plots, i.e. the reference range that will be used as input to infer a plot\u0026rsquo;s effective range. The BASE_RANGE determines the initial size of history sizes that plots can commit to initially. The effective range that a plot can commit to at a a specific point in time is determined by a transformation of the BASE_RANGE based on the plot\u0026rsquo;s nonce and the current history size. As the history of the chain grows, this is expanded accordingly so it can fit new history sizes. The following image belongs to a simple visualisation tool that an LLM has built for me to visualise how the base range and effective range of a plot behaves as the history of the chain grows: Sectors expire as they are used to in current Subspace protocol, so even if the effective range for plots grow, sectors committed to old history sizes will be alreaedy expired (or close to expire) still limiting the number of history sizes that sectors can commit to at a given time. This is key to ensure that farmers do not need to re-plot their sectors as the history grows, and that they can archive the most recent history without having to re-plot, maintaining all the nice properties for sector expiration of the original Subspace protocol. Putting all pieces together # With this, we have all that we needed to finally implement farming in shards:\nPlot IDs are unique and stable, allowing us to allocate space into shards. Sectors are committed to a range of history sizes determined by the plot ID, preventing farmers from gaming shard allocation. Piece selection, sector expiration stay unchanged. Farming stays mostly unchanged, apart from the additional information required to verify that the chunk for the piece in the solution range belongs to a sector bound to a plot currently assigned to the shard. This is how the verification logic for a solution in a shard would look like:\nStandard Subspace Verification (Unchanged): First, perform all existing Subspace checks // Verify piece offset is valid if piece_offset \u0026gt;= max_pieces_in_sector { return Err(\u0026#34;Invalid piece offset\u0026#34;); } // Check sector expiration if sector_is_expired(sector_id, current_history_size) { return Err(\u0026#34;Sector has expired\u0026#34;); } // Verify the piece is part of blockchain history if !verify_piece_inclusion(piece, segment_root, proof) { return Err(\u0026#34;Invalid piece proof\u0026#34;); } Plot Identity Verification: Verify that the solution comes from a valid plot: // Step 2.1: Compute expected plot ID from farmer\u0026#39;s public key and nonce expected_plot_id = hash(farmer_public_key || plot_nonce) // Step 2.2: Verify the sector was created with this plot ID // The sector_id should incorporate the plot_id if !sector_belongs_to_plot(sector_id, expected_plot_id) { return Err(\u0026#34;Sector doesn\u0026#39;t belong to claimed plot\u0026#34;); } Range Validation: Ensure the sector\u0026rsquo;s committed history size is within the plot\u0026rsquo;s valid range: // Step 3.1: Calculate the plot\u0026#39;s base range from its nonce window_level = (current_history_size / BASE_WINDOW_SIZE).ilog2() window_size = BASE_WINDOW_SIZE * (2 ^ window_level) nonce_in_window = plot_nonce % NONCES_PER_WINDOW min_history = (window_size * nonce_in_window / NONCES_PER_WINDOW) - GENESIS_nonce max_history = (window_size * (nonce_in_window + 1) / NONCES_PER_WINDOW) - GENESIS_nonce // Step 3.2: Calculate effective range (extends with blockchain growth) effective_max_history = max(max_history, current_history_size) // Step 3.3: Verify sector\u0026#39;s history size is within range if sector_history_size \u0026lt; min_history || sector_history_size \u0026gt; effective_max_history { return Err(\u0026#34;Sector history size outside plot\u0026#39;s valid range\u0026#34;); } Shard Assignment Verification: Confirm the plot is assigned to the current shard: // Step 4.1: Determine which shard this plot belongs to // Construct VRF input using the same format as allocation vrf_input = plot_id || randomness; // Changed from min_history to randomness vrf_signature = plot_vrf_signature; // From block seal // Verify the VRF signature if !verify_vrf(plot_public_key, vrf_input, vrf_signature) { return Err(\u0026#34;Invalid VRF signature\u0026#34;); } // Calculate expected shard assignment using same logic as allocation expected_shard_id = vrf_output_to_shard(vrf_signature) % NUM_SHARDS; // Verify the claimed shard_id matches expected if claimed_shard_id != expected_shard_id { return Err(\u0026#34;Shard assignment mismatch\u0026#34;); } To perform sharded verification, clients need:\nFrom the Solution\nfarmer_public_key: The farmer\u0026rsquo;s identity plot_nonce: The nonce chosen when creating the plot sector_index: Which sector within the plot sector_history_size: The history size this sector committed to piece_offset: Which piece within the sector Standard Subspace proofs (piece inclusion, etc.) From the Block Header and the network\ncurrent_history_size: Current blockchain history size current_shard_id: Which shard is performing verification Protocol constants (BASE_WINDOW_SIZE, NUM_SHARDS, etc.) A script to tinker with and next steps! # This week I\u0026rsquo;ve also been spending some time implementing all the formal models from the last few updates into a Python script that allow us to tinker with the protocol and see how it impacts its security and correctness. So far the results are promising, but it still needs a few iterations to double-check that the model makes sense and that the protocol is sound. I am also working on try to extensively document the script so others can pick it up and play with it and/or point out potential mistakes.\nWith this in mind, this week I will be focusing on:\nMapping all the sector expiration and history range logic into pseudocode so Nazar can start implementing the validation logic that will in itself start enabling adapting plotting and farming to shards. Exploring the implementation of a simple simulator that allows us to simulate the protocol and its properties, so we can start reasoning about different parameters and its specific sub-protocols more concretely than the current Python script. Let\u0026rsquo;s see how far we get this week! I am excited to see how this will evolve and how we can start building the foundation for a more robust and scalable protocol. As always, reach out if you have any questions, suggestions, or feedback. Until next week!\n","date":"30 June 2025","externalUrl":null,"permalink":"/blog/2025-06-30-expiring-sharded-subspace-plots-and-improving-model-script/","section":"Blog","summary":"\u003cp\u003eAs mentioned on last week\u0026rsquo;s status update, one of the key pieces that I was missing to have the\ndetailed operation of plot membership allocation was the impact of sector expiration on the\nprotocol. By having a unique plot identifier, we are able to uniquely link sectors to plots, but\nthese sectors need to expire in a way that does not require farmers to re-plot while archiving the\nmost recent history. Fortunately, we can leverage the current expiration mechanism of the Subspace\nprotocol, and build a layer on top of it to adapt it to the sharded version while maintaining the\noriginal guarantees in terms of plot expiration, sector re-plotting, and history archiving.\u003c/p\u003e","title":"Expiring Sharded Subspace Plots and improving model script","type":"blog"},{"content":"This was a lighter week on meaningful changes, but there are still few things to share. First of all, last week\u0026rsquo;s block import was (and still is) and incomplete prototype, but this week it was extended to become a bit more complete. I was also working on bringing up more components frm Subspace, including farmer, which lead me to attempt GPU plotting in Rust, so let\u0026rsquo;s get into it.\nBlock processing progress # The design of the block import is such that allows for completely parallel import of even interdependent blocks, but they need to be scheduled sequentially, such that parent header is seen by the block import before the next block comes. Well, buffering of these parent headers was not implemented originally, but PR 297 finally implemented that in a fairly efficient way. Then PR 298 took advantage of the fact that there should be very few blocks buffered and the most common case is to reference the last scheduled block to replace a map with a much simpler VecDeque.\nAnother thing that was missing was Merkle Mountain Range root. The primary reason for that was that MMR itself was not implemented. I have implemented MMR (which is a more generic version of unbalanced Merkle Tree, which is in turn more generic version of balanced Merkle Tree) in PR 299. There is a bunch of tests, but more tests and some benchmarks still need to be added. With that PR 300 implemented maintenance and checking for MMR in block import. This required some changes to client API and block buffering (MMR for a block needed to be created before its import starts, see parallel block import above).\nAnother smaller thing was timestamp in the header, which together with BlockTimestamp new type introduction was implemented in PR 301 in the spirit of how it is done in Substrate\u0026rsquo;s pallet-timestamp.\nAfter looking at block structure closely and thinking about future protocol upgrades, I ended up removing version field from block header in PR 302, I no longer think it\u0026rsquo;ll be necessary.\nGetting closer to having a farmer # In order for blocks to exist, they need to be created, which implies creation of solution, which will need a farmer. So I looked at dependency tree and extracted both ab-data-retrieval and ab-farmer-components in PR 292 as modified versions of their counterparts from Subspace codebase.\nThis should technically be sufficient to plot and farm blocks in some test environment, but subspace-farmer has more heavy dependencies before it can become ab-farmer. Some like networking stack will likely remain largely the same, but GPU plotting that was broken for some time due to getting rid of KZG needs a replacement.\nI looked at CubeCL more closely and tried to prototype something, but was immediately disappointed with several things. First of all, it only works with data structures and functions/methods that are annotated with its macros, so forget about using basically anything that exists on crates.io ðŸ˜ž.\nWell, the use case is somewhat narrow, so I wouldn\u0026rsquo;t really need many dependencies out of my control to begin with, so I decided to start with writing a simple kernel that creates a ChaCha8 keystream only to be disappointed again by the fact that I can\u0026rsquo;t use [u32; 16] as an input to the kernel and workarounds are quite ugly ðŸ˜­\nI then looked again at rust-gpu more closely. The benefit of it is that it does seem to compile normal Rust code, meaning using external dependencies not aware of rust-gpu should generally be fine, but then I discovered that it is a compiler backend and is tied to a specific version of nightly rustc, which is from more than a year ago. There is a PR to update it to something from this year, but it makes slow progress and is already substantially older than what I\u0026rsquo;m targeting already. At this time I\u0026rsquo; not ready to commit to old compiler versions and even if it is only used for compiling GPU code it\u0026rsquo;ll be problematic due to various unstable features used.\nOverall, rust-gpu seems like a more straightforward and Rust-like design, while CubeCL has interesting features that may result in higher performance at the cost of Rust-but-not-really DSL, but neither of them work the way I expected them, which is a bummer. I\u0026rsquo;ll probably try again with CubeCL again this week to see how far I can go before pulling my hair out.\nOther things # There were some CI improvement: PR 303 extended checks to make sure all crates can compile separately with default feature set, PR 304 increased CI concurrency to make things finish faster overall. I could make it even faster, but that would require ugly workarounds due to GitHub Actions limitation, so I abandoned that idea for now.\nPR 296 introduced block root caching since otherwise it would have been recomputed every time BlockHeader::root() is called, which is fairly common. PR 307 improved balanced Merkle Tree performance slightly and reduced amount of unsafe there but a tiny amount.\nPR 300 already reduced owned block data structures by wrapping internals with Arc since some of these (like header) are long-living and will be cloned many times, which PR 309 followed-up with usage of rclite, offering an even more compact Arc implementation.\nUpcoming plans # I spent some time looking around and thinking about state management, Sparse Merkle Tree implementations and may start implementing something this week. This work is also a pre-requisite for block execution, which needs to deal with state and commit to state root.\nI\u0026rsquo;d like to work with CubeCL some more as well, hopefully implementing GPU plotting that may not be very fast, but at least in the right overall shape to move forward with farmer CLI.\nIn short, still a lot of work ahead, but steady progress is made every week, and you can find me on Zulip in case of any questions.\n","date":"23 June 2025","externalUrl":null,"permalink":"/blog/2025-06-23-block-processing-progress/","section":"Blog","summary":"\u003cp\u003eThis was a lighter week on meaningful changes, but there are still few things to share. First of all, last week\u0026rsquo;s block\nimport was (and still is) and incomplete prototype, but this week it was extended to become a bit more complete. I was\nalso working on bringing up more components frm Subspace, including farmer, which lead me to attempt GPU plotting in\nRust, so let\u0026rsquo;s get into it.\u003c/p\u003e","title":"Block processing progress","type":"blog"},{"content":"Last week I shared a model that can help us reason about the security of shards assuming an honest majority in the beacon chain. The model evaluates what are the trade-offs in terms of the number of shards, the number of farmers per shard, and the proportion of malicious farmers in the system. But if you recall from the overall design of the system, shards are periodically submitting segments and blocks to the upper layers of the hierarchy and to the beacon chain. We need to verify that these are valid, available, and correctly encoded before they are included in super segments and the global history of the system. Can we do so without relying on fraud proofs? This has been one of my focuses for the week, let\u0026rsquo;s jump right into it.\nFarmer reshuffling to prevent fraud proofs # Let me refresh your memories by describing how we are planning to work around requiring fraud proofs. The core idea here is that by leveraging the reshuffling of farmers (or actually plots) across the different shards, and the fact that when farmers are assigned to a new shard they need to sync and verify the latest history of that shard, we can ensure that any fraudulent segment will be detected by honest farmers.\nIf a malicious farmer in a shard creates an invalid block or segment (invalid transaction, state transition, incorrect encoding, etc.), eventually an honest farmer will be assigned to that shard during a reshuffle. This honest farmer will download and verify the latest shard\u0026rsquo;s history. Upon encountering the invalid block or segment, they will trigger a re-org that will be detected in follow-up submissions by the parent and the beacon chain. Intermediate shards and the beacon chain do not immediately accept new blocks being submitted by child shards, they wait for one or more reshuffling intervals to ensure that the data has been verified by honest farmers. In the next section I will share the model to determine the optimal values depending on the desired security level. When the honest farmer detects fraud, they will refuse to build on top of the fraudulent chain. If the shard has a sufficient number of honest farmers that detect this, they will collectively build a valid fork, effectively causing a re-org that prunes the fraudulent history. This mechanism also addresses data availability directly. If a block or a segment is unavailable, the syncing farmer cannot download it and thus cannot reconstruct the state. This inability to sync would trigger the same \u0026ldquo;re-org\u0026rdquo; or \u0026ldquo;rejection\u0026rdquo; behavior, preventing the malicious chain from progressing as new farmers will only be able to grow the chain from the latest valid state. The missing block or segment would have to be re-created. The assumption is that if data is not available, it effectively means the chain cannot be validly extended by honest participants and the chain needs to be recovered from the last known state. Skimming through the literature around sharded blockchains and fraud proofs uncovered that this approach is often referred to as \u0026ldquo;periodic reshuffling with full-state downloads\u0026rdquo; or state-sync-based fraud detection in other protocols. This looks promising, but we need to model it to understand the implications in terms of security and the trade-offs involved (especially regarding the reshuffling interval).\nModeling the Reshuffling Interval (R) # Let\u0026rsquo;s define some parameters and then formalise the security of the mechanism.\nKey Parameters: # \\(N\\): Total number of farmers (or plots) in the system. \\(S\\): Total number of shards. \\(n=N/S\\): Average number of farmers per shard. \\(p\\): Proportion of malicious farmers in the entire system. \\(f\\): Minimum proportion of honest farmers required within a shard to detect and force a re-org. This is typically \\(\u0026gt;0.5\\) (e.g., \\(f=0.51\\)) to outvote or out-compute the malicious actors. \\(T_B\\): Average block time of the beacon chain. \\(T_S\\): Average block time of a shard. \\(L\\): Number of blocks in a reshuffling interval (i.e., reshuffling happens every \\(L\\) blocks on the beacon chain). \\(R=L \\cdot T_B\\): Duration of a reshuffling interval in real time. \\(\\alpha_{fraud}\\): Acceptable probability that a fraudulent segment remains undetected after a certain number of reshuffles. This is your desired \u0026ldquo;security level\u0026rdquo; against undetected fraud. Modeling the Detection Probability: # Probability of a Malicious Shard: # Using the Chernoff bound from my previous update, the probability that a specific shard has \\( \\ge f \\cdot n \\) malicious farmers is:\n$$ P(\\text{Shard } k \\text{ is insecure}) = P(M_k/n_k \\ge f) \\le e^{-n(f-p)^2/(3p)} $$\n(assuming \\(f\u0026gt;p\\)).\nLet this be \\(P_{\\text{insecure_shard}}\\). This is the probability that a shard is immediately vulnerable upon formation.\nProbability of an Honest Farmer Being Assigned to a Shard: # In each reshuffling interval, a farmer is randomly assigned to a shard. The probability that a specific honest farmer is assigned to a specific shard is \\(1/S\\).\nMore generally, the probability that a newly assigned farmer to a shard is honest is \\(1-p\\).\nProbability of Detecting Fraud in One Interval: # Consider a shard where fraud has occurred. We need an honest farmer to be assigned to this shard.\nLet \\(k\\) be the number of farmers assigned to a shard during one reshuffling. For simplicity, let\u0026rsquo;s assume \\(k \\approx n\\) (the average).\nThe probability that at least one of the \\(n\\) newly assigned farmers to a specific shard is honest is: $$P(\\text{at least one honest farmer in new assignment}) = 1 - P(\\text{all new assignments are malicious})$$ $$P(\\text{at least one honest in new assignment}) = 1 - p^n$$\nThis is an oversimplification, as it assumes all \\(n\\) farmers are new. In reality, farmers are shuffled amongst existing shards, and some farmers might remain in the same shard, or might be replaced by other farmers. A more accurate model considers that a random subset of \\(n\\) farmers are drawn from the \\(N\\) total farmers and assigned to this specific shard, but our case is closer to the scenario where all farmers are new.\nLet\u0026rsquo;s refine: For a specific shard that has committed fraud, what\u0026rsquo;s the probability that none of the \\(n\\) farmers assigned to it in the next reshuffling interval are honest? This is \\(p^n\\) if assignments are done with replacement from the global pool (which is a reasonable approximation for large \\(N\\)).\nSo, \\(P(\\text{fraud detected by new assignment in 1 interval}) = 1 - p^n\\).\nThis assumes that if an honest farmer is assigned, they will detect the fraud and initiate a re-org.\nProbability of Fraud Remaining Undetected After k Reshuffles: # If a fraudulent segment or a block is submitted to the upper layers of the hierarchy, it potentially remains undetected until an honest farmer is assigned to that shard and syncs.\nThe probability that fraud in a specific shard remains undetected for \\(k\\) consecutive reshuffles is \\( (p^n)^k \\).\nSo, \\( P(\\text{fraud detected by interval } k) = 1 - (p^n)^k \\).\nOverall Security Bound (\\(\\alpha_{fraud}\\)): # We want the probability that any fraudulent segment or block remains undetected for \\(k\\) intervals to be very low. \\( P(\\text{any fraud undetected for } k \\text{ intervals}) \\le S \\cdot (p^n)^k \\) (Union Bound over shards)\nSetting this to \\( \\alpha*{fraud} \\): \\( S \\cdot (p^n)^k \u0026lt; \\alpha*{fraud} \\)\nThis equation allows us to reason about the trade-offs (that actually match the intuition we had):\nIncreasing \\(k\\) (more reshuffles): Decreases the probability of undetected fraud, but increases the time a segment might be considered \u0026ldquo;tentatively final\u0026rdquo; before true probabilistic finality. Decreasing \\(p\\) (fewer malicious farmers): Drastically reduces the probability, but implies a stronger honest majority assumption. Increasing \\(n\\) (fewer shards/more farmers per shard): Drastically reduces the probability, but impacts scalability. Decreasing \\(S\\) (fewer shards): Reduces the number of opportunities for fraud across the system. Solving for \\(k\\) (number of reshuffles until detection): # Given \\(S, p, n, \\alpha*{fraud}\\): \\( k \u0026gt; \\frac{\\log(\\alpha*{fraud}/S)}{\\log(p^n)} \\)\nThis \\(k\\) gives use the number of reshuffling intervals we might need to \u0026ldquo;wait\u0026rdquo; before a block or a segment can be considered probabilistically secure against undetected fraud. If a block or segment submitted to the beacon chain and eventually included in a SuperSegment at beacon height \\(H\\), and \\(k\\) reshuffles happen, the \u0026ldquo;true\u0026rdquo; finality could be argued to be at beacon height \\( H+k \\cdot L \\).\nWith this, we can determine the optimal reshuffling interval based on the desired security level (\\(\\alpha_{fraud}\\)) and the parameters of the system.\nFarming in shards # Another piece of the puzzle that I left behind from my previous update is how farmers are chosen to propose the next block in the shard they\u0026rsquo;ve been allocated to, and how can this allocation be verified by the rest of the network. Last week, we introduced the concept of a PlotID used to uniquely identify a plot (i.e. a batch of storage sectors), and used to determine the allocation of storage in the system.\nFarming in shards work in the same way as it currently works in the Subspace protocol, with the main difference that now the winning farmer will have to also present along with the block a proof that one of its plots belongs to the shard and hence is entitled to propose a new block in that shard.\nIf we look at the data structure from below, we can see that by including the plot_id that determines the allocation of a farmer\u0026rsquo;s storage to the shard and some additional information, we can determine if the farmer is entitle to propose a block in the shard.\n// -- The shard ID is explicitly shared in the block header. struct Solution { public_key_hash: 32 bytes sector_index: 2 bytes history_size: 8 bytes piece_offset: 2 bytes record_commitment: 48 bytes record_witness: 48 bytes chunk: 32 bytes chunk_witness: 48 bytes proof_of_space: 160 bytes // --- NEW FIELD FOR MEMBERSHIP AND PLOT UNIQUENESS PROOF --- // Unique identifier for the plot or the pieces of information required to infer it within the block // (e.g. public key, max plot size, min and max history size). plot_id: 32 bytes, } // Additional information that we may need to include as part of the `Solution` struct // if we make the configurable instead of protocol parameters (we can keep them out of the struct // but consider them available for verification for the sake of this discussion.) max_plot_size: 64 bytes, // Maximum size of the plot. // These two parameters may be simplified into a single one depending on how we end up implementing re-plotting and // the expiration of sectors as it will be described below. min_history_size: 8 bytes, // Minimum history size the sectors in the plot are allowed to commit to. max_history_size: 8 bytes, // Maximum history size the sectors in the plot are allowed to commit to. // This parameters determines that randomness used in the last reshuffling interval to determine the membership allocation, // and will be required to determine that the proposing farmer is currently allocated to the shard. membership_allocation_randomness_seed At a really high-level these are the steps required to verify that a block includes the right solution, and that the farmer plot is allocated to the shard:\nStandard Subspace PoAS Verification: It works in the same as it currently works in Subspace.\nVerify public_key is not in block list. Verify consensus log (solution_range, global_randomness). Verify PoT items. Compute global_challenge = hash(global_randomness || slot_number). Verify current_chain_history_size \u0026gt; winning_piece_index / NUM_PIECES. (This is key for history_size validity). Verify piece_offset \u0026lt;= max_pieces_in_sector. Derive public_key_hash = hash(public_key) (already in current process). Re-derive the sector_id Verify Implicit plot_id Consistency and Uniqueness Constraints:\nRe-derive declared_plot_id: declared_plot_id = keyed_hash(solution.public_key_hash, solution.max_plot_size || solution.min_history_size || solution.max_history_size) (Note: public_key_hash is computed from block.public_key). Verify history_size falls within plot\u0026rsquo;s declared window: Check that solution.history_size \u0026gt;= solution.min_history_size AND solution.history_size \u0026lt;= solution.max_history_size. This is the core check preventing \u0026ldquo;double-committing sectors\u0026rdquo; to different history sizes for allocation bias. If a farmer tries to submit a sector_id that uses a history_size outside the declared [min_history_size, max_history_size] window for that plot_id, the block is invalid. Derive constrained_sector_id: constrained_sector_id = keyed_hash(declared_plot_id, solution.sector_index || solution.history_size) This is the critical new sector_id that incorporates the plot\u0026rsquo;s history window constraint. Now, use constrained_sector_id for all subsequent verification steps that currently use the public_key_hash derived sector_id: Replace sector_id in sector_expiration_check_history_size, expiration_history_size, sector_slot_challenge, s_bucket_audit_index, evaluation_seed with constrained_sector_id. This means the farmer\u0026rsquo;s proof must ultimately derive from a sector_id that respects the plot_id\u0026rsquo;s declared history boundaries. Verify Shard Assignment (incorporating plot_id):\nRetrieve the randomness_seed from the beacon chain state root at solution.shard_assignment_seed_beacon_height and solution.shard_assignment_seed_beacon_block_hash. (This assumes the verifier has access to canonical beacon chain history). Compute expected_shard_id = VRF_verify(randomness_seed || declared_plot_id) % S. Check that expected_shard_id == ShardBlock.header.shard_id. This proves the farmer was legitimately assigned to propose for this shard based on their declared plot. Complete Existing Subspace PoSt Verification (using constrained_sector_id):\nVerify sector_id expiration (now using constrained_sector_id). Re-derive sector_slot_challenge = global_challenge XOR constrained_sector_id. Re-derive s_bucket_audit_index = sector_slot_challenge mod NUM_S_BUCKETS. Re-derive evaluation_seed = hash(constrained_sector_id || piece_offset). Verify proof_of_space. Ensure chunk satisfies challenge criteria (using sector_slot_challenge from constrained_sector_id). Verify chunk_witness. Re-derive piece_index (this uses history_size, which is checked against plot bounds). Retrieve segment_commitment (needs to match history_size). Verify record_witness. Verify farmer_signature on chunk. Verify signature on block content. Check for equivocation. What\u0026rsquo;s next? # The farming protocol presented above assumed a really specific mechanism for sector expiration and re-plotting. Unfortunately, after discussing it with Nazar we realised that my re-plotting proposal had some holes, so while the high-level of the verification stands, expect some minor changes in the information included in the header and the verification process. This will mainly be related with the history size window of the plot. I don\u0026rsquo;t expect any big refactor to be needed here, but stay tuned!\nWith this in mind, my two main focus for this week are on:\nDetailing the protocol for sector expiration in plots while maintaining the commitment of plots to a specific history size through their plot ID. Updating the python script that I wrote a few weeks ago that allows us to model and tweak the parameters of the protocol with the new model of the past two weeks merging all together to try to surface the optimal protocol parameters and what that entails to the security of the system. And that\u0026rsquo;s all for this week, looking forward to share my progress next week (hopefully with a big batch of good results).\n","date":"23 June 2025","externalUrl":null,"permalink":"/blog/2025-06-23-reshuffling-interval-and-living-without-fraud-proofs/","section":"Blog","summary":"\u003cp\u003eLast week I shared a model that can help us reason about the security of shards assuming an honest\nmajority in the beacon chain. The model evaluates what are the trade-offs in terms of the number of\nshards, the number of farmers per shard, and the proportion of malicious farmers in the system. But\nif you recall from the overall design of the system, shards are periodically submitting segments and\nblocks to the upper layers of the hierarchy and to the beacon chain. We need to verify that these\nare valid, available, and correctly encoded before they are included in super segments and the\nglobal history of the system. Can we do so without relying on fraud proofs? This has been one of my\nfocuses for the week, let\u0026rsquo;s jump right into it.\u003c/p\u003e","title":"Reshuffling interval and living without fraud proofs","type":"blog"},{"content":"There was no update last week again since I didn\u0026rsquo;t feel like there was enough to share at the time, but now that more things have settled I\u0026rsquo;d like to share what I\u0026rsquo;ve been busy with.\nBlock production/import work # As mentioned in last few updates, block production remained the goal. I\u0026rsquo;m currently focusing on beacon chain block, but APIs overall should be later applicable to intermediate and leaf shard blocks as well. Each kind of shard has its own nuances, so I\u0026rsquo;m focusing on one at a time.\nConsensus in Subspace is coupled with Substrate quite tightly, so I focused on reducing that in PR 273 and PR 274. PR 274 introduced generic block-related traits, extended in PR 275, such that block production and import APIs can describe abstract interfaces for any kind of block being produced/imported.\nWith all that in place, PR 278 finally introduces slot worker and initial abstract interfaces for block production. It was just an abstract interface that doesn\u0026rsquo;t care about specific kind of shard. PR 280 was built on top and implemented a prototype of beacon chain block production, which was extended to block import in PR 286, though \u0026ldquo;import\u0026rdquo; doesn\u0026rsquo;t have any block execution yet, just verification of consensus rules.\nAt this point a lot of things are still missing: block execution doesn\u0026rsquo;t exist, MMR is not missing, child shards are not a thing yet either, state management, the list goes on\u0026hellip; It is not really usable yet, but with a few more steps it should be able to actually build and import blocks, at least locally. BTW, block import is conceptually split into verification and execution, which should help with light client implementation.\nOne key difference from Subspace, which I think will help with reviews and understanding of the code, is that it assumes availability of sufficient number of recent blocks for consensus purposes in RAM instead of reaching out to state all the time. This required a special API of the block verification/import, but as the result avoids storage and maintenance of intermediate values, which results in a bunch of tricky logic that makes thing look more complex than the protocol specification would suggest. This, again, helps light clients to reuse the code of the reference implementation and avoid the need to support execution environment in any capacity, at least for now.\nOther changes # PR 282 simplified implementation and improved Merkle Tree performance a bit, I\u0026rsquo;m quite pleased with how it looks right now even though more performance is still left on the table. After that PR 283 introduced no-panic feature to Merkle Tree implementation, guaranteeing that its implementation is completely panic-free (except memory allocation code, of course), which together with extensive tests and Miri should provide strong reliability guarantees. I\u0026rsquo;ll soon be working on Merkle Mountain Range, which will likely share some code with unbalanced Merkle Tree implementation. To make sure performance doesn\u0026rsquo;t regress, PR 288 finally implemented Merkle Tree benchmarks.\nFrom a different side PR 284 improved block APIs with better efficiency using self-referential data structures to avoid repeated parsing work, especially for cached block headers and to provide. PR 287 refactored public APIs to make key block-related data structures #[non_exhausive], which forces users to go through provided constructors, which in turn allows to simplify APIs that convert to owned data structures since the only way to create non-owned data structure is going through constructors that ensure proper invariants. It is still currently possible to mess up data structures after they have been created, which I\u0026rsquo;ll probably fix in the near future too.\nUpcoming plans # This week I plan to work on MMR, super segment abstraction, look into state management, block execution, probably port farmer RPC interface so that it is possible to start block production.\nCurrently, farmer crate depends on now broken GPU plotting implementation crate, so if I\u0026rsquo;m in the mood I might try to implement something using CubeCL. Will probably not be very performant initially, but should at least serve as a stopgap and inform potential API changes while hopefully avoiding coupling to AMD/Nvidia libraries during compile time (need to look closer into it). If it is really ergonomic to use, might open possibilities to use GPU acceleration more widely in the node too, not just farmer.\nWith enough components in place it should become possible to prototype node CLI implementation. With more clarity in hierarchical consensus, it should be possible to start prototype intermediate and later leaf shard block production/import.\nBasically there are more things left to do than those which were done already, but we\u0026rsquo;re making progress. Zulip chat is the best place to ask questions and discuss future work.\n","date":"16 June 2025","externalUrl":null,"permalink":"/blog/2025-06-16-beacon-chain-block-processing-infrastructure/","section":"Blog","summary":"\u003cp\u003eThere was no update last week again since I didn\u0026rsquo;t feel like there was enough to share at the time, but now that more\nthings have settled I\u0026rsquo;d like to share what I\u0026rsquo;ve been busy with.\u003c/p\u003e","title":"Beacon chain block processing infrastructure","type":"blog"},{"content":"This week has been all about objectively assessing the security of the system. After all of the work around the membership allocation protocol and its security there are still two questions that we need to answer to understand the feasibility of the protocol: (i) what is the security bound of the protocol as a whole (from beacon chain to shards), and (ii) how can we ensure that plots are uniquely identified and that farmers cannot cheat by committing the same plot to different history sizes to try and game the shard allocation mechanism.\nComputing the security bound of the protocol # The beacon chain and all shards in the system run a longest chain consensus. All farmers are dedicating their storage to protect the beacon chain. From this, we can easily derive that as long as there\u0026rsquo;s at least 51% of the storage in the system that is honest, the beacon chain will be secure. Unfortunately, this is not the real security bound for the protocol, as we want to understand what is the minimum amount of honest storage that we need as a whole (from beacon chain to shards) to ensure for all shards to be honest with high probability. The overall security bound, thus, will be determined by the maximum proportion of malicious nodes such that all shards are secure with a high probability. This means we need to ensure that the probability of any shard having 50% or more malicious farmers is very low.\nWe make the following assumptions to compute this probability:\nTotal Malicious Nodes: Let \\( p \\) be the proportion of malicious nodes in the entire system (i.e., among all farmers). So, if \\( N \\) is the total number of farmers, then \\( pN \\) are malicious. Beacon Chain Security: The beacon chain is secure as long as less than 50% of its participants are malicious. Shard Security: A shard is secure if the proportion of malicious farmers within that shard is less than 50%. Uniform and Random Allocation: Each farmer (malicious or honest) has an equal probability of being assigned to any given shard, assuming all of them have pledged the same amount of storage. This is a safe assumptions considering the fact that we are considering plots (and not farmers) as the basic unit of membership allocation. Large Number of Farmers and Shards: For statistical analysis, we\u0026rsquo;ll assume a sufficiently large number of farmers and shards so that we can use probabilistic approximations. In the script I was working on last week we can refine these numbers to evaluate the impact of the number of nodes and shards. We base our analysis on the Chernoff bound, which provides a way to estimate the probability of deviations from the expected value in a binomial distribution.\nNote: For this analysis I am using farmer and plot indistinctly as if each farmer had a single plot of maximum size. If you recall from my previous updates, and as described above, we are considering plots and not farmers as the minimal unit of membership allocation. Farmers are free to decide how they organise their storage into plots (although rationally, they will try to fit as much storage as possible into a single plot to reduce the number of shards they need to sync with).\nLet\u0026rsquo;s consider a single shard. Suppose there are \\( S \\) shards in total, and \\( N \\) total farmers. On average, each shard will have \\( N/S \\) farmers. Let \\( n_k \\) be the number of farmers in shard \\( k \\). Due to random allocation, \\( n_k \\) will vary, but for large \\( N \\) and \\( S \\), it will be close to \\( N/S \\).\nLet \\(X_i\\) be an indicator random variable for farmer \\(i\\) being malicious.\nFor a specific shard \\( k \\), let \\( M_k \\) be the number of malicious farmers in that shard. We want \\( M_k / n_k \u0026lt; 0.5 \\).\nSince farmers are allocated uniformly and randomly, the distribution of malicious farmers within a shard can be modeled by a binomial distribution (as discussed in PR277). If a shard has \\( n \\) farmers, the number of malicious farmers \\( M \\) in that shard follows \\( B(n, p) \\).\nWe are interested in the probability that \\( M/n \\geq 0.5 \\). This is \\( P(M \\geq 0.5n) \\).\nFor a system to be secure, all shards must be secure. This means we want the probability of at least one shard being insecure to be very small.\n$$ P(\\text{System Insecure}) = P(\\exists k \\text{ such that } M_k / n_k \\geq 0.5) $$\nUsing the union bound, we can say:\n$$ P(\\text{System Insecure}) \\leq \\sum_{k=1}^S P(M_k / n_k \\geq 0.5) $$\nIf we assume all shards have approximately \\( n = N/S \\) farmers, then:\n$$ P(\\text{System Insecure}) \\leq S \\cdot P(M/n \\geq 0.5) $$\nNow, we need to bound \\( P(M/n \\geq 0.5) \\). Since \\( E[M/n] = p \\), and we want \\( M/n \u0026lt; 0.5 \\), we need \\(p\\) to be significantly less than 0.5.\nWe can use a Chernoff bound. For a sum of independent Bernoulli random variables, if \\( X_1, \\dots, X_n \\) are i.i.d. Bernoulli with \\( P(X_i = 1) = p \\), and \\( X = \\sum X_i \\), then for \\( \\delta \u0026gt; 0 \\):\n$$ P(X \\geq (1+\\delta)np) \\leq e^{-np\\delta^2 / 3} $$\nIn our case, we are interested in the upper tail, but when \\( p \u0026lt; 0.5 \\), we are interested in the probability that \\( M/n \\) exceeds its mean. Let\u0026rsquo;s rephrase it. We are looking for the probability that the observed proportion of malicious nodes \\( (M/n) \\) is \\( \\geq 0.5 \\), given the true proportion is \\( p \\).\nWe set \\( (1+\\delta)p = 0.5 \\), so \\( \\delta = (0.5/p) - 1 \\). This bound is for \\( \\delta \u0026gt; 0 \\), which means \\( 0.5 \u0026gt; p \\). If \\( p \\geq 0.5 \\), the probability of a shard being insecure becomes very high.\nSo, \\( P(M/n \\geq 0.5) \\leq e^{-np((0.5/p)-1)^2 / 3} = e^{-n(0.5-p)^2 / (3p)} \\)\nLet \\( \\epsilon = 0.5 - p \\). This is the \u0026ldquo;security margin\u0026rdquo; for each shard. \\( P(M/n \\geq 0.5) \\leq e^{-n\\epsilon^2 / (3p)} \\)\nNow, we need \\( S \\cdot e^{-n\\epsilon^2 / (3p)} \\) to be very small (e.g., \\( 10^{-6} \\) or \\( 10^{-9} \\)).\nTo provide a general number, we need to define the acceptable probability of an insecure system. Let this be \\( \\alpha \\).\nThe condition for system security is roughly:\n$$ \\frac{N}{S} \\cdot e^{-n(0.5-p)^2 / (3p)} \u0026lt; \\alpha $$\nWhere:\n\\(N\\): Total number of farmers in the system. \\(n\\): Average number of farmers per shard (\\( n=N/S \\), where \\( S \\) is the number of shards). \\(p\\): Proportion of malicious farmers in the entire system. \\(\\alpha\\): Acceptable probability of the overall system being secure (e.g., \\( 10^{-6} \\) for very high security). You would then solve for \\(p\\). As shown in the example, this often leads to a quadratic inequality.\nTranslating it into numbers # Based on the analysis, the percentage of malicious nodes (\\( p \\)) that the overall system can afford will be significantly less than 50%. The exact value depends critically on:\nThe total number of farmers (\\( N \\)) in the system. The number of shards (\\( S \\)) you choose, which determines the average number of farmers per shard (\\( n \\)). The desired probability of the overall system being secure (\\( \\alpha \\)). As a rule of thumb, to achieve a high degree of confidence that all shards remain secure, \\( p \\) will likely need to be in the range of 30-45%, depending on the specific parameters (\\( N, S, \\alpha \\)). It will always be a value smaller than 50% and decrease as the number of shards increases (for a fixed total number of farmers) or as the desired security level increases.\nTo get the specific bound we need to choose the specific parameters desired (\\( N, S, \\) and \\( \\alpha \\)) and then calculate the \\( p \\) value using the Chernoff bound (or a similar concentration inequality) to get a precise answer for your design.\nIf we, for instance, consider total number of farmers \\( N = 1000 \\), number of shards \\( S = 100 \\), and an average number of farmers per shard \\( n=N/S=10,000 \\) with a security level of \\( \\alpha = 10^{-2} (1%)\\), we get that \\( p \\) should be approximately less than 46.42%.\nThe trade-off between the number of shards (which improves scalability) and the security threshold (\\( p \\)) is clear, and it aligns with our intuition so far. More shards mean each shard has fewer farmers (for a fixed total number of farmers), making individual shards more susceptible to higher concentrations of malicious nodes.\nRealising plot Uniqueness # In Subspace, plots do not have a unique identifier, and are sectors (that conform a plot) the ones uniquely identified. This is the way in which is done: sectors are indexed sequentially and for each sector their ID is derived through sector_id = keyed_hash(public_key_hash, sector_index || history_size), where sector_index is the sector index in the plot and history_size is the current history size at the time of sector creation. This means that farmers can easily create sectors with the same sector index that commit to different history sizes so they can keep them around to include them in a plot when is convenient for them to achieve the desired allocation.\nTo prevent this attack, the idea is to assign a unique identifier to each plot that bounds the plot to a specific window of history sizes, and uniquely assign sectors to plots based on this (i.e. sectors from a plot need to be committed to a specific history size in the range bound by the plot). Additionally, the specific size of the history size window allowed for a plot will depend on the maximum size that the farmer want to assign to that plot. This way, we limit the surface of the attack were a farmer tries to create different sectors with the same sector ID in a plot to try and bias the allocation of shards in their favour.\nThus, this is how the plot and sector IDs are derived and bound together:\nplot_id = keyed_hash(public_key_hash, max_plot_size || min_history_size || max_history_size) sector_id = keyed_hash(plot_id, sector_index || history_size) With this approach we are:\nPreventing Double-Committing Sectors: A farmer cannot commit the \u0026ldquo;same\u0026rdquo; sector ID to different history_size values and then choose the most convenient one for membership. Why? Because the history_size is part of the sector_id. If history_size_A is used, it produces sector_id_A. If history_size_B is used, it produces sector_id_B. These are distinct. If both history_size_A and history_size_B fall within the [min_history_size, max_history_size] range of the same plot_id, the farmer would effectively be storing two different sectors (with different piece selections) at the same sector_index within that plot, which is impossible. Each (plot_id, sector_index) pair uniquely maps to a single valid sector_id based on its history_size at creation time. Ensuring Verifiability: All parameters necessary for plot_id and sector_id derivation are either public (on-chain registered plot_ids) or committed by the farmer in proofs (sector_index, history_size). This allows full independent verification by any node. The max_plot_size parameter in the plot_id ensures that farmers must commit to a specific maximum plot capacity upfront, preventing them from retroactively adjusting plot boundaries to game the allocation mechanism. If we don\u0026rsquo;t want to make the history window size have to be shared in plaintext off-band, and we want the verification to be self-contained, we can force a specific size for the history window (so it is enough with sharing the beginning of the history window), or we can make a commitment on the history window size so that given the history size that a sector is committed to, we can succinctly verify that the sector is valid for that history size of the plot. This verifiability will allow us to include membership allocation proofs in the block proposal to verify that the farmer is entitled to propose a block in that shard. What\u0026rsquo;s next? # We are making pretty good progress on surfacing all the existential risks to the system and modelling it in a way that allows us to clearly reason about the different trade-offs we may incur in. Unless something unexpected comes up, my focus for the week is to:\nEmbed the high-level plot ID proposal that I shared into the membership allocation protocol for verification. With this, continue modeling the security of membership allocation and the security of the protocol as a whole so we can have a clearer model of this looks like, and if this design satisfy our security requirements without the need of fraud proofs or off-band mechanisms. Finally, I want to spend some time fleshing out the segment submission description from PR267 after the most recent feedback to start integrating into the block proposal in shards (as Nazar has already made good progress to make this work practically and it would be great to unblock him). ","date":"16 June 2025","externalUrl":null,"permalink":"/blog/2025-06-16-thinking-about-the-overall-security-of-the-system/","section":"Blog","summary":"\u003cp\u003eThis week has been all about objectively assessing the security of the system. After all of the work\naround the membership allocation protocol and its security there are still two questions that we\nneed to answer to understand the feasibility of the protocol: (i) what is the security bound of the\nprotocol as a whole (from beacon chain to shards), and (ii) how can we ensure that plots are\nuniquely identified and that farmers cannot cheat by committing the same plot to different history\nsizes to try and game the shard allocation mechanism.\u003c/p\u003e","title":"Thinking about the overall security of the system","type":"blog"},{"content":"Last week I shared a high-level of how I was thinking farmer membership selection should work. After some discussions early in the week, we realised there were still some blind spots and attacks that we weren\u0026rsquo;t protecting against (or if we were, we didn\u0026rsquo;t have an objective measure of how robust they were). Thus, this week has been exclusively focused on modeling the membership selection protocol so we can reason objectively about its design.\nModelling farmer membership selection # I was half-way in the process of writing this update when I realised that I was just paraphrasing a lot of the text that was already part of PR277. This PR presents a draft proposal for the membership selection protocol, along with an analysis of the security guarantees of the protocol for different kinds of adversary, mainly static and fully adaptive adversaries.\nThis set the groundwork to identify the optimal protocol parameters to ensure the security of the allocation protocol, and to what extent it is technical feasible. And to help with this process, you\u0026rsquo;ve probably seen that as part of the PR I\u0026rsquo;ve included a really simple Python script that allows you to input different parameters and assumptions into the model to see how the security and feasibility of the protocol changes. The script even includes a simple estimation of the economic cost required for a farmer to pull off an attack with high probability.\nThe following image shows the output of a sample execution of the aforementioned script (do not pay attention at the specific numbers, these are just some preliminary tests I\u0026rsquo;ve been doing).\nUseful paper and next steps # One of the key issues that were bugging me when working on the allocation mechanism was the modelling of the fully adaptive adversary. I had the intuition of how to protect against it, but I didn\u0026rsquo;t know how to model it. Fortunately, Nazar pointed me once again to the right place to unblock me. When I described to him what I was trying to design to protect against a specific attack from an adaptive adversary he mentioned that it reminded him of the Free2Shard: Adaptive-adversary-resistant sharding via Dynamic Self Allocation. Coincidentally, I read this paper when it was released but completely forget about it, and it includes a really nice analysis of fully adaptive adversaries and how their Free2Shard proposal can protect against it. This paper gave me all the tools that I needed for the analysis. I highly recommend everyone to give it a quick read, as it also includes a nice introduction where it presents the different approaches and architectures used to implement sharded blockchains, and the benefits and disadvantages of each approach. It can give you a good intuition of the challenges behind these designs, and where our proposal fits in.\nSo what\u0026rsquo;s next? I\u0026rsquo;ll keep having a few back-and-forths with Nazar to understand the technical feasibility of this allocation protocol by running different scenarios through my script, and I want to move next to detailing the specific operation of farming and block proposal in shards. Let\u0026rsquo;s see how this goes\u0026hellip; see you next week!\n","date":"9 June 2025","externalUrl":null,"permalink":"/blog/2025-06-09-modelling-farmer-membership-allocation/","section":"Blog","summary":"\u003cp\u003eLast week I shared a high-level of how I was thinking farmer membership selection should work. After\nsome discussions early in the week, we realised there were still some blind spots and attacks that\nwe weren\u0026rsquo;t protecting against (or if we were, we didn\u0026rsquo;t have an objective measure of how robust they\nwere). Thus, this week has been exclusively focused on modeling the membership selection protocol so\nwe can reason objectively about its design.\u003c/p\u003e","title":"Modelling farmer membership allocation","type":"blog"},{"content":"This week has been another of those weeks where I am pretty happy with the progress made. The highlights of the week are the following: (i) we now have a pretty good sense of the end-to-end operation to commit shard segments into the global history of the beacon chain (as described in the discussion of PR267); (ii) and Nazar had an idea to tackle the verification of the availability and correctness of shard segments without requiring an independent data availability mechanism, by leveraging the longest-chain rule and farmers membership allocation (which was an issue that was really bugging me). Let\u0026rsquo;s jump into the details of these two topics.\nDetailing the flow of information between shards and the beacon chain # If we look at the current design of the Subspace consensus protocol, it is composed of three main stages:\nFarming, which is responsible for the proposal of new blocks. Archiving, which takes care of preparing the history of the blockchain for archival through the creation of segments. Plotting, which is responsible for actually archiving the history by aggregating segments from the history into plots. These plots are used as the sybil-resistant mechanism of the protocol, and they determine the next farmer entitled to propose a new block in every slot. In order to propose new blocks, we first need to have segments created and archived into plots so that we can determine farmers\u0026rsquo;eligibility based on their allocated space. This is why, once figured out the high-level architecture of a hierarchical protocol like ours, the first mechanism that we started designing was the archiving process, so we could build upon it for the rest of the stages of the protocol.\nThe last few weeks, Nazar has done an amazing job implementing the basic data structures and the \u0026ldquo;pipes\u0026rdquo; that enable the flow of information from shards to the beacon chain (mainly through the ShardBlockInformation data structures that we\u0026rsquo;ve been discussing in previous updates). Building upon that and with the latest realisation that we may be able to embed all core protocol verifications and base the security guarantees as part of the longest-chain rule, in PR267) I re-wrote the end-to-end steps that describe how to make segments available in the beacon chain and committed in the global history so that they can be archived into plots.\nUnfortunately, a missing piece in the current design is the verification of the availability and correctness of shard segments before they can be archived. Initially, I thought that to achieve this we would necessarily need an orthogonal data availability mechanism that periodically samples shard segments, verify their correctness, and report any issues to the beacon chain. However, this would introduce additional complexity and overhead to the protocol, which we wanted to avoid.\nLuckily, Nazar had a brilliant idea to tackle this problem by leveraging the re-shuffling of farmers across shards. Every farmer is assigned to a new shard (or a set of them), they may need to sync with the latest state of that shard, and we can potentially leverage that work that it already need to do to verify recent segments and report any misbehaviors to the beacon chain. This, however, means that before we can detail the process of shard segment verification we need to make quick detour on our design process to get a better sense of how we want farmer membership to work in the system.\nFarmer membership allocation for segment verification # The main goal of the membership selection mechanism is to ensure: (i) the load balancing of \u0026ldquo;power\u0026rdquo; (plotted space) across all shards, maintaining an equally high and consistent level of security for each shard, and preventing potential attacks over shards due to the membership stickiness of farmers to specific shards that could lead to collusion, or the dilution of power due to farmer churn.\nFor this, we will consider plots as the basic unit of membership, and we will design a membership selection mechanism that allows farmers to be assigned to shards based on their plotted space, while also ensuring that the membership is dynamic and can adapt to changes in the network conditions (e.g. farmer churn, changes in the total plotted space, etc.).\nFor this, we need to create a deterministic and verifiable mapping between a plot\u0026rsquo;s unique identity and its assigned shard. We need to prevent malicious actors from gaming the system by pre-calculating shard assignments or attempting to concentrate plots in a single shard. Ideally, this mechanism should be oblivious, so even light clients are able to verify the membership for a specific epoch without requiring the full list of members or any external information.\nHere\u0026rsquo;s a high-level idea of how this allocation mechanism could work, and that I am hoping to get drafted in more detail the next few days:\nWhat determines if a farmer needs to sync with a shard is their plots. Plots are the basic unit of membership, and they are assigned to shards based on a deterministic mapping. Each plot has a unique identity that depends on the farmer\u0026rsquo;s public key, the sector index, the global history size, and a unique public key that farmers will generate for each plot (they may choose to re-use their own public key for this). Each MEMBERSHIP_RESHUFFLE_INTERVAL which is a protocol parameter that determines the number of slots between membership re-shuffles, farmers will draw the PoT randomness for that slot to determine locally the new allocation for their slots. This new membership doesn\u0026rsquo;t come to effect immediately, there is a NEW_MEMBERSHIP_WARMUP_INTERVAL also specified in number of slots that is the interval that farmers have to prepare for their new allocation (by syncing with the shard, verifying the most recent segments, and any other operation that may be required). All this protocol parameters will be determined by the security robustness of the protocol against the potential attacks described above, and the technical limitations in terms of warmup requirements and overhead of frequent re-shuffles. I already have a sketch for the mathematical model that will help us analyse this, and in this discussion Nazar has already shared some number of the technical requirements that we need to consider in terms of syncing times and plotting performance. Once the new farmer membership comes to effect, farmers will only be allowed to propose blocks in their assigned shard, and they will need to prove their membership by signing the new block with the public key used to derive the identity of the assigned plot. There are still a few low-level details that need to be fleshed out here, but once we agree on the basic operation of the membership selection mechanism, we can start detailing the verification of segments. Even more, I am thinking that we may be able to implement the membership selection in a way that allows us to roll-out a simple allocation protocol (e.g. an homogenous epoch-based re-shuffling as the one described above), that is progressively improved into more complex space-weighted assignments that introduce improvements to the simple approach.\nSome reading and what\u0026rsquo;s next? # This week I also came across the whitepaper of Solana\u0026rsquo;s new consensus protocol: Apenglow. I spent some time reading it and thinking about how the ideas presented in it can be applied to our design (you can follow the this discussion with pointers to the announcement, the paper and the implementation code). My feeling is that their proposal prioritises block throughput and finality over security, presenting a pretty relaxed security model that tolerates 20% of the stake being malicious, and an additional 20% of it being unavailable (crash-tolerant). It is interesting how they leverage error correction to disseminate blocks (and the underlying votes) in a way that a subset of shreds are enough to validate a block parallelising as much as possible the proposal of blocks. The paper includes interesting ideas, but considering a security and trust model that is a bit too relaxed for us. In any case, an interesting read for those of you that like reading about consensus protocols.\nAnother interesting read that I would also recommend everyone interested in the project is our meeting notes from this week. They may be a bit intelligible, but there are a lot of nice ideas that came out of our discussion and worth skimming through.\nAnd to wrap this up, what\u0026rsquo;s up for this week? As mentioned above, I want to have a first detailed draft of the membership selection protocol in order to try to fit the segment verification it, and see if it fulfills all our needs or if we need to explore other ideas.\nAs always, I\u0026rsquo;ll keep you posted with the progress, and see you next week!\n","date":"2 June 2025","externalUrl":null,"permalink":"/blog/2025-06-02-membership-selection-and-segment-verification/","section":"Blog","summary":"\u003cp\u003eThis week has been another of those weeks where I am pretty happy with the progress made. The\nhighlights of the week are the following: (i) we now have a pretty good sense of the end-to-end\noperation to commit shard segments into the global history of the beacon chain (as described in the\ndiscussion of \u003ca\n  href=\"https://github.com/nazar-pc/abundance/pull/267\"\n    target=\"_blank\"\n  \u003ePR267\u003c/a\u003e); (ii) and Nazar had an idea\nto tackle the verification of the availability and correctness of shard segments without requiring\nan independent data availability mechanism, by leveraging the longest-chain rule and farmers\nmembership allocation (which was an issue that was really bugging me). Let\u0026rsquo;s jump into the details\nof these two topics.\u003c/p\u003e","title":"Membership selection and segment verification","type":"blog"},{"content":"There have not been an update from me last week, how come? Well, it didn\u0026rsquo;t seem like there was anything particularly substantial to share, mostly some refactoring, so I decided to skip it. This week though I have a few words to say about the path towards block production (not there yet, but getting closer). Some this is a boring process, I procrastinated some too, diving into various topics with some interesting performance improvements and new learnings.\nSharding architecture # Block-related changes # In the previous update I explained the way block is going to look like, but that was only parsing part of the binary format, there was no owned data structures to move around, send over the network, etc. Owned version of data structures landed in PR 253 with some follow-up changes in PR 254. I then looked at reward singing and noticed we didn\u0026rsquo;t actually benefit from advanced features of Sr25519, so I migrated to Ed25519 to make it easier for third-parties to build custom farmers and verify blocks, especially in other languages, in PR 251 and PR 252. Funnily enough, we may need to switch back depending on how we need to modify consensus implementation for sharding, but we\u0026rsquo;ll see when we get there.\nRefactoring of components and reducing Substrate dependencies # Now to build and verify blocks, proof of time is crucial, but they were still under subspace directory. I refactored and moved subspace-proof-of-{space,time} into crates/shared/ab-proof-of-{space,time} in PR 256. I even made PoT verification a bit faster by removing heap allocations from it (both alloc and std features were removed).\nThen I went on to refactor sc-subspace-proof-of-time to make it better suited for separation from Substate dependencies in PR 262, eventually extracting crates/node/ab-client-proof-of-time, which was very similar to sc-subspace-proof-of-time, except thanks to refactored abstractions I was able to move just local timekeeper, leaving block import and gossip handlers in sc-subspace-proof-of-time for now. This way I can continue work on block production without implementing networking stack and having the whole block import pipeline working first. Local timekeeper is all I need to get started, but the API is already in place to bring other components when they are ready.\nWith that done I moved on to massaging sc-consensus-subspace in PR 265, whose slot worker was refactored to integrate the minimal logic of sc-consensus-slots without relying on it anymore. This is a stepping stone for disentangling it from Substrate completely. More work is needed, but that is basically where I am so far.\nBTW the node and farmer implementations inherited from Substrate are still functional enough to produce blocks, etc. A lot of original logic is gone by now of course, but at least I have something to sanity check my changes against.\nPerformance improvements # All that is often boring mechanical work or something that feels really vague and unbounded, which is procrastination-inducing. What that means is that I\u0026rsquo;m spending time reading about random things and doing experiments that sometimes lead to unexpected performance improvements.\nThere are Proof-of-Time and Proof-of-Space components in the protocol design, which are performance sensitive. Especially since sync and plotting performance will dictate parametrization and to some degree feasibility of sharding design, they need to be as fast and efficient as possible.\nProof-of-Time # Since I was working around PoT a bit and already improved its verification performance by ~8-9% by avoiding heap allocations, I recalled that aes crate still doesn\u0026rsquo;t support VAES, so I decided to implement an optimized version for PoT with VAES specifically myself in PR 260. The results are awesome, on Zen 4 CPU (AVX512-capable) the time to verify PoT reduced ~2x, taking overall 16x less time than proving and ~10x less time than the fastest CPUs out there can prove:\nBefore: verify time: [204.92 ms 205.10 ms 205.30 ms] After: verify time: [102.38 ms 102.43 ms 102.53 ms] Then I noticed something strange in CI when working on extending no-panic to ab-proof-of-time in PR 264 and later enabling it on Windows in PR 268, that I decided to not only implement it for AVX512-capable CPUs, but also for a few of those that support VAES with AVX2 only (2 blocks at a time rather than 4) and even optimized version for many older CPUs with just AES-NI in PR 269.\nWhile in a mood, I ended up implementing optimized version for aarch64 as well in PR 270, which on Raspberry PI 5 SBC allowed to substantially improve performance (my guess is that Apple Silicon CPUs will improve as well, but I do not own any):\nBefore: verify time: [1.1665 s 1.1817 s 1.1976 s] After: verify time: [835.30 ms 835.30 ms 835.31 ms] Proof-of-Space (Chia) # Proof-of-Space was next. Over last 2 years I learned a lot about performance on modern CPUs and how to optimize Proof-of-Space, but I was still repeatedly trying to improve its performance, often unsuccessfully. Last week though I did make substantial progress again.\nFirst there was PR 266, where I managed to optimize code generation to make compiler vectorize more code for me by structuring it a little differently. As a nice coincidence, the code became more readable too, which is not always the case when optimizing for performance.\nBefore: chia/table/single time: [1.0668 s 1.0756 s 1.0862 s] chia/table/parallel/8x time: [905.01 ms 919.11 ms 933.34 ms] chia/verification time: [11.094 Âµs 11.533 Âµs 11.985 Âµs] After: chia/table/single time: [1.0245 s 1.0329 s 1.0432 s] chia/table/parallel/8x time: [830.37 ms 843.37 ms 855.47 ms] chia/verification time: [8.9454 Âµs 8.9612 Âµs 8.9726 Âµs] I also was looking at GPU implementation more closely last few weeks and tempted to try to implement Proof-of-Space table creation on GPU using CubeCL. Implementation in Subspace is designed for CUDA and ROCm, but it only supports GTX 16xx+/RTX 20xx+ GPUs on Nvidia side and on AMD side it is quite awkward to use, doesn\u0026rsquo;t work on Windows with consumer GPUs and requires separate binaries for Nvidia and AMD due to strange design decisions used in underlying libraries. Not to mention that it doesn\u0026rsquo;t work on AMD iGPUs, doesn\u0026rsquo;t support GPUs from Intel or other vendors (like on various ARM SBCs).\nSo going back to GPU implementation, I noticed it follows the Rust CPU implementation quite closely, even copied some comments from it verbatim. The big unique thing was a custom GPU-optimized sorting implementation. Not only that, it only sorted Y values, not the rest ðŸ¤”\nThinking about it some more, for the first Chia table we can indeed use stable sort and only consider Y because X is already sorted originally and will remain in deterministic order if stable sort is used for Y. Now for the rest of the tables it turns out a similar logic can be applied, but the buckets need to be processed in deterministic order, which while not the case in Rust implementation right now, can be done and will open the possibility to sort by only Y as well.\nThere is no PR with those changes, but I confirmed that substantial gains can be had from that change alone. Not only that, GPU uses Radix sort, which is really well suited for sorting integers. I read a bunch about that and looked at few Rust implementations like voracious_sort, which bring further gains (though that particular library is actually not no_std-compatible yet). But then while reading about voracious sort on author\u0026rsquo;s blog, I learned that there is even more niche sort that is even faster and fits the use case of sorting Y (which while are u32 internally right now, only store K bits of information in them, K=20 right now in Subspace) called counting sort, which is very similar to Radix sort, but much simpler and has a single round!\nCounting sort is something I\u0026rsquo;m planning to use in Proof-of-Space, and it will work great or GPUs as well. Since the use case is very narrow and specific, very simpler yet efficient implementation can be written and I have high hopes for it! It\u0026rsquo;ll also be a stepping stone for me in preparation for writing a GPU implementation.\nBTW most of these changes are already backported to Subspace with more coming soon.\nUpcoming plans # So that is what I\u0026rsquo;ve been busy with last couple of weeks. Planning to continue working on block production in the near future and will probably tackle that sorting replacement adventure, maybe even dabble into GPU programming (was successful at avoiding it so far).\nIf you have any questions or thoughts, Zulip chat is currently the best place for reaching out. See you next week with a fresh portion of updates.\n","date":"2 June 2025","externalUrl":null,"permalink":"/blog/2025-06-02-path-to-block-production-and-procrastination/","section":"Blog","summary":"\u003cp\u003eThere have not been an update from me last week, how come? Well, it didn\u0026rsquo;t seem like there was anything particularly\nsubstantial to share, mostly some refactoring, so I decided to skip it. This week though I have a few words to say about\nthe path towards block production (not there yet, but getting closer). Some this is a boring process, I procrastinated\nsome too, diving into various topics with some interesting performance improvements and new learnings.\u003c/p\u003e","title":"Path to block production and procrastination","type":"blog"},{"content":"I started the week thinking about the mechanics of shard segment commitment into the global history of the beacon chain. If you recall from previous updates, we already had a pretty good idea of how the information about child shard segments flow up to the beacon chain, but there were still a few questions that were really bugging me.\nMainly:\nWhen a segment is committed in the beacon chain, how can we inform the corresponding child shard that the segment has been added to the global history and assigned a global piece index? How can we ensure that a segment is \u0026ldquo;final\u0026rdquo; with a really high probability before committing it to the global history? How do block re-organisations in the beacon chain, or lower level shards, affect the flow information up the hierarchy? How do we ensure that the information about the shard segments is available and verifiable, and that segments are encoded correctly and include the set of blocks that is supposed to have? Thinking about all these questions led to a realisation that, while obvious in hindsight, is going to really simplify the design of the protocol and its security analysis moving forward: we can embed all the core protocol verifications and mechanics into the longest-chain rule (including block shard submissions, segment verification and commitments, chain re-organisations, challenges and data availability checks).\nWith this, all chains in the hierarchy are able to deterministically reason about the validity of the information included in blocks without having to rely on external mechanisms like fraud proofs or a dedicated data availability chain (and this is why the design of Bitcoin is so elegant!).\nImpact of chain re-organisations in shard block submissions # Let\u0026rsquo;s illustrate how this new realisation of leveraging the longest-chain for all core protocol mechanics come to play starting with how it affects the shard block submission process, which is going to be the main data structure impacted by chain reorganisations. See the image below for visual aid to the following explanation.\nWhen, e.g. an intermediate shard proposes a new block blkA, this block will have a reference to a valid block from the history of the beacon chain.\nblkA will be submitted to the intermediate\u0026rsquo;s shard parent, in this case the beacon chain, inside an IntermediateShardBlockInformation data structure.\nThis process will be repeated for every new block in the shard: blkB, blkC, etc.\nVerifying the validity of a shard block is done by making the regular block validity verification and all the additional verifications imposed by the hierarchical consensus like checking that the referenced beacon chain block is valid. You can already glimpse here how we are not only embedding in the longest chain rule local consensus verifications, but also logic involving the overall hierarchical consensus operation.\nSo far so good. However, all the shards in the system are running a probabilistic consensus. What happens if there is a chain re-organisation and a heaviest chain surfaces replacing the current longest-chain? Depending on the shard suffering the re-org, the impact is different. Fortunately, all of them can be handled quite gracefully through the longest-chain rule.\nUpward reorgs, meaning leaf or intermediate shards that suffer a re-org, are easily detected. As blocks are being submitted to the parent, the parent is able to identify that there is a heaviest chain that needs to replace its current view of the child shard. This requires no immediate action from the parent apart from this update of the parent\u0026rsquo;s view of the heaviest chain. Reorgs from intermediate shards do not involve any immediate action from its child shards. The newest heaviest chain may change the way (and specific blocks) where the information about the child shard is being included into the parent chain and propagated to the parent chain, but this shouldn\u0026rsquo;t have any additional impact. Finally, beacon chain re-orgs are the only onces that may trigger actions in the lower levels of the hierarchy. When there is a heaviest chain in the beacon chain replacing the current longest chain, this can trigger a re-org in some shards of the lower levels of the hierarchy that were following the previous longest chain, as their beacon chain references for some of their blocks may be deemed invalid. This event is also handled gracefully by the longest-chain rule. The farmer entitled to create the next block in the child shard after the beacon re-org will build the new block on top of the latest block in the child shard with a valid beacon chain block reference after the re-org, continuing in this way constructing a valid chain in the shard. Shard segment commitment and block probability of reorganisation # While handling re-organisations of shard block submissions is somewhat \u0026ldquo;cheap\u0026rdquo;, this is not the case for shard segments. Segments committed to the beacon chain become part of the global history, and will be conveniently archived in farmers plots. Reverting the archival of a segment is an \u0026ldquo;expensive\u0026rdquo; operation that should be avoided at all costs. Consequently, segments should only be propagated up and committed in the beacon chain when their probability of being reverted or re-organised is negligible. This is the high-level proposal of how I am thinking about handling this:\nEvery parent chain (intermediate shard and the beacon chain) keeps a view of the current state of their child shards through the shard blocks that they are periodically submitting. Through this view, they can track of the segments that have been submitted by their children, and in what block they were submitted. Along with this view of blocks and segments, the parent chain will also keep track of the probability of re-organisation for each recent block in the child shard to have a sense of when a block can be considered final with high probability, and thus any segments submitted with it. Segments are propagated to parents as soon as they are created, but they will only be confirmed once the probability of re-org for the block where they were included is below certain threshold (e.g 0.1%). Similarly, the beacon chain will only consider segments for inclusion in the global history once the probability of re-org for the block where they were included is below 0.1%. In parallel, as soon as a segment is created in a child shard, the data availability layer will be checking that it is correctly encoded and can submit challenges that the segment is not valid. If a block or a segment is challenged because is not available, it can\u0026rsquo;t be verified, or it was wrongly encoded, the beacon chain (or its corresponding parent for leaf shards) will pause the commitment of new segments and blocks for the shard until a counter-challenge is sent reverting the failure situation. All of this is flow of information (i.e. block and segment challenges, re-org confirmations etc.) is handled through the proposal of new valid blocks submitted from the lower to the upper layers of the hierarchy. With this, we add a dynamic waiting time for segments that depend on their probability of re-org that gives enough time to consider the segments final and to challenge potential misbehaviors and mistakes. To try and clarify the description above, let me share how this would work step-by-step for the commitment of a segment s1 for a leaf segment:\nA farmer in the leaf shard creates a segment s1 that is included and submitted to the parent as part of the own_segments_root field of the LeafShardBlockInformation data structure that is included in blkA of its intermediate shard parent. s1 was created in blk1 of the leaf shard. As soon as the parent sees this new child segment, it includes it in its next IntermediateShardBlockInformation inside child_segment_root included in blkX of the beacon chain. This makes the leaf segment immediately available in the beacon chain, but it won\u0026rsquo;t be included into the global history, and be included in the corresponding super segment, until the probability of re-org of all the blocks involved in the submission of the segment, mainly blk1, blkA and blkX is over the finality threshold chosen. The intermediate shard is periodically updating its perceived probability of reorg for the leaf\u0026rsquo;s shard blk1 and it will notify the beacon chain in the following IntermediateShardBlockInformation that the finality threshold has been achieved. Similarly, the beacon chain keep track of this same threshold for blkA. When both are confirmed as final, the leaf segment of that block is committed as part of the global history, assigned a global piece index within that history, and made available for archiving. Computing the block probability of reorganisation # And many of you may be wondering at this point, but how can we compute the probability of reorganisation for a block in the first place? Fortunately, there is a lot of literature around this topic and attempts to objectively measure this metric or related ones for longest-chain protocols (including the original Bitcoin white paper, with its analysis of its probability of attack):\nSatoshi Nakamoto\u0026rsquo;s Bitcoin Whitepaper \u0026ldquo;On Finality in Blockchains\u0026rdquo; by Anceaume et al. And quantitative approaches to blockchain finality for Markov Chain models, and formal analysis of Bitcoin\u0026rsquo;s backbone and for game theory and selfish mining (relevant to miner incentives and reorgs). From the analyses on these papers, I extracted the following high-level formula to compute the probability of block reorganisation that we will be able to combine to our needs:\nProb (num_blocks_replaced) = (2*time_to_first_block_propagation/avg_time_between_blocks)^(num_blocks_replaced) What\u0026rsquo;s next? # I shared here a pretty high-level overview of all of these new mechanics for block re-orgs and segments commitments into the global history. This week I\u0026rsquo;ll focus on having a spec-like low-level step-by-step description of all of this mechanics so they can be reviewed by Nazar for them to (hopefully) be in a good state to start being eventually prototyped.\nIn parallel, and as it\u0026rsquo;s been the case for several weeks now, I want to find time to share how I am imagining the data availability layer to work as part of the core protocol (not as an independent layer as it is the case in other projects), and how I am planning to integrate its operation into the longest-chain rule and the core protocol. As always, any feedback, ideas or suggestions? Hit me up!\n","date":"26 May 2025","externalUrl":null,"permalink":"/blog/2025-05-26-longest-chain-rule-and-blocks-probability-of-reorganisation/","section":"Blog","summary":"\u003cp\u003eI started the week thinking about the mechanics of shard segment commitment into the global history\nof the beacon chain. If you recall from previous updates, we already had a pretty good idea of how\nthe information about child shard segments flow up to the beacon chain, but there were still a few\nquestions that were really bugging me.\u003c/p\u003e","title":"Longest-chain rule and blocks probability of reorganisation","type":"blog"},{"content":"I am pretty happy with the progress this week. Funnily, the main culprit for all of this progress has been the the work that Nazar has been doing on the definition of the block structure for our hierarchical consensus. I\u0026rsquo;ll let Nazar dig deeper into what he\u0026rsquo;s been doing here, but let me share in this post what this block structure entails, and how this has unblock several lines of work, and solved many issues for me.\nCracking the first piece of the puzzle: blocks as trees. # If you recall from previous posts, we already had a pretty good sense of what is going to be the lifecycle of blocks in the system: from being created in a shard, to being submitted and committed to its parent, and how this information is implicitly propagated up and can be used to generate proofs about the history of a shard. Unfortunately, we were missing one piece to this puzzle that was dragging progress: making a decision of what would be the block structure, as this would influence the design of other mechanics in the system. After a few back-and-forths, it looks like Nazar has cracked it down on the PRs referenced by [this discussion][block-discussion-link].\nThe key \u0026ldquo;innovation\u0026rdquo; that this design for blocks has is that all the block information (including the header) is structured as trees. This has a lot of interesting properties, as it allows not only to leverage information from the header to generate proofs about the information included in the body, but also to generate proofs about the header itself without having to provide the whole header. This is a pretty powerful property for a hierarchical network architecture like the one we have, because it enables the generation of recursive proofs involving information from blocks (and implicitly state) belonging to different shards.\nHere\u0026rsquo;s a high-level sketch from one of our discussion syncs that briefly depicts what this block as trees would look like (although I would recommend you to check the code to get a full sense of what is going on under the hood):\nEverything is a tree! # Nazar already alluded to this in his post Everything is a tree!, but now it has become even clearer. By leveraging tree-based data structures are able to logically organise the information scattered in different shards as if they were part of the same network. These trees will become \u0026ldquo;the verifiable glue\u0026rdquo; of the system.\nLet me try to back my claim with an example: in order for a shard block to be valid, it needs to reference a valid block in the beacon chain that is in the past but builds upon the last beacon chain reference included by its parent block. Looking at the image below, shard block s3 in the figure references block b4 of the beacon chain, that builds upon s3\u0026rsquo;s parent s2 beacon chain reference, b2. So shard blocks are not only implicitly included in the headers of their parents, but also cross-link the history of the beacon chain, linking in this way the histories of all the shards in the system. What this enables is that, assuming their availability, we can walk the hierarchy from a block to the beacon chain, to any block in a leaf shard of the system, and consequently generate (and verify) proofs of it.\nBasing the system on the generation and verification of recursive tree proofs that can be combined to create more complex behaviors allows us to have a simple and elegant way to reason about the system as a whole. I personally think that one of the reason for Bitcoin becoming so successful is because it is based on really simple rules that are easy to understand and reason about (simplifying the analyses of its security, performance, and operation). And this is what we should aim to achieve.\nWhat this block definition unblocked? # What this block definition has enabled for me is:\nThe ability to start thinking about system re-orgs, forks, and syncing shards from scratch. After some thinking and a brief discussions with Nazar, it turns out that we may leverage many of the mechanics that Subspace currently has in place and leverage them almost unchanged for our case. Some notes about this can already be found in this meeting notes. I will, however, try to get a few sections written about how I am imagining this to work on this discussion While discussing about the different fields that the header would include, we surfaced interesting questions about how the difficulty adjustment for the different shards, along with block rewards, can be leveraged to assign rational farmers to shards in a way that load balances the system and improve the security of weak networks (trying to address the problem of power dilution). More context about these can be found in this meeting notes and this Zulip discussion. Finally (and this is what I want my focus to be next week), with the basic structure designed, we can start defining how will the information about shard segments be propagated to the beacon chain to commit them in the global history and into super segments, and the basic information that super segments should include so light clients can verify the history of a shard seamlessly. The essential need for a data availability layer # As always, not all that glitters is gold. This forest of trees is great to recursively proof information stored anywhere in the hierarchy, but as briefly mentioned above, this only works as long as we can assume the information to be available somewhere. A verifiable reference to some piece of information it is worth nothing if we can\u0026rsquo;t access the underlying content. This is why it has become clearer than ever this week the need for a data availability layer for a system like this to work. Ideally, this data availability layer won\u0026rsquo;t be an orthogonal system like it is the case in other L1 and L2 projects, where they either leverage systems like Celestia or Avail to handle data availability, or build their own independent data availability system, like Espresso.\nIn our case, data availability will be implemented and integrated into meeting-notes-1the consensus protocol itself, to prevent as much as possible the need of fraud proofs and the implementation of special logic to cover corner cases. Fortunately, data availability is a field that it is still being actively researched, and Nazar already found pointers to interesting and relevant work.\nWhile not immediately applicable, the concept introduced by the ZODA (Zero-Overhead Data Availability) paper of the \u0026ldquo;Accidental Computer\u0026rdquo; may allow us to embed different types of proofs within the data availability layer. In our case, we not only want to prove that a block (or a segment) is available in the system, but also that its encoding is correct, that the information included in them is valid. Reading through this work and thinking a little bit about this problem has also occupied a bit of my week.\nNext steps # With all of this, my focus this week will be on organising all of the ideas surfaced from last week\u0026rsquo;s work:\nBy writing down into the relevant discussion PR all the outcomes from our research and conversations. Coming up with a draft design of segment commitment and the structure of super segment leveraging the current structure of blocks. And time permitting, continue researching the design of our data availability layer. And as always, any feedback, suggestions, or comments, please let me know so I can improve my work. Cheers!\n[block-discussion-link]: https://abundance.zulipchat.com/#narrow/channel/495788-research/topic/Mechanics.20of.20child.20block.20submission.20to.20parent.20chain/near/518994821).\n","date":"19 May 2025","externalUrl":null,"permalink":"/blog/2025-05-19-the-unblocker-blocks-as-a-forest-of-trees/","section":"Blog","summary":"\u003cp\u003eI am pretty happy with the progress this week. Funnily, the main culprit for all of this progress\nhas been the the work that Nazar has been doing on the definition of the block structure for our\nhierarchical consensus. I\u0026rsquo;ll let Nazar dig deeper into what he\u0026rsquo;s been doing here, but let me share\nin this post what this block structure entails, and how this has unblock several lines of work, and\nsolved many issues for me.\u003c/p\u003e","title":"The Unblocker - Blocks as a forest of trees","type":"blog"},{"content":"The question might seem somewhat obvious: you have a header and a body with transactions, many blockchains have it, what might be so difficult about it? Well, as I mentioned in the previous update, there are some complications and part of the challenge is related to the fact that we\u0026rsquo;re dealing with sharded architecture, which most blockchains don\u0026rsquo;t need to deal with.\nSharding architecture # The sharding design is not fully worked out yet, but it is quite certain that it\u0026rsquo;ll be hierarchical and look something like this:\nL e a f I s n h t a e r r d m e 1 d 1 i a t e L e s a h f a r s d h a 1 r B d e a 1 c 2 o n c L h e a a i f n I s n h t a e r r d m e 2 d 2 i a t e L e s a h f a r s d h a 2 r d 2 2 So it is a tree with a Beacon chain at the top of the hierarchy, with intermediate shards below it and leaf shards at the bottom. The architecture supports about one million shards in total.\nThe shards are using a shared namespace for addresses and support cross-shard communication with both the ability to send information between shards and using contracts deployed on any shard on any other shard.\nAll this implies the need to be able to prove and verify that things exist or have happened on other shards, while only having access to the beacon chain block root. While at it, we\u0026rsquo;d like for proofs to be efficient too, which means both block header and block body need to be designed in a way that co-locates related information and minimizes the depth of various Merkle Trees.\nIn addition to cross-chain communication, we have to combine segments produced by individual shards into a global history, which also needs to be represented in a block in some verifiable way.\nWhile the actual number of shards will be larger than on the above diagram, the number of layers is expected to be the same. This also means that both headers and bodies for different kinds of shards will be different. For example, the beacon chain will have no user transactions and leaf shards will have no shards below it. There are many more differences, of course.\nCurrent state # While the current version is certainly not final, I think it\u0026rsquo;ll give a good idea of what to expect and how things work together. I\u0026rsquo;ll visualize it as a tree of trees, where you can think of every tree as a Merle Tree. It is about 95% accurate (there are segment root proofs missing on the diagram due to the difficulty to show them correctly).\nLeaf shard block # Leaf shard blocks are the simplest. They reference the beacon chain in the header (which defines some consensus parameters) and include segment roots and transactions in the body.\nB l o c k H e a d e r B o d y C o n s O T e w r n B n a s e n u a s s P R s c e a r e o g c e s i S n m t f u n e e i i l f a c n o x t o l h t n a s i r n o o i t v n s t p m b s s p f s p s n s e u h i a m o t l r u o u i f r m a m r r d a o o t l b g o s b r e e y t t o u u l n [ i e d s n r e f r t i a o r t t o r n e i c t t n i a o o r u o o u r n m r t o o m f p n k r a d p o t o b r e e b b [ n e o t e t o y l l s x t r i o o o s a m f c c e c e k k g t o m i f n r e o u o n n t m o t i b t ] m e r e r o o t ] G h e e n a e d r e i r c Intermediate shard block # Intermediate shards on top of what is done by leaf shards need to include information about leaf shards in both block header and body.\nB l o c k H e B a o d d e y r O w C n o n s s e L e g e n B m a T s e C e f r u a h n L a P R s c i t e s n r e o l a h s e s i S n d r f a a f u n e o r c i l f a c s o s d t x t o l h h t h i a a s a s o i r r e n n d d g s m i b b e v n s t p m b s s p f s p s n l l n e u h i a m o t l r u o u i f o o t r m a m r r d a o o t l b g o c c s b r e e y t t o u u l n k k r i e d s n r e f r t i a s [ o o r t t o r n e i c t h o [ n i a o o r u o o u s e t n m r t o o m f p n k r e a s t d p o t o b r e e b b g d r e o t e t o y l l m e a x t r i o o o e r n m f c c n s s e k k [ t a o c f n r b r t u o l o [ i t m o o o o i b t c t o n m e k w e r ] [ n ] r o b s o l e t o g c m G h ] k e e e n n a h t e d e r e a r i r d o c e o r t s ] ] Beacon chain block # Beacon chain replaces reference to the beacon chain with actual consensus parameters. It still tracks child shards, but also contains PoT checkpoints in its body for faster verification later.\nB l o c k H e B a o d d e y r C o n O s C w e h C n I n i o n s l n s t u d s e e P R s e g r r e s n m m e s i S h s e e f u n e a u n d I P i l f a r s t i n o x t o l d a t T p r r e b a o e r c l r o m h o a t s e e c m s h d c v n s t p m b s s p f s p s k e a i k e u h i a m o t l r u o u i s t r a p r m a m r r d a o o t l b g e d r o s b r e e y t t o u u l n r e i i e d s n r e f r t i a s s n o r t t o r n e i c t e s t n i a o o r u o o u g h s n m r t o o m f p n k r m a d p o t o b r e e [ e r e o t e t o y [ n d x t r i o s p O O O } s t m f b o o p p p p \u0026gt; e b e l l t t t t o g r l o o u i i i t m o o f c t s o o o e o c [ k i l n n n p n t k t o o \u0026lt; \u0026lt; \u0026lt; a t s c i r n t s n { r s s e h h m o u e a l l n r e e e o r i p x m o o t o a c t a t e t e t t r o d k n e r t o t e p ] g r s e i p r o e a s o r t y ] s i t e l s e [ ] n G h i g u r o c t e e o m t c a w h n a n e i h t n i ] e d s n o a i l r e t n n o s d i r g n e c r r e s g s [ o a m e o n e g b t g n m l \u0026gt; e t e o \u0026gt; n c r t k o o r h t o e s o a . t d . s e , . r . ] As you can see, all three block types share some similarities, but also have unique differences dictated by their role.\nNow if we need to prove that something was stored in leaf shard, we can do that by generating proofs for the following path:\nB e a c o n c h S S a t t i a a n t t e e b l r i o o t c o e k t m r o o t R e s u l C t h i l d s L h e a a r f d s b h l a o r c d k s b l o c k I r n o t o e t r m e d i a C t h e i l s d h a s r h d a r b d l o b c l k o c r k o s o t This way we can reach any piece of block header or body or anything stored in a state of any block. Not only that, information submitted from leaf shards to the intermediate shards and from intermediate shard to the beacon chain is verifiable for integrity (that segment roots correspond to the header). Moreover, since all block roots are added to the MMR, any historical information can also be proven/verified as well. It trees all the way down!\nThe whole structure and decoding ability was introduced in PR 245 with follow-up fixes in PR 246. There are no builders or owned versions of the data structures yet, but it will be easier to have them now that the structure is known.\nThere will be more changes to this in the future once/if we have to deal with data availability and potential misbehavior, but this should be sufficient for now.\nBlock root? # You may notice that instead of \u0026ldquo;block hash\u0026rdquo; I used \u0026ldquo;block root.\u0026rdquo; That is simply a reflection of the fact that a header in itself is a Merkle Tree. Our headers are larger than those in blockchains like Bitcoin, so it is desirable to compress the size of the proof when only a small part of it is needed (like timestamp).\nUpcoming plans # I hope you appreciate the ASCII art, I spent a non-negligible amount of time formatting it ðŸ˜….\nWith block layout in this state, I\u0026rsquo;ll need to write even more boilerplate for builder and owned versions of data structures. Once that is done, I\u0026rsquo;ll be back to consensus verification in an attempt to get primitive blockchain going. There are still some open questions around plotting, but I think we have a pretty good intuition with Alfonso for how to approach it.\nWe post even more research updates on our Zulip as they happen, including meeting notes from out 1:1s with Alfonso.\nWith that, I\u0026rsquo;ll see you next time with more updates and maybe even more ASCII art ðŸ˜†.\n","date":"19 May 2025","externalUrl":null,"permalink":"/blog/2025-05-19-what-does-a-block-look-like/","section":"Blog","summary":"\u003cp\u003eThe question might seem somewhat obvious: you have a header and a body with transactions, many blockchains have it, what\nmight be so difficult about it? Well, as I \u003ca\n  href=\"../2025-05-12-address-formatting/#block-structure\"\u003ementioned in the previous update\u003c/a\u003e, there are some complications and part of\nthe challenge is related to the fact that we\u0026rsquo;re dealing with sharded architecture, which most blockchains don\u0026rsquo;t need to\ndeal with.\u003c/p\u003e","title":"What does a block look like?","type":"blog"},{"content":"The discussions with Alfonso and attempts to start building an actual blockchain led to spending a big part of last week working on block structure. That work is not done yet. However, I like to have some sort of accomplishment at the end of the week if possible, so I spent some time to finally implement the formatting of addresses, which will be the main part of this update.\nBlock structure # Before getting to addresses, I\u0026rsquo;d like to share some challenges with the block structure. Block header and body look like those things that were figured out a long time ago and do not need a lot of attention. However, in our sharded design, we need to both include information from shards in their parent shards, and include reference to the beacon chain in shards below. Not only that, this whole thing should be verifiable and allow to generate and inclusion of anything recursively referenced by the beacon chain. Moreover, it must be possible to verify just from the beacon chain block hash (or super segment root in case of archival history).\nThose constraints require a lot of careful consideration about the order of things and how things are aggregated into \u0026ldquo;block hash.\u0026rdquo; Block hash is sometimes just a hash of the block header contents as bytes, but doesn\u0026rsquo;t have to be exactly that, strictly speaking.\nSo I\u0026rsquo;ve been working on a custom structure of the block header and block body that does allow to store all the necessary information, while optimizing the size and verification complexity of generated proofs. A lot of interesting questions popped up, even something as simple as Whether to include timestamp in block header or not? was up for debate.\nI hope to finish the initial version of that by next update and explain the rationale behind key decisions.\nAddress formatting # Now the key part of this update: what should the address in a blockchain look like?\nI think we\u0026rsquo;re all used to addresses looking like a long unreadable gibberish unless some system for aliases is used like ENS (.eth) or SNS (.sol), neither of which is a part of the core protocol of Ethereum or Solana accordingly. Sure, we need to encode some binary information that inevitably looks like gibberish, but there is more than one way to skin a cat and not all of them are equally good.\nLet\u0026rsquo;s first look at the way blockchain addresses look like for some popular blockchains and what are the good and not-so-good things about them:\nBlockchain Example address Good Bad Ugly Ethereum (hex) 0x661cda03aba8d39â€‹35f6b456f1668b987â€‹03332333 Short-ish, simple unambiguous vocabulary Basically unreadable and unpronounceable, no indication of which chain it belongs to No integrity check, accidental typos can lead to loss of funds Bitcoin (Bech32/Bech32m) bc1qzqhl7npmadm56â€‹nvmvm2pexmmkrc6mâ€‹msyjcdclrjzpzn6sn4vynâ€‹usdzewtu Bech32m supports strong error detection, avoids ambiguity, clear chain identification Depending on address kind is even longer and less readable than Ethereum (like in provided example) Solana (Base58) 5ddo32xdfxBvxweFYeâ€‹SDbteK53Fj68fAVvVqyâ€‹RF6Mpâ€‹HY Unambiguous vocabulary About as unreadable as Ethereum, slightly longer, no chain identification No integrity check, accidental typos can lead to loss of funds Cardano (Bech32) addr1q9mu9r0ynn034â€‹qwwm8lkncu9dd0864â€‹cn8wd74rd5j90q3ydcqâ€‹228ammxm02j45gudl5â€‹pgklvhvpkxx2stxth5pcâ€‹xv2xseh43q6 Bech32 supports strong error detection, avoids ambiguity, clear chain identification Extremely long, painful to type manually Polkadot/Substrate (SS58) 12pZ8VV6o3A6BFYg3â€‹b3kk6TC1ZD53x9bXwâ€‹koFenvâ€‹183DEods SS58 supports error detection, avoids ambiguity Fairly long, chain identification is not as clear as in some other formats and causes some confusion due to the context where these addresses are used There are many more examples, but I think you get an idea. Neither option is truly great, so it is worth thinking what it could be.\nFun fact, I spent a non-negligible amount of time trying to format the above table so that it is at least somewhat readable, especially on mobile devices. Those addresses really do not want to fit on the screen, and there isn\u0026rsquo;t any natural way to slice them up. This isn\u0026rsquo;t a problem with the format described below.\nIf talking about balance transfers, there are examples from a world of fiat:\nAddress type Example address Good Bad Card number 3782 8224 6310 8005 Reasonably short, only digits, services that respect users group digits into blocks of 4 for readability No integrity check, though this is less catastrophic than in blockchain context, sometimes isn\u0026rsquo;t formatted with spaces IBAN CY17002001â€‹2800000012â€‹00527600 Contains checksum for error detection Long gibberish that while can be, in most cases is not grouped in meaningful way for humans to read or type One thing that impacts the length of the address is the amount of data stored in it. In Ethereum it is 20 bytes, in Solana 32 bytes, Cardano clearly stores even more data there. As mentioned previously, I decided to go with just 16 bytes in a form of u128 for an address, which is an artifact of the smart contract design.\nSince Bech32m looks like one of the best options features-wise, let\u0026rsquo;s use it as an example and format a random 128-bit unsigned integer with abc prefix (human-readable part, Bitcoin mainnet uses bc for example):\nabc1qzq2mr3rnsrwcka6fk6sdzcfv5ygv5w7 When compared to the blockchain alternatives above, it is already the shortest one, but it still looks very much like gibberish that is unreadable and unpronounceable. We\u0026rsquo;ll do something about that, but let\u0026rsquo;s first talk about the contents of that 128-bit unsigned integer that the address is.\nThe address logically has two parts:\n20 bits at the beginning correspond to shard index (the least significant bits first) the remaining 108 bits correspond to an address allocated on that shard index (the most significant bits first) While address can be used on any shard, it can only be allocated (when contract is deployed) on a shard that corresponds to shard index.\nConveniently, since Bech32m is a base-32 representation, 20 bits of shard index correspond to exactly 4 characters in the above example right after 1. With the last 6 characters being Bech32m checksum, we can break the address down into components like this:\nabc 1 qzq2 mr3rnsrwcka6fk6sdzcfv5 ygv5w7 ^prefix ^separator ^shard index ^allocated address ^checksum Now let\u0026rsquo;s re-assemble the address back with a few tricks:\ninsert one separator after shard index insert another separator before checksum segment allocated address into groups of 4-3-4-4-3-4 characters Now the address looks a bit more like a credit card number and should be easier to read and type:\nabc1qzq2-mr3r-nsr-wcka-6fk6-sdz-cfv5-ygv5w7 Now it is a bit more readable, but it is still quite long, about as long as Ethereum address is. However, since the shard index and allocated address are not random, we can make a few observations:\nwhen blockchain launches, there will not be many shards in existence, meaning out of 20 bits of shard index, most will be zero at the end: shard 2: 01000 00000 00000 00000 similarly, addresses are allocated using a simple increment, so there will be a lot of zeroes at the start, for the fifth address allocated: [105 zeroes]101 So when we encode that as Bech32m, we\u0026rsquo;ll get something that looks like this:\nabc1gqqq-qqqq-qqq-qqqq-qqqq-qqq-qqq5-a7dw6e See all of those qs in there? That is what 00000 bits encode to, and we have a lot of those. So the next logical step is to strip all q to the right of the shard index and to the left of the allocated address, removing extra separators in the process as well. Then we end up with this:\nabc1g-5-a7dw6e It is much shorter, can still be deterministically mapped back into full Bech32m and original u128, abc1 is a fixed prefix that is easy to recognize and remember.\nThis is exactly the format that I envisioned to use a few months ago, but only implemented last week in PR 237.\nFor a long time, the addresses will look not much longer than that and will only grow slowly as more contracts are deployed on the blockchain. There are plenty of pain points related to blockchain adoption by regular users, and addresses are a part of it. I hope this is a useful contribution not just to the blockchain we\u0026rsquo;re building, but to the ecosystem more broadly.\nUpcoming plans # In the nearest future, I\u0026rsquo;m planning to finish the initial design of the block header and block body format and work towards building consensus verification logic. There will probably be more discussions with Alfonso about how all of these fits together, because there is an annoying number of moving parts that should fit together, hopefully in a somewhat elegant way.\nIf you found any of this interesting and would like to discuss, please join our Zulip. In any case I should have more updates to share next week.\n","date":"12 May 2025","externalUrl":null,"permalink":"/blog/2025-05-12-address-formatting/","section":"Blog","summary":"\u003cp\u003eThe discussions with Alfonso and attempts to start building an actual blockchain led to spending a big part of last week\nworking on block structure. That work is not done yet. However, I like to have some sort of accomplishment at the end of\nthe week if possible, so I spent some time to finally implement the formatting of addresses, which will be the main part\nof this update.\u003c/p\u003e","title":"Address formatting","type":"blog"},{"content":"This week has been mainly focused on clearing the fog around shard archiving and trying to start fleshing the low-level details for the protocol. It feels like in the past few weeks we\u0026rsquo;ve been surfacing more questions than answers, and I honestly think this is a good sign. It means that we are getting to the point where we can start to see the details of the protocol and how it will work in practice.\nIterating on the protocol spec # If you recall from last week\u0026rsquo;s update, I started PR220 with the end-to-end mechanics of child shard block submission to the parent chain and the generation of global history inclusion proofs; and PR227 with the operation for segment generation and commitment to the global history. Nazar has been kind (and patient) enough to have done already a few rounds of feedback. This has really helped to dig deeper into details and flesh out the spec of these protocols. You can refer to the discussions on those PRs if you are interested about the details, but let me give you a few highlights after these iterations:\nWe have a better sense of how the end-to-end of child block submission and their proof generation will look like. This is allowing us to iterate on what the final data structure for BlockHeaders will look like. We still don\u0026rsquo;t have a good idea of how to deal with potential reorgs in the beacon chain and child shards. Brainstorming about the high-level of reorg handling will be one of the main focus for me this week. Regarding segment commitments, as you may see from Nazar comments in the PR227, the spec was still a bit under-defined, and many details were still missing. Fortunately, after our sync from last Friday I think I have all the details needed to flesh out the spec. This week I am planning to re-write the whole spec to include all of these details before a new round of feedback. Discussing the source of randomness for the system # One of the big questions that have been bothering me throughout the week, and that took me down the rabbit hole of reading about the state of the art of unbiased randomness beacons and randomness chains was the following: \u0026ldquo;Should we consider a PoT chain that is independent of the beacon chain for randomness beacon generation?. This question was triggered by a comment from Nazar in one of the PRs that made me realise that a reorg in the beacon chain can trigger a reorg of the PoT chain. This is caused by the fact that the PoT chain periodically injects entropy from the main chain (in our case the beacon chain). Our system will leverage this source of randomness to sync all the shards in the system, so any reorg of the randomness chain will inevitably cause the rest of shards to have to reorg to accommodate the changes on past randomness slots. Additionally, Nazar was mentioning in this [research thread](https://abundance.zulipchat.com/#narrow/channel/495788-research/topic/Light.20client/with/516959267 \u0026ldquo;https://abundance.zulipchat.com/#narrow/channel/495788-research/topic/Light.20client/with/516959267) some potential complications with verifying randomness in light clients.\nAs a result of this I started wondering if we should consider using an independent randomness chain that is based in other cryptographic primitives that is not an AES-based VRF (which is currently the case) and is efficient to verify, like is the case of drand. We even discussed the possibility of leveraging directly the drand network as independent randomness chain. Unfortunately, this may not be possible at this stage (as described here).\nThe outcome of this discussion has been that we are going to keep the operation of the PoT chain as it works in Subspace for now and we are going to tweak the parameters so that we minimise the probability of a reorg.\nFollowing the progress # Which leads me to the next important highlight of the week: so far, the best way to follow our progress in terms of design was to read the spec discussions PRs, or the topical discussions from Zulip. For instance, here are a few of the discussions that have been produced from this week\u0026rsquo;s work:\nMechanics of child block submission to parent chain Whether to include timestamp in the block header or not OptimumP2P (RLNC) v.s. libp2p gossipsub Address formatting' Light client But you\u0026rsquo;ve probably realised by now that I keep referring on this updates to my syncs with Nazar, and I didn\u0026rsquo;t like the fact that these weren\u0026rsquo;t public and the outcomes of these meetings couldn\u0026rsquo;t be followed publicly. This is why I\u0026rsquo;ve started publishing my meeting notes directly as Zulip discussions: here\u0026rsquo;s the link to the notes from last week\u0026rsquo;s meeting. These are raw and unformatted notes that I am taking live, so expect typos and some inconsistencies, and don\u0026rsquo;t expect them to be easily readable, but at least it allows me to give you a glimpse of what we\u0026rsquo;ve been chatting about in case you are interested.\nWhat\u0026rsquo;s next # As already advanced above, my goals for this week hasn\u0026rsquo;t changed significantly from last week. I will try to continue iterating with Nazar in the spec discussion for sharded archiving so we clarify some of my current blind spots like the handling of reorgs by child shards and the beacon chain. I want to leave the specs for sharded archiving in a state that allows me to start new spec discussions for sharded plotting and data availability sampling (for which I already have lots of notes of the high-level design). That\u0026rsquo;s all for now! As always, any feedback or suggestions about the content of this post or my work overall, please hit me up!\n","date":"12 May 2025","externalUrl":null,"permalink":"/blog/2025-05-12-digging-deeper-into-sharded-archiving/","section":"Blog","summary":"\u003cp\u003eThis week has been mainly focused on clearing the fog around shard archiving and trying to start\nfleshing the low-level details for the protocol. It feels like in the past few weeks we\u0026rsquo;ve been\nsurfacing more questions than answers, and I honestly think this is a good sign. It means that we\nare getting to the point where we can start to see the details of the protocol and how it will work\nin practice.\u003c/p\u003e","title":"Digging deeper into sharded archiving","type":"blog"},{"content":"We keep iterating on the best way to discuss and make progress on the design of the protocol. Using issues for discussions have shown less efficient than originally expected. The inability to make in-line threads, and having to quote every single detail of the spec that we want to discuss about was really cumbersome. I started the shard block submission issue as an attempt to start iterating the low-level details of specific protocol mechanisms in a way that is narrow enough and easy to track, but it didn\u0026rsquo;t fulfill all our needs. The solution? Creating discussion PRs that I don\u0026rsquo;t expect to get merged, but gives us all that we need to have low-level discussions about specific parts of the protocol, track our progress, open ideas, and discussions, and have them public so anyone can contribute or follow along.\nFinalising the mechanics for sharded archiving. # Last week I\u0026rsquo;ve mainly focused on trying to flesh out the basic mechanics for sharded archiving, detailing the basic data structures and proofs required to start designing the next steps in the protocol, mainly plotting, farming, and data availability. The result of this work are these two PRs: PR220 with the end-to-end mechanics of child shard block submission to the parent chain and the generation of global history inclusion proofs; and PR227 with the operation for segment submission to the global history in the beacon chain, and the corresponding inclusion proofs for segment pieces. As mentioned above, the goal for these PRs is not to get them merged as \u0026ldquo;authorative spec artifacts\u0026rdquo; but to have constraint discussions that allows us to iterate more efficiently and help with surfacing the requirements for prototyping them.\nParking lot of discussions # Along with the discussions above, I also pushed in PR228, a new section on the spec discussions directory that I called the parking lot where I am starting to collect some discussion points that we\u0026rsquo;ve been having sync or a sync of things that need to be defined further in the future but that we\u0026rsquo;ve \u0026ldquo;parked\u0026rdquo; until we have designed more core components that we could build upon. I didn\u0026rsquo;t want all this to be lost on my meeting notes so I decided to start sharing them publicly (\u0026ldquo;hey, maybe someone has already some ideas and can pick up these problems themselves. Ping me in Zulip or anywhere else if this is the case\u0026rdquo;). At some point, I am even considering pushing all my meeting notes to make them pubic, there\u0026rsquo;s a lot of good discussions and information there, but it may end up adding more noise to the repo than anything else, so I am refraining from it for now.\nComing next: Sharded plotting and data availability # This week I\u0026rsquo;ll probably be stretched into different fronts:\nI\u0026rsquo;ll try to start addressing any feedback that surfaces from the PRs with the spec of proof and segments mechanics of sharded archiving. With sharded archiving in a good place, I\u0026rsquo;ve started drafting the design for sharded plotting. While this subprotocol may not introduce big changes from how it currently works in Subspace, getting it right will be fundamental for the correct and efficient operation of sharded archiving and block proposal. Finally, the more progress I make into the design, the clearer it becomes that on a sharded architecture like ours, the data availability layer will be key to ensure the security of the protocol. Thus, I want to re-purpose the spec PR (PR193) that I started a few weeks ago with the high-level design into narrower discussions like I did for sharded archiving with PR220 and PR227. ","date":"5 May 2025","externalUrl":null,"permalink":"/blog/2025-05-05-from-sharded-archiving-to-sharded-plotting/","section":"Blog","summary":"\u003cp\u003eWe keep iterating on the best way to discuss and make progress on the design of the protocol. Using\nissues for discussions have shown less efficient than originally expected. The inability to make\nin-line threads, and having to quote every single detail of the spec that we want to discuss about\nwas really cumbersome. I started the\n\u003ca\n  href=\"https://github.com/nazar-pc/abundance/issues/215\"\n    target=\"_blank\"\n  \u003eshard block submission issue\u003c/a\u003e as an attempt to\nstart iterating the low-level details of specific protocol mechanisms in a way that is narrow enough\nand easy to track, but it didn\u0026rsquo;t fulfill all our needs. The solution? Creating discussion PRs that I\ndon\u0026rsquo;t expect to get merged, but gives us all that we need to have low-level discussions about\nspecific parts of the protocol, track our progress, open ideas, and discussions, and have them\npublic so anyone can contribute or follow along.\u003c/p\u003e","title":"From sharded archiving to sharded plotting","type":"blog"},{"content":"This week was very similar to the last one with a bunch of refactoring in cleanups. There were important archiver improvements/fixes (depends on point of view) and more work on Merkle Trees. Two more crucial crates were moved from subspace to crates.\nUnbalanced Merkle Tree # In previous updates, I mentioned implementation of the balanced Merkle Tree, which was important to get done for KZG removal. That was helpful at the time, but there are many places in the protocol where number of elements will not be a power of 2, for example, root of transactions in a block.\nI spent a few days working on an efficient implementation that uses minimal amount of stack-allocated memory and made sure to make it compatible with the existing balanced Merkle Tree. It is compatible in a sense that for the number of leaves that is a power of 2, the root and proofs are identical. Moreover, the way the root is constructed is actually the same as in Merkle Mountain Range, so once we introduce MMR, it will be a special case of the unbalanced Merkle Tree. Essentially, all three will be a generalization of each other, using the same mechanisms and producing the same roots and proofs for the same input.\nThe implementation landed in PR 216 in the simplest case. In the future, all variants can be further optimized with both SIMD and (if necessary) parallelism. For example, a large tree can be built in parallel, we can split a large unbalanced tree into a bunch of smaller balanced subtrees, process them in parallel, recombine and repeat until a single element is left, tree root.\nThe process was a bit frustrating at times, but I\u0026rsquo;m quite happy with the result and performance, and kind of excited that it can be improved further with SIMD by A LOT.\nRefactoring core components # PR 217 finally introduced new type for BlockNumber, PR 218 added the same for BlockHash and BlockWeight. These were a long-standing wish of mine, happy to have them now. With those I finally moved subspace-core-primitives as crates/shared/ab-core-primitives in PR 219.\nNow that there is ab-core-primitives, it was time to integrate some of the duplicated data structures and type aliases of ab-* crates. In PR 221 ab-transaction crate became transaction module of ab-core-primitives and a few generic types were either moved from ab-contracts-common to ab-core-primitives or replaced with those from it.\nArchiver # I mentioned some rounds of archiver updates in the past and that I was not done. Yes, it was much faster already and easier to reconstruct data from, but some major issues with data retrieval remained.\nRemaining problems ended up originating from encoding complexity. Specifically, SCALE codec\u0026rsquo;s compact length encoding of vectors. While it makes a lot of sense as a general purpose encoding feature, it does cause problems with non-determinism of the total encoding length. For example, imagine there are a few more bytes left at the end of a segment. Variable length encoding for numbers meant that sometimes increasing segment item by a single byte results in compact length encoding growing from 1 to 2 or from two to four bytes. So adding one byte of information may mean actually adding two or even three bytes to the encoding, which may no longer fit into the segment.\nHandling this was not straightforward and required careful test cases to make sure implementation doesn\u0026rsquo;t regress. It also meant that during data retrieval it wasn\u0026rsquo;t possible to know how many bytes of padding there are for certain without decoding the whole segment, which is very inefficient. I suggested both mildly horrifying and impressive â„¢ hack for this problem that Teor courageously implemented. As you can see, implementation wasn\u0026rsquo;t pretty because the problem isn\u0026rsquo;t to begin with.\nI started with some data structure cleanups in PR 223, the important outcome of which is that SegmentHeader is now constant size data structure and not an enum, it also implements TrivialType now and in-memory representation now happens to be identical to SCALE codec, nice! In PR 226 I found the courage myself to get rid of compact length encoding and replace all lengths with little-endian u32 representation. These two PRs together make data retrieval a breeze: just by knowing the length of the data (either because it was stored somewhere or after retrieval of the first piece) it is possible to know exactly how many pieces are left to download (ideally concurrently) and no extra hashing trickery or other extra work is needed. This will be an exercise for another time, though. I left object fetcher in a broken state for now.\nWith these changes, I felt it was time to move subspace-archiver as crates/shared/ab-archiver in PR 229.\nUpcoming plans # Now that the primitives are coming together at crates, I\u0026rsquo;ll probably try to move some consensus logic from pallet-subspace to a new system contract, which will be the foundation for building an actual blockchain. Client (node) side will require more infrastructure and iterations due to how tightly it is integrated with Substrate, so one step at a time.\nThat is all I have to share at this point, please join Zulip if you have anything to discuss. We have a whopping eight members there now! ðŸ¤¯\n","date":"5 May 2025","externalUrl":null,"permalink":"/blog/2025-05-05-subspace-codebase-refactoring-part-2/","section":"Blog","summary":"\u003cp\u003eThis week was very similar to the last one with a bunch of refactoring in cleanups. There were important archiver\nimprovements/fixes (depends on point of view) and more work on Merkle Trees. Two more crucial crates were moved from\n\u003ccode\u003esubspace\u003c/code\u003e to \u003ccode\u003ecrates\u003c/code\u003e.\u003c/p\u003e","title":"Subspace codebase refactoring (part 2)","type":"blog"},{"content":"We started last week with two PRs that attempted to describe in detail the operation of sharded archiving (PR192), and the data availability layer of the system (PR193). When I started writing this spec, it was meant to be for a broader audience, but we realised after a few rounds of feedback that the project is still in a really early stage and in constant change, so it would be more efficient to focus on detailing the parts of the protocol that are currently under-defined instead of trying to give a deep overview of the overall operation of the protocol from the get-go. The actual goal behind this protocol specification is to unblock the implementation of a prototype that can help us gain certainty about the design decisions that we are making, and surface potential blind spots in the design, and not to have a reference spec (just yet).\nMaking the code the source of truth # We inherit a lot of the mechanics of the protocol from Subspace\u0026rsquo;s operation, and as I was writing the spec I saw myself repeating many of the details already specified in the Subspace specification. Even more, as I was writing it, Nazar was making really good progress on the implementation of the prototype, replacing KZG with the new Merkle-proof based archiving, refactoring and simplifying the implementation of different modules, and stripping the code base from unnecessary overhead and dependencies, which led to parts of the spec becoming obsolete quite fast.\nWith this in mind, we decided to repurpose the spec to make it more maintainable and useful for its original purpose, allowing us to flesh out the design that can unblock the implementation of a prototype and iterate on it. Thus, what the spec will become from now on is an outline of all of the basic sub-protocols and modules of the system. Instead of having a detail spec describing their operation, they will reference the parts of the code that implement them, making the code the source truth for the protocol implementation. This avoids unnecessary duplication, potential mismatches between the spec and the implementation, removes a lot of maintenance overhead, and it enables us to move faster in the design focusing on the parts that are still under-defined. This will also help us move from code to spec once we are comfortable with the design, as we just need to follow the reference in this spec outline and translate the implementation code into a spec \u0026quot;(hey, we may even be able to delegate this task to an AI in the not-so-distant future)\u0026quot;.\nProving blocks and segments belong to the history of the system. # One of those parts of the protocol that were currently under-defined and that were blocking the implementation, were the proofs to verify that blocks and segments of a shard belong to the global history of the system. After some feedback iterations, this has been my main focus for the last week, and I am proud to share that we may have a first candidate design for these.\nBlock proofs # Let\u0026rsquo;s start with blocks. If you recall from previous updates, shards from the lower levels of the hierarchy are periodically committing the new blocks they are creating to their parents. This makes their history available to the rest of the system before a new segment of shard is created and made available in the global history maintained by the beacon chain (which make take some time to be available). These commitments would allow nodes to prove (and verify) that a block belongs to the history of the shard, and is consistent with the global history of the rest of the shards and the global history of the system.\nThe following diagram shows a high-level of how I am thinking about the proofs for blocks. The idea is to use a Merkle Mountain Range (MMR) to store the history of the blocks in the shard, and use references to blocks in the beacon chain to attach them to the global history in the beacon chain. This allows for the generation of incremental proofs of inclusion of blocks in the history of the shard, and their consistency with the global history. Child shards are periodically sending their block headers to their parent. Parent shards keep track of child shard new blocks, and they build their own view of the history of the shard. New blocks in shard need to always point to recent blocks in the beacon chain that can be consider final.\nThus, when a network participant wants to prove that an event has happened, and this proof relies on a block in a child shard being valid, they can generate a proof of inclusion of a shard block by requesting a MMR proof for the block from the shard\u0026rsquo;s history kept by the parent, and verifying that is consistent with the history of the parent shard, and that they point to valid blocks in the beacon chain (see the bottom-right part of the diagram for the high-level steps of the proof generation/verification).\nSegment proofs # And what about segments? For segments, the main thing that we want to proof is that a piece is part of a segment that is in itself part of the global history of the system committed in the beacon chain. Shards are periodically generating new segments of their local history. Information about new segments in a child shard are submitted to their parents that are responsible for forwarding them to the beacon chain. When new segments are committed in the beacon chain (coming from any shard in the lower-levels, or even from local segments from the beacon chain), they are added to the global history of the system kept in the beacon chain. Thus, child shard segments leave their shards with a local index that determines the sequence in which they were generated in the shard, but as soon as they are committed to the beacon chain, a global history index is assigned to them that determines their position in the global history of the system.\nFinally, a new data structure that we are calling a super_segment is created with every new block in the beacon chain that includes the commitment of new segments to the global history, and is used to aggregate the commitment of several segments, limiting the amount of information required by nodes in order to verify that a segment was successfully committed in the beacon chain, and that it belongs to the global history of the system.\nThe following diagram describes at a high-level (but with a little more of low-level details) the operation described in the previous paragraph. A basic description of how to generate the proofs can also be found in the text box of the bottom-right section of the diagram.\nWhat\u0026rsquo;s next? # Now we need to go from these high-level diagrams to the low-level details of the specific data structures, information required for the proofs, and were they need to be stored to make it available for nodes in the system. My week will be focused on writing a set of GH issues that detail these so I can get feedback from Nazar, and hopefully enable him to start kicking-off their implementation. But as always, if you have any feedback or suggestions in the meantime, please let me know. Until next week!\n","date":"28 April 2025","externalUrl":null,"permalink":"/blog/2025-04-28-proving-blocks-and-segments/","section":"Blog","summary":"\u003cp\u003eWe started last week with two PRs that attempted to describe in detail the operation of sharded\narchiving (\u003ca\n  href=\"https://github.com/nazar-pc/abundance/pull/192\"\n    target=\"_blank\"\n  \u003ePR192\u003c/a\u003e), and the data availability layer\nof the system (\u003ca\n  href=\"https://github.com/nazar-pc/abundance/pull/193\"\n    target=\"_blank\"\n  \u003ePR193\u003c/a\u003e). When I started writing this\nspec, it was meant to be for a broader audience, but we realised after a few rounds of feedback that\nthe project is still in a really early stage and in constant change, so it would be more efficient\nto focus on detailing the parts of the protocol that are currently under-defined instead of trying\nto give a deep overview of the overall operation of the protocol from the get-go. The actual goal\nbehind this protocol specification is to unblock the implementation of a prototype that can help us\ngain certainty about the design decisions that we are making, and surface potential blind spots in\nthe design, and not to have a reference spec (just yet).\u003c/p\u003e","title":"Proving blocks and segments","type":"blog"},{"content":"The last week was lighter on major changes, but there was a lot of cleanups and refactoring done to prepare Subspace components reuse for building a new blockchain from scratch. Also some improvements based on new developer feedback.\nSubspace refactoring # This has been happening on and off for some time, but I\u0026rsquo;m at the point when the pieces of code from Subspace need to be moved into Substrate-independent new part of the codebase.\nSince we\u0026rsquo;re building a substantially different blockchain, some assumptions made in Subspace are not directly applicable and had to change. The good thing is, it can be done by modifying code in place and keeping Substrate-based node operational in the meantime.\nFor example, I mentioned in previous updates the aim to make blockchain fundamentally post-quantum secure. This means the core of the protocol should not be locked into something that would not be post-quantum secure, while at the same time elliptic curve crypto today is way more efficient than any PQC schemes. One place conflicting with this was the use of the public keys in the solution. Interestingly, nothing in consensus really cares about the public key. Instead, public key hash was used to create plots on the farmer and verifying solution on the node, the only place where the actual public key was needed is to verify block signature. PR 200 recognized this fact and replaced public key with its hash in Solution data structure, opening the possibility to hash any kind of public key without changes to this fundamental data structure. The block signature verification is aware of the public key, of course, and will have to be adjusted once more signature schemes (including PQC) are introduced, but that is a very narrow scope that is easier to handle.\nPR 204 finally added test to modernized erasure coding implementation and after further tweaks moved it under crates (rather than subspace). It also introduced a bunch of new types that add a lot of type safety in places where type aliases were used before. Together with PR 199, this allows adding helper methods on many data structures, which were previously standalone methods. For example, it is now possible to call solution.verify() instead of verify_solution(\u0026amp;solution, ...), which I think is a bit more elegant to write and easier to read.\nBlock was always u32 in Subspace codebase, but for long-running blockchain that may end up using smaller block times it feels too small, so PR 207 changed it to u64, discovering places where type aliases were misused. Similarly, Subspace is based on Substrate and expected that pallets would be able to provide object mapping logic for their transactions, but that seems at odds with high-performance blockchain and general Blockchain as a library architecture. This is why PR 208 removed object mapping logic from the runtime, expecting that application-specific object mapping will be done by developers using APIs that the native blockchain will expose to developers off-chain.\nFinally, there were updates to the document describing the difference from Subspace implementation, which for now is the closest thing we have to a specification. Notably, PR 196 renamed all commitments/witnesses to roots/proofs now that KZG is no longer used and was replaced with Merkle Trees.\nNew APIs and developer feedback # Serge reached out to me recently and was kind enough to try building some contracts, for which I prepared a couple of handy data structures in PR 203 and then implemented some fixes and improvements in PR 209, PR 210 and PR 211.\nThe feedback was very detailed and extremely helpful, I was only half-joking when I told him that he is the first developer on a blockchain that doesn\u0026rsquo;t even exist. I hope his adventure will end up in more improvements and additional example contracts in the repository for others to learn from.\nUpcoming plans # Now with a lot of refactoring complete, I\u0026rsquo;ll be moving more code from subspace directory as it is in good enough state and start designing many consensus-related system contracts. I\u0026rsquo;ll try to somehow use them in Subspace codebase still, so I can keep some version of the node working end-to-end at all times, but not yet sure how feasible it really is.\nIf any of this is interesting to you at all, feel free to join our Zulip and let\u0026rsquo;s keep the conversation going there.\n","date":"28 April 2025","externalUrl":null,"permalink":"/blog/2025-04-28-subspace-codebase-refactoring/","section":"Blog","summary":"\u003cp\u003eThe last week was lighter on major changes, but there was a lot of cleanups and refactoring done to prepare Subspace\ncomponents reuse for building a new blockchain from scratch. Also some improvements based on new developer feedback.\u003c/p\u003e","title":"Subspace codebase refactoring","type":"blog"},{"content":"Most blockchain implementations are pieces of software that include the logic to support many different roles/features, possibly all at once: bootstrap node, block producer, RPC node, archival node, light client, etc. That is one way to do it, but one thing I learned over the years is that you can do a lot of interesting optimizations if you can apply additional constraints during the design phase.\nSo why is basically everyone trying to combine all into one? Let\u0026rsquo;s consider different roles separately first and see what is special or interesting about them.\nBootstrap node # Bootstrap node is a crucial role of P2P networks, you have to know an existing node in the network to join and software usually comes with a set of bootstrap nodes preconfigured just for this. One can use their own existing node for this or ask a friend, but from my experience, most people don\u0026rsquo;t and expect things to work out of the box without extra configuration.\nWhat does the bootstrap node need? Generally just networking and as little bandwidth as possible so it can sustain a lot of connections. What it certainly doesn\u0026rsquo;t need is running a fully-featured blockchain node, yet this is currently the only way to run Substrate-based node and that can easily cause issues, especially if some parts of the node are inefficient/unoptimized.\nBlock authoring node # Block producers in contrast to bootstrap nodes don\u0026rsquo;t need as much network connectivity, but they should be able to follow the chain, maintain transaction pool and create blocks when they have a chance according to the consensus mechanism rules. This should be done with as little latency as possibly (both in networking stack and execution) since any delays can reduce the rewards. There is no need to store or query deep history/state, and reorgs are typically only a few blocks deep. It doesn\u0026rsquo;t really need to know or do much beyond checking that blocks follow consensus rules and author the next block occasionally.\nRPC node # An RPC node can be used to query blockchain information by browser extension and other tooling, sometimes (often?) used to index the blockchain to build a block explorer (although inefficiently). In contrast to block producer, it is important to query blocks and transactions, capture various events happening on a blockchain and associate them with user transactions (e.g., to display transaction confirmation in a wallet). Depending on use case delays in block import may be less important since user may want to wait a few blocks for transaction to be confirmed anyway, so a second here or there doesn\u0026rsquo;t matter as much.\nArchival node # Archival nodes are often combined with RPC nodes to be able to query deeper history/state. However, in many blockchains they are also important for the ability to sync from genesis, which is generally considered to be the most secure (although inefficient) way to sync a blockchain node. In contrast to block authoring, there is no rush to import blocks ASAP at all. Archival node can even afford to wait for block to be reasonably deeply confirmed before even bothering to import it, which will barely compromise its functionality.\nLight client # Light client is the most special out of all mentioned so far. The goal of the light client to be \u0026ldquo;light\u0026rdquo; in terms of CPU, storage and memory requirements. In some cases, it may run in a smart contract, a very constrained environment! More likely, though, it may be a component of a wallet application, making it somewhat independent of RPC nodes and more secure and private than simply querying a centralized server. Light client typically doesn\u0026rsquo;t care about transactions at large, only monitoring a small subset of them that are of interest, otherwise simply verifying block headers to stay in sync with the network.\nWhat does this tell us? # Well, as we can see, different kinds of nodes have sometimes conflicting requirements, so trying to design a single piece of software that serves all (or most) of them will inevitably lead to inefficiencies.\nFor example, block author can probably afford to store recent data in memory and prune most of the historical data rather quickly, which leads to interesting performance optimization opportunities.\nThe archival RPC node needs to store a lot of historical information, and it needs to be efficiently retrievable, but it is not practical to fit it all into memory. Moreover, while the majority of requests will likely hit information from recent blocks, there might be older blocks of historical significance that might be queried relatively frequently, so caching might be very helpful to respond to queries quickly.\nLight clients are a completely different beast, often requiring a separate reimplementation with a different set of tradeoffs like smoldot.\nReal world # The real world is even messier than the above description. When hitting a public RPC endpoint, you may assume you\u0026rsquo;re connecting an RPC node described above. However, due to the need to serve a large number of clients, rate limit (and sometimes charge) connections/requests, you\u0026rsquo;re likely connecting to a complex geo-distributed cluster composed of many different pieces of custom software. It just happens to provide an identically looking RPC endpoint, but internals are completely different.\nIf you tried to index a blockchain through queries to RPC endpoint of an archival node, you may have noticed how painfully slow it could sometimes be, with even JSON encoding/decoding alone taking a non-negligible amount of compute. This doesn\u0026rsquo;t even account for the fact that blockchain indexing may require more detailed information than \u0026ldquo;standard\u0026rdquo; RPC can return.\nMake it a collection of libraries instead # What if there wasn\u0026rsquo;t a single implementation for all use cases? What if instead there was a collection of libraries, which can be composed into larger apps in a way that makes most sense instead?\nThis is to some degree what Substrate allows to do. In fact, Subspace does use it as a library. In contrast to most Substrate-based chains, it doesn\u0026rsquo;t use Substrate\u0026rsquo;s CLI interface at all. This allows for Subspace node implementation itself to be a library, which together with a farmer component was wrapped into a desktop application in the past, most recently into Space Acres. But even then, Substrate is not as flexible or at least not as easily usable for building blockchain client implementations with completely different requirements. It was designed to build different kinds of blockchains and serve that role successfully. However, it is often very difficult or even impossible to swap important parts and challenging to hyper-optimize due to a lot of generic APIs that any custom implementation must satisfy, whether it needs them or not (ask me how I know).\nBootstrap node implementation could pick just the networking stack and configure it in a way that supports multiple clients.\nBlock producer could include a hyper-optimized execution environment with a blazing fast database that doesn\u0026rsquo;t hit the disk very often and doesn\u0026rsquo;t need a lot of disk space. RPC node may not be needed at all.\nIndexing software can embed blockchain as a library, bypassing constraints and inefficiencies of an RPC interface. Extensible VM might allow transaction introspection with arbitrary precision (down to individual contract calls or even specific instructions) that requires much more processing power without affecting other kinds of clients. And the results can be written into a high-performance distributed database like ScyllaDB where they will actually live for production needs, possibly without maintaining historical information on the node itself.\nHaving Proof-of-Archival-Storage consensus means archival nodes are not needed to sync from genesis at all.\nLight client could combine core logic that verifies the consensus, combine it with a custom networking stack that compiles to WASM and lives in a browser extension.\nThis is what we do # We\u0026rsquo;re not building one-size-fits-all massive blockchain node implementation, but rather a collection of libraries that can be combined in various ways to satisfy a diverse set of requirements. The ability to reduce the design space for each unique use case allows writing better software. Software, which works faster, needs fewer resources and delights its users.\nThe reference implementation will offer an optimized block authoring node and likely a desktop application like Space Acres for the same purpose. There should be examples of other nodes/clients and abstractions to make them possible, but it\u0026rsquo;ll require a community/ecosystem effort to produce high-quality software that does things that people need.\nA lot of software engineering is about picking the right set of tradeoffs, and I believe this is the way to go here. If you agree or disagree and would like to discuss it, join our Zulip, I\u0026rsquo;ll be happy to chat about it.\n","date":"26 April 2025","externalUrl":null,"permalink":"/blog/2025-04-26-blockchain-as-a-library/","section":"Blog","summary":"\u003cp\u003eMost blockchain implementations are pieces of software that include the logic to support many different roles/features,\npossibly all at once: bootstrap node, block producer, RPC node, archival node, light client, etc. That is one way to do\nit, but one thing I learned over the years is that you can do a lot of interesting optimizations if you can apply\nadditional constraints during the design phase.\u003c/p\u003e\n\u003cp\u003eSo why is basically everyone trying to combine all into one? Let\u0026rsquo;s consider different roles separately first and see\nwhat is special or interesting about them.\u003c/p\u003e","title":"Blockchain as a library","type":"blog"},{"content":"Over the past weeks, my updates have highlighted many of the ideas emerging from our open design discussions. Now that we have a clearer direction for the design, I wanted to consolidate these ideas into a draft spec. This will serve as a foundation for implementing the first few prototypes, while also providing a structured way to gather feedback and uncover potential blind spots. I expect this spec to suffer significant changes, but it felt like the perfect way to consolidate the ideas, get feedback from the community, and unblock Nazar in case he wants to start prototyping some of the ideas we\u0026rsquo;ve been discussing.\nWhere to find the spec and the first drafts # PR192 adds a README to the specs directory including a description of all the components of the spec that have been drafted, and that I am currently working on. In this PR you can already find a first draft of the spec for the sharded archiving protocol that we\u0026rsquo;ve been discussing the past few weeks. Additionally, PR193 includes the draft for the data availability sampling protocol.\nThese drafts are not final, and may still require several rounds of feedback and iterations, so don\u0026rsquo;t expect these PRs to be immediately merged. However, you can already follow the progress through these open PRs, as well as use them to provide direct feedback about the designs. Even more, by giving us feedback in the PRs, your suggestion can be directly incorporated into the spec as we continue refining it (otherwise, you know that you have the Zulip server available for discussions and feedback).\nWhat\u0026rsquo;s next? # I decided to keep this update brief so you can focus on reading the spec and providing feedback. You will find there all the new ideas and progress from the past week. Let me hype it a bit by giving you the highlights and main changes from last week:\nInstead of using super commitments as a way to recursively commit segments to the global history of the beacon chain, we\u0026rsquo;ve simplified the protocol to allow shards, independently of their level in the hierarchy, to directly commit their segments to the global history in the beacon chain. Super segments are now created by the beacon chain to allow light clients to easily verify that shard segments belong to the global history in the beacon chain, given the right witnesses. The data availability sampling protocol specification now includes a more detailed description of the end-to-end of the protocol, including what happens if a block (or a segment) are flagged as unavailable, and how farmers in the beacon chain handle this event. This week I hope to start collecting a lot of feedback for the draft specs so I can iterate on the design and add more details and improvements where needed. In parallel, I am already figuring out how sharded plotting will work on top of sharded archiving. Fortunately, all the work Nazar has been doing to replace KZG by Merkle proofs already handles (hopefully) a lot of the heavy lifting of what will be needed for plotting.\n","date":"21 April 2025","externalUrl":null,"permalink":"/blog/2025-04-21-the-beginning-of-a-spec/","section":"Blog","summary":"\u003cp\u003eOver the past weeks, my updates have highlighted many of the ideas emerging from our open design\ndiscussions. Now that we have a clearer direction for the design, I wanted to consolidate these\nideas into a draft spec. This will serve as a foundation for implementing the first few prototypes,\nwhile also providing a structured way to gather feedback and uncover potential blind spots. I expect\nthis spec to suffer significant changes, but it felt like the perfect way to consolidate the ideas,\nget feedback from the community, and unblock Nazar in case he wants to start prototyping some of the\nideas we\u0026rsquo;ve been discussing.\u003c/p\u003e","title":"The beginning of a Spec","type":"blog"},{"content":"Last time I mentioned that I was looking into Merkle Trees to replace KZG. This week it happened, the whole codebase is basically free from KZG. The only place that is not fully fixed and where I am looking for help is GPU plotting, it broke with all these changes and isn\u0026rsquo;t the highest priority for me to fix right now.\nReplacing KZG # KZG was used for vector commitments in a few places, including archiving and plotting.\nFor archiving, it was easy: we have records with chunks to commit to, then we take commitments of all roots and create another commitment over them and include corresponding witnesses/proofs in each piece, so they can be verified against global history. Works basically the same way as before, but the proof is now larger than before.\nFor plotting, it turned out to be a bit more tricky. There is a homomorphic property that allowed farmer to extend the polynomial created over record chunks to get some parity chunks (erasure coding), but still being able to generate proofs for parity chunks that verify against original record commitment. With Merkle Trees that is no longer the case, but with both Merkle Tree creation and erasure coding (see below) being so much cheaper, we can erasure code record chunks during archiving and commit to them too. This is still fast, and when the farmer is redoing the same erasure coding later, they can generate proofs that successfully verify against record commitment. Problem solved!\nPR 175 is where KZG was completely replaced with Merkle Trees, but a lot of TODOs remained in the code for further cleanups. This alone more than doubled the archiver performance, while doing 2x erasure coding and commitments than before!\nPR 180 also added parity chunks directly to the piece, such that when doing piece verification, it isn\u0026rsquo;t necessary to do erasure coding (and heap allocations as the result). In fact, after some more changes and refactoring, PR 187 finally made it possible to use subspace-verifiction in no_std environment with no heap allocations.\nReplacing BLS12-381 erasure coding # With KZG gone, we were still doing erasure coding using BLS12-381, but we don\u0026rsquo;t have to!\nAfter looking at various options, I ended up picking reed-solomon-simd, which appears to be the fastest readily available library in Rust and has a reasonably nice public API. Unfortunately, it doesn\u0026rsquo;t yet work in no_std environment, so I sent a corresponding PR implementing that.\nErasure coding was swapped from using grandinetech/rust-kzg in PR 181, which again more than quadrupled archiving performance, wild!\nFurther archiver improvements # The usage of BLS21-381 meant we could only operate on 254-bit values, which in practice means 31-byte chunks for efficiency and simplicity purposes. This resulted in a lot of boilerplate across the codebase, introduction of RawRecord vs normal Record (first contains a multiple of 31-byte chunks vs 32-byte chunks of normal record). This was also very annoying for data retrieval since even within a single record, it wasn\u0026rsquo;t possible to simply slice the bytes. It was necessary to chunk the record into 32 byte chunks and then throw each 32nd byte away, what a mess!\nNow that BLS12-381 was completely gone from both commitments and erasure coding, it was possible to end this.\nFirst, PR 182 unified chunk size to be 32 bytes. This simplified the code a lot across the board.\nAnother issue that plagued the codebase for a long time was source/parity pieces/records interleaving. This is how polynomial extension worked in [grandinetech/rust-kzg,] and we went with it as a natural behavior, but it was another source of complexity and inefficiency, especially in data retrieval. PR 184 ended that too, streamlining the code even further.\nFinally, with refactoring in PR 185 and PR 187, the performance improved even further, while code was even easier to read.\nThe results and lessons learned # So what are the results? Multithreaded archiving time decreased from ~4.5 seconds per segment to just 0.24 seconds. This is ~18x performance improvement ðŸŽ‰. In fact, single-threaded archiving at 1.8 seconds is now faster than multithreaded before, imagine that! And there are still opportunities for performance optimizations left to explore if it ever becomes critical. This also implies much faster piece reconstruction in case it is necessary due to node sync or plotting, this was a very costly operation in the past.\nMoreover, I originally thought that the logarithmic proof size would increase the piece overhead. It turns out we\u0026rsquo;re saving a lot more by not wasting each 32nd byte of the record on padding due to KZG, so there is actually a large utilization improvement! It will actually be very handy as out proofs increase in size when we add sharding to the mix. The only place where it does use more space is the solution data structure. While unfortunate, being one per block it is not too bad.\nWhen designing Subspace we had many iterations of the design, and I think at some point we were stuck with KZG without necessarily fully taking advantage of it. There were various ideas about things like distributed archiving, which might end up benefiting from KZG, but at this point I don\u0026rsquo;t really think it is worth it. It is too slow, cumbersome to use and not post-quantum secure.\nIn retrospective, all the changes described here could be applied to Subspace if we looked back at things from the first principles after we have designed the latest version of the protocol.\nFuture improvements # Funnily enough, these are not all the breaking changes I want to do to the codebase. issue 183 describes the remaining known data retrieval challenges only discovered after mainnet launch (we didn\u0026rsquo;t really try to retrieve a lot of data during testnets) and should be addressed to greatly simplify the logic. Once done, retrieval will be much more efficient and easier to reason about (current code in Subspace is very convoluted, but unfortunately it has to be to maintain efficiency, naive retrieval is much more wasteful).\nUpcoming plans # So what is next? Well, with a bunch of technical debt dealt with, I\u0026rsquo;ll probably try to put some of the consensus logic into system contracts. All system contracts have so far managed to avoid heap allocations and making subspace-verification work without any was a stepping stone towards using it in contracts too.\nIn no so far future I\u0026rsquo;d like to have a simple single-node \u0026ldquo;blockchain\u0026rdquo; that produces blocks, while not being based on Substrate in any way. It\u0026rsquo;ll take time to get there, but progress is being made towards that.\n","date":"20 April 2025","externalUrl":null,"permalink":"/blog/2025-04-20-very-fast-archiving/","section":"Blog","summary":"\u003cp\u003eLast time I mentioned that I was looking into Merkle Trees to replace KZG. This week it happened, the whole codebase is\nbasically free from KZG. The only place that is not fully fixed and where I am looking for help is \u003ca\n  href=\"/book/Contribute.html#gpu-plotting\"\u003eGPU plotting\u003c/a\u003e, it\nbroke with all these changes and isn\u0026rsquo;t the highest priority for me to fix right now.\u003c/p\u003e","title":"Very fast archiving","type":"blog"},{"content":"This week has been another good week of progress. I finally have a good idea of how shard archiving should work in the happy path, and I\u0026rsquo;ve started writing a low-level spec for it (that I am hoping to push to this repo soon). Unfortunately, there is still a slight gap in the spec that we need to fill before we can move forward: the data availability problem.\nCurrent state of sharded archiving design # High-level, this is how the end-to-end of sharded archiving currently looks like:\nBlock Creation (L2 Child): New blocks (blk1, blk2, etc.) are created normally in the child shard. As soon as they are created, their headers (or block hashes) are immediately committed to the parent chain (L1 child). Parent Shard Tracking (L1 Child): The parent shard keeps track in a block buffer of all the headers (or hashes) from the child shard blocks that have been committed, creating in this way an intermediate block history of all its child shards until a new segment from the shard arrives. Block History Management (L2 Child): When the history buffer in a shard accumulates enough blocks, a new segment is created in the shard. This triggers the creation of a super segment (super_segment_L2_1) which includes information about all segments and commitments added to the history and that is correspondingly submitted to the parent for commitment. It is worth nothing that super segment do not propagate the whole segment, but aggregate commitments for the segments that can be used to verify them in the upper layers of the system. Recursive operation (L1 Child): The protocol is recursive, so in the same way that the L1\u0026rsquo;s child shard was committing blocks to the parent as they were being created, it will do the same with its own parent (the beacon chain). Super-Segment Commitment (L1 Child): When a super-segment from a child (super_segment_L2_1) is committed, it is added to the shard\u0026rsquo;s history buffer along with other local segments that may have been created, triggering the clearing of the block buffer for child shards. Once a segment is committed in the parent, there is no need to keep raw blocks in the buffer anymore as they are explicitly available in the committed segments. Beacon Chain Integration (L1 Child): The beacon chain receives the blocks and segments from the immediate shards below (L1 shards), committing them to its chain (as any other regular parent shard in the system that has immediate child shards below). Super-Segment Commitment (L1 Child): When a super-segment from a child (super_segment_L1_1) is committed, it\u0026rsquo;s added to the shard\u0026rsquo;s history buffer, triggering the clearing of the block buffer. Segment History (L1 Child): Segments in a child shard are created as super-segments, committed with a list of shard and child super-segments created up to that point. Beacon Chain Role: The beacon chain commits all super-segments (segment_commitment_L1_1, super_segment_BC_1, etc.), maintaining the whole system\u0026rsquo;s historical buffer and creating a unified history. How many shards can we afford if we are committing every block? # You see in the description from the previous section that one of the key design changes since last week is that we will be committing every block to the parent to have part of the history of child shards as raw blocks until full segments for the shard are created.\nAssuming that we dedicate only a 10% of the full size of a block for the commitment of child blocks, let\u0026rsquo;s do a back-of-the-envelope calculation of the total number of child shards that we can afford. Assuming:\nBLOCK_SIZE = 4MiB BLOCK_TIME = 6 seconds BLOCK_HEADER = 224B type BlockHeader (260B) { number (8B) extrinsic_root (32B) state_root (32B) parent_hash (32B) shard_id (4B) hash (32B) solution (120B) } And that every 6 seconds all the shards in the lower level submit a block at the same time (this is the average case scenario), we can support the following number of shards in the lower level:\nNUM_SHARDS = (0.10 * BLOCK_SIZE) / BLOCK_HEADER =~ 153 SHARDS This assumes that the block time of shards is also 6 seconds and that the average case scenario happens where all shards submit their blocks at the same time on a block.\nBut maybe committing full block headers is just too much, what happens if we just commit the block (32B) hash and the shard id (4B)? Then things look a bit better, as we can get to the order of the thousand shards under each parent:\nNUM_SHARDS ~= 1111 SHARDS If we come back to our target of around 1M shards discussed last week, these numbers mean that with just two layers of shards (and the beacon chain in the root), we would be able to achieve the desired scale for the system.\nEnter the data availability problem # So far so good, we haven\u0026rsquo;t found any big blockers to the design, right? Well, we still have an elephant in the room that we need to address. While we are only committing the block headers (or hashes) to the parent, and super segments (that include an aggregate commitment for the segments in a shard) we need the raw blocks and segments to archive the history. These commitments can be used for verification, but we still need the full blocks and segments to be available in the shard when we need them. This is the data availability problem.\nFortunately, this is a problem that has been around for a while in the space (especially in the scope of Layer 2s and sharded architectures). There is a lot of literature and working implementations of projects dedicated exclusively to solving this data availability problem for other networks like Celestia or Avail.\nRoughly speaking, all the protocols proposed to solve data availability share common primitives: (i) chunking of blobs into smaller pieces that can be distributed throughout the network, (ii) erasure coding to ensure that the data can be reconstructed even if some pieces are missing, (iii) the use of vector commitments to generate proofs of possession of the data, and (iv) random sampling to verify that the data is available without having to download the entire blob, and to force holders of data to create availability proofs of the stored data.\nNext steps # This week I will be focusing on trying to specify our solution for the data availability problem so I can have a first draft of the end-to-end spec for sharded archiving. High-level, how I am thinking about this is as follows:\nWhen segments are created, they are already in an amenable format to create data availability proofs (with erasure coding and vector commitments), but block aren\u0026rsquo;t. The blocks included in the block buffer will thus have to be chunked and encoded before being committed to the parent to make them more friendly to generating availability proofs. Instead of forcing the sampling rate by protocol, nodes will be allowed to sample the availability of segments at any slot. In order to access the pieces they need to create their plots, farmers need the pieces of these segments to be available, so they are incentivised to do it periodically to ensure that they are available when needed with a high-probability (otherwise they won\u0026rsquo;t be able to create their plot until the piece is recovered). When a piece is sampled and not available, the reporting node will have to submit a transaction reporting the unavailability to the beacon chain. There are small details that need to flesh out from the sketch above, and I am planning to go through many of the data availability protocols to see if we can borrow some of their ideas. Really looking forward to sharing more of my progress next week.\n","date":"14 April 2025","externalUrl":null,"permalink":"/blog/2025-04-14-the-data-availability-problem/","section":"Blog","summary":"\u003cp\u003eThis week has been another good week of progress. I finally have a good idea of how shard archiving\nshould work in the happy path, and I\u0026rsquo;ve started writing a low-level spec for it (that I am hoping to\npush to this repo soon). Unfortunately, there is still a slight gap in the spec that we need to fill\nbefore we can move forward: the \u003cem\u003edata availability problem\u003c/em\u003e.\u003c/p\u003e","title":"The data availability problem","type":"blog"},{"content":"Last week was lighter on code changes and more heavy on research. Specifically, I\u0026rsquo;ve been looking into commitment schemes generally and Blake3 hash function in particular, which was already used in the codebase, but turns out can be applied in more interesting ways than just a hash function.\nKZG and quantum computers # There are several places in blockchains where vector commitments are used, and there are several commitment schemes that might be applicable depending on the use case. Subspace being Proof-of-Archival-Storage consensus, required a way to commit to the archival history of the blockchain among other things, where KZG commitments were used.\nKZG feels magical: it allows committing to data set with both commitment and a witness being fixed size (48 bytes) rather than logarithmic proof when Merkle Tree is used. Another neat thing is homomorphic property, which makes erasure coding of commitments equal to commitment of erasure coded data. This is unfortunately not free as KZG both require a trusted setup (the Subspace team participated in and used parameters from Ethereum\u0026rsquo;s KZG Summoning Ceremony) as well as higher compute cost when committing to data set and generating witness. The drawbacks are unfortunate but manageable.\nRecently, however, I\u0026rsquo;ve been thinking about future-proofing the design and one of the questions that popped-up is cryptography resistance to quantum computers. And sadly, KZG is not resistant to it, just like a lot of cryptography used for digital signatures (for example, when singing blockchain transactions).\nAs a result, I\u0026rsquo;ve been looking into alternatives. While there are some post-quantum schemes out there, they are not as optimized and well studied, also none of them have compact constant size commitment and proofs like in KZG. Not only that, they don\u0026rsquo;t even remotely approach proofs of Merkle Trees, the proofs are simply huge for the use case of archiving and plotting.\nSo the conclusion is, we\u0026rsquo;ll have to use Merkle Trees as a reliable, well studied and high-performance alternative.\nThis issue of quantum computers is the reason why I removed incremental archiving support in PR 168 (workaround that amortizes slow KZG commitment creation). With Merkle Trees the complexity of incremental archiving is unnecessary, so we can simplify the code a bit.\nThe topic of quantum computers will surface a few more times in the future. In the past, transaction handling was already described in a way that is agnostic to the cryptography used for transaction signing. This is also part of the reason why reward address (tied to Sr25519 signature scheme) was removed from Solution data structure in PR 169.\nBlake3 # Blake3 is a modern and very fast hash function. The reason it is fast is not only because the underlying primitives are fast, but also because it is actually based on Merkle Tree internally, rather than more common (in hash functions) Merkle-DamgÃ¥rd construction. Its design allows for both instruction-level and multi-threading parallelism.\nMost use cases probably use it as a regular hash function due to its speed, but it can also be used in a more advanced way as an actual tree! In fact, Bao project uses it for verified streaming, meaning it can verify downloaded contents as it is being downloaded without waiting for the whole file to be downloaded before hash can eb checked. Bao was originally based on already fast blake2 served as a prototype for blake3 initially, while these days it is rebased on blake3. The neat thing is that the hash of the file in Bao is the same as if the file was simply hashed with blake3!\nSo blake3 has a Merkle Tree internally, and we need Merkle Tree to commit to some data. Does this mean we can use blake3 directly instead of building a custom Merkle Tree and use blake3 as a regular hash? Turns out it is not trivial, but yes! And not only that, the cost to create such a tree is basically the hashing of the data, very nice!\nBlake3 is already used in the codebase as it was used in Subspace reference implementation in many places.\nExposing such Merkle Tree will require some low-level access to blake3 primitives and non-trivial logic, so that is delayed for now. However, PR 171 already introduced the first (of likely multiple) Merkle Tree implementation that will be upgraded to take advantage of blake3 properties later.\nOther updates # There were other minor updates, but I want to mention just two.\nA book now has Contribute page with a few topics that we are not actively working on, but would like to see contributions or collaborate on. Please join out Zulip chat to discuss if any of them are interesting, or you have something else of interest in mind.\nA second update is that PR 172 introduced a document that attempts to describe the difference with Subspace specification until the protocol has its own spec.\nUpcoming plans # The immediate next step is probably to swap the KZG commitment scheme with Merkle Tree in the archiving. There was some awkwardness in the implementation due to KZG using 254-bit scalars (or field elements or whatever they are called there), resulting in hopefully redundant abstractions. Though erasure coding will have to be replaced with a different implementation though because it is based on the same BLS12-381 curve right now.\nWe\u0026rsquo;ll see what the future holds for us soon enough, and I\u0026rsquo;ll make sure to post an update about that. See you next week!\n","date":"14 April 2025","externalUrl":null,"permalink":"/blog/2025-04-14-trees-everywhere/","section":"Blog","summary":"\u003cp\u003eLast week was lighter on code changes and more heavy on research. Specifically, I\u0026rsquo;ve been looking into commitment\nschemes generally and Blake3 hash function in particular, which was already used in the codebase, but turns out can be\napplied in more interesting ways than just a hash function.\u003c/p\u003e","title":"Trees everywhere","type":"blog"},{"content":"Welcome post mentioned briefly initial set of constraints that led to the creation of this project, but I figured it might be helpful to have a short writeup about it that might be helpful for sharing.\nIn short: we\u0026rsquo;re building a blockchain.\nBy \u0026ldquo;we\u0026rdquo; I really mean just me and Alfonso so far, but I hope more people will join over time if they find it interesting.\nA blockchain # Yes, we are simply building a blockchain. Just a blockchain, not a blockchain for AI, finance or data storage.\nAll of those things can be built on a blockchain that actually scales, but no one can predict how new technology will be used eventually, so it is important to distinguish: what the thing is from what it can be used for.\nWhy? # Blockchains started as a technology supposed to allow everyone to participate in a distributed permissionless P2P network, where and arrive at consensus for a set of state transitions without trusting anyone.\nUnfortunately, blockchains today are neither permissionless nor distributed, with a lot of trust assumptions and not scalable at all. Frustratingly, I don\u0026rsquo;t see anyone actually trying to fix all of it. Some are fixing some parts of the issue, but a comprehensive solution is lacking.\nProof-of-work that wastes computation went out of favor some time ago, partly because of energy consumption and partly because it ended up fairly centralized in practice. Proof-of-stake dominates the landscape these days but is no longer permissionless and not decentralized either. As of today, two biggest Bitcoin mining pools (Foundry USA and AntPool) have more than 50% of the hashrate, while top-5 produce more than 77%. The situation with Ethereum is not better with top-2 (Lido pool and Coinbase), resulting in more than 50% of the stake with top-5 having more than 84% of the total staked ETH. This is not the future I\u0026rsquo;m looking forward to.\nThe problem is not just that it is not decentralized, those protocols inherently can\u0026rsquo;t be decentralized. Any proof-of-work protocol that gets popular ends up centralized pools, any proof-of-stake ends up with centralized stake. I gave examples of two biggest networks above, but the same exact thing happens across the board.\nObscure languages and execution environments dominate the landscape with Solidity/EVM arguably being the biggest one. Why do we have to reinvent compilers, rewrite cryptographic libraries over and over again? There is so much software written already, wouldn\u0026rsquo;t it be better to be able to include a normal generic C library in your smart contract if it gets the job done?\nThe last big issue I have with most blockchains is that they either aren\u0026rsquo;t even trying to scale or claim to be scalable without actually being scalable. My definition of \u0026ldquo;scalable\u0026rdquo; is that the network is able to store and process more transactions as more participants join the network, without an upper bound. Ethereum\u0026rsquo;s microcontroller-like compute capabilities are not enough, Solana\u0026rsquo;s vertical scalability can\u0026rsquo;t possibly satisfy all possible demand.\nSo what? # You might agree with everything above, but wondering \u0026ldquo;so what?\u0026rdquo;\nI believe that due to countless shortcomings, many interesting applications are simply not being built, not even attempted to be built. A lot of real issues that could be solved with blockchain technology aren\u0026rsquo;t solved because of it.\nThe solution # What we\u0026rsquo;re building is a blockchain to solve all the above issues and then some.\nWe\u0026rsquo;re building a blockchain that can support literally any number of consensus participants. With Proof-of-Archival-Storage consensus, individual participants pledge disk space to store the history of the blockchain. The goal is to have a weekly payout for each terabyte of space pledged even as the blockchain gets enormously large, making pools pointless and real decentralization possible.\nWe\u0026rsquo;re building a blockchain that scales through sharding. The practical constant for max number of supported shards is expected to be one million, with no inherent limit at the protocol level. This means the ability to upload data to the blockchain at a rate of terabits per second and beyond. This also means the ability to get real compute done on chain with millions of modern high-performance CPU cores.\nWe\u0026rsquo;re building a blockchain that allows running applications written in traditional languages, with Rust being the primary target. It will be possible to debug and optimize using traditional tools like gdb and perf. With RISC-V ISA and support for standardized extensions, the code is fast and has access to modern hardware accelerators for things like cryptography without using obscure custom opcodes and VM-specific code, use high-quality high-performance libraries that already exist.\nWe\u0026rsquo;re building a blockchain that is future-proof with post-quantum cryptography.\nWe\u0026rsquo;re building a blockchain that can describe itself through metadata embedded into smart contracts, so you\u0026rsquo;ll never have to do blind signing or trust the computer when using a hardware wallet.\nIn the end, we\u0026rsquo;ll have a user-friendly blockchain that supports billions of transactions per second without compromising on security or distributed nature of the protocol.\nWhat is happening? # We\u0026rsquo;re building a blockchain already, but would love to collaborate and discuss ideas with others.\nJoin our Zulip chat for discussions and check Contribute page for the most pressing issues that are not being worked on right now, but should be.\n","date":"8 April 2025","externalUrl":null,"permalink":"/blog/2025-04-08-we-are-building-a-blockchain/","section":"Blog","summary":"\u003cp\u003e\u003ca\n  href=\"../2025-01-13-welcome\"\u003eWelcome\u003c/a\u003e post mentioned briefly \u003ca\n  href=\"https://gist.github.com/nazar-pc/760505c5ad7d56c20b2c75c1484e672f\"\n    target=\"_blank\"\n  \u003einitial set of constraints\u003c/a\u003e that led to the creation of this project, but I figured it\nmight be helpful to have a short writeup about it that might be helpful for sharing.\u003c/p\u003e\n\u003cp\u003eIn short: we\u0026rsquo;re building a blockchain.\u003c/p\u003e\n\u003cp\u003eBy \u0026ldquo;we\u0026rdquo; I really mean just me and Alfonso so far, but I hope more people will join over time if they find it\ninteresting.\u003c/p\u003e","title":"We are building a blockchain","type":"blog"},{"content":"This week I\u0026rsquo;ve gone a bit deeper into the design of the multi-shard Subspace protocol idea which I briefly introduced in my last update. The protocol is conformed by the following parts:\nSharded archiving, responsible for creating a global canonical history of the whole system, and of creating the history records that will eventually become part of farmers\u0026rsquo; plots. Sharded plotting, which takes records from the global history and seals them in plots that include segments of the history of every shard of the system, and that will be used for the farming process. And finally, merged farming, which is the protocol responsible for challenging farmer plots, and deriving the corresponding winning tickets that elect block proposers in specific shards. Let me introduce the high-level operation behind each of these sub-protocols, while digging deep in the one that I\u0026rsquo;ve focused the most on this week: sharded archiving.\nAdding the pieces together # High-level, this is how I am imagining each of these sub-protocols working together:\nFarmers self-assign themselves to whatever shards they want to farm and participate in. At this point, this is a free choice, but we will build the rewards and incentive model in a way where rational farmers will prioritise farming on shards that have less competition, thus balancing the power in the system among all shards. Farmers in every shard, independently, build the history buffer for the shard with new segments as new blocks are created and considered final in the shard. As new segments are created in a shard, their segment commitments (along with some extra metadata useful for verification) are committed to the shard\u0026rsquo;s parent so they are propagated up the hierarchy to the root (beacon chain) where the global history of the system is constructed. This history buffer in the beacon chain includes information about all segments in all shards (we will discuss briefly how this can be done in the next few sections). The way in which farmers assign themselves, i.e. show interest in participating in a shard, is by committing storage in a specific shard. So in the multi-shard version of the Subspace protocol, space is assigned exclusively to a shard, and plotting is performed in a way where sectors need to be explicitly linked to a shard. For plotting, the selection of pieces for a plot is performed globally by selecting pieces from any shard available in the global history at the beacon chain. All plots are built with pieces from all shards, and the selection of pieces is done in a way that is proportional to the amount of space assigned to each shard. So we see that plotting relies on the construction of this global history in the root of the hierarchy for its operation, but the overall mechanism is still the same as in the single-shard Subspace protocol. Finally, farming is done similarly to how is done in single-chain Subspace protocol, but with the difference that winning tickets of a plot belong exclusively to the shard where that plot is committed. I will start sharing more details on how this is done in the next few weeks, but the idea is that shards\u0026rsquo; difficulty, and thus their solution range, are independent and dynamically adjusted according to the storage committed in the shard and their historical block generation rate. Even more, we are thinking of a way to allow farmers to \u0026ldquo;merge\u0026rdquo; their plots across shards, so they can farm in multiple shards at the same time when the power in a shard is low. For this, we expect plots to be primarily committed to a specific shard, and be assigned to a set of fallback (secondary) shards so when something bad happens in one of these secondary shards, or the storage (and thus the block farmers will be allowed to generation rate) falls, farmers secondarily assigned to these shards are allowed to draw winning tickets and vote (or propose) blocks in these shards. Sharded Archiving # The operation of sharded archiving is quite straightforward, nodes in a shard are creating segments of their history that need to be submitted to the beacon chain to make them part of the global history of the system. The first question that we may ask ourselves is: can we have all shards committing segments commitments (or headers) directly into the beacon chain as soon as they are created?\nCan we commit all segment headers to the beacon chain? # Let\u0026rsquo;s a assume we have in the order of 1M shards in the system with:\nRECORDED_HISTORY_SEGMENT_SIZE = 128MiB WITNESS_SIZE = 48B COMMITMENT_SIZE = 48B BLOCK_SIZE = 4MB BLOCK_TIME = 6s This translates into:\nBLOCKS_PER_SEGMENT = SEGMENT_SIZE / BLOCK_SIZE = 32 SEGMENT_GENERATION_TIME_SHARD= (BLOCK_TIME * 32) = 192 seconds SEGMENT_RATE_PER_SLOT = 0.03 Assuming the SegmentHeader with the following struct being committed to the beacon chain:\nV0 { /// ShardID shard_id: ShardID, (20bits ~ 4Bytes) /// Segment index segment_index: SegmentIndex, (8 Bytes) /// Root of commitments of all records in a segment. segment_commitment: SegmentCommitment, (48 Bytes) /// Hash of the segment header of the previous segment prev_segment_header_hash: Blake3Hash, (32 Bytes) /// Last archived block last_archived_block: LastArchivedBlock (8 Bytes), } So if we commit each SegmentHeader we will be committing 100 bytes per segment (committing the segment header is the worst case scenario, we could optimise this and make it more compact by just committing the segment commitment with some additional metadata, but let\u0026rsquo;s consider the worst case scenario for this exercise).\nIf we get all shards committing their segment headers to the beacon chain directly instead of letting segment commitments flow up through the hierarchy, this means:\nAVG_SEGMENTS_PER_SLOT_IN_BEACON = NUM_SHARDS * 0.03 = 30k SEGMENTS / SLOT BYTES_COMMITTED_PER_SLOT_IN_BEACON = AVG_SEGMENTS_PER_SLOT_IN_BEACON * SEGMENT_HEADER_SIZE = 30 * 100 KBYTES = ~3MB With 4MiB blocks, committing all segments into the beacon chain could actually fit a single block of the beacon chain with the current rate being considered.\nThe number of lower level shards that can be supported by a parent network (and thus the beacon chain) with 4MiB blocks would be: NUM_SHARDS = BLOCK_SIZE / (SEGMENT_RATE_PER_SLOT * SEGMENT_HEADER_SIZE) = 4 / (0.03 * 100) = ~1.3M Shards This is great! It means that with a beacon chain and a single level of shards below committing their segments directly as soon as they are created we could get to the order of 1M independent shards.\nBut what if we use larger proofs? # The back-of-the-envelope calculation above assumes 48B commitments for segments, but what if we want to use a type of vector commitment for segments that is not KZG and requires bigger proofs? Then we may not be able to commit every proof, and we may need to rely on several layers of shards forming a hierarchy where parents aggregate segments to propagate them up and commit them to the global history. On top of this, we also may want to aggregate commitments from lower levels to minimise the state growth in the beacon chain, but this is a secondary concern at this point.\nI actually explored several solutions for this throughout the week:\nThe first one is a bit complex and may have been an overkill in hindsight (thank you Nazar for pointing this out). I won\u0026rsquo;t jump into the nitty gritty details and will leave this image explaining the high-level here for reference. The idea was for every parent shard to build a tree with all the segments from their children. With every new segment created in the parent chain an updated commitment for the tree of children segments is attached to the parent shard segment along with proofs for all the new segments included in the tree since the last update. In this way, parent chain segments implicitly propagate relevant information about their children to their upper layers (and eventually the beacon chain). Nazar rightly pointed out the following: why wait for parent chain segment generation as the cut-off point for committing segments? Why not batch whatever segments from the children have been submitted to a shard to propagate them to upper layers as soon as they have been included in a block? And this is the solution that I will consider for the design. The process is as follows: Let\u0026rsquo;s take as an example the tree of shards from the figure above. Shards C and D are generating new segments and submitting their commitments (or segment headers) to their parent, A. These segments are submitted to A by nodes from C and D as transactions that will be included in a block of A. As soon as A verifies a block including transactions with segments from C and D, nodes in A will create a \u0026ldquo;super segment\u0026rdquo; commitment (this is how we are going to call this data structure), that creates a commitment for all the segments included in the block of A. Nodes in A will be periodically creating their own segments and committing them to the beacon chain, as well as the super segment commitment for the segments from C and D triggered through the submission of segments in A. The super segment commitment will be a vector commitment that proves the commitment of segments in the history of A, and are used to include segments from deeper layer in batch into the global history kept in the beacon chain. Super segments will be sequenced along with regular segments from immediate children of the beacon chain conforming the global history of the system with all of the segments created in all shards. The purpose of committing these super segments from lower level shards into the beacon chain is to create the global history of the system. As soon as a super segment is committed in the beacon chain, the underlying history segments from the shards conforming this super segment are appended to the global history (implicitly sequencing the history of all shards in the global history of the beacon chain). Super segments include additional metadata with information about the segments included in them and any additional information required for verification (e.g. an aggregate commitment for the individual segments of the super segment). More details on this will be included in the spec I am currently working on. As a reference for folks with deep knowledge of the implementation of the Subspace protocol, the idea is to get a lot of inspiration to how incremental archiving currently works for the aggregation and submission of segment commitments into upper layers of the hierarchy.\nWhat\u0026rsquo;s next? # For next week I am hoping to get a first draft of the spec for sharded archiving with more lower level details of how it works (and how I imagine the implementation to be). It may be a stretch, but I am also hoping to have a good picture of how sharded plotting will look like.\nOne of the early mandates that I got from Nazar when I started was to identify any potential reasons why a system like this may not be feasible or may not reach the scale that we are aiming for, and so far I haven\u0026rsquo;t found any, which is exciting! That being said, please reach out if you see any potential flaws on my reasoning.\nHope to see you all in the next update!\n","date":"7 April 2025","externalUrl":null,"permalink":"/blog/2025-04-07-merged-farming/","section":"Blog","summary":"\u003cp\u003eThis week I\u0026rsquo;ve gone a bit deeper into the design of the multi-shard Subspace protocol idea which I briefly introduced in\nmy last update. The protocol is conformed by the following parts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSharded archiving, responsible for creating a global canonical history of the whole system, and of creating the\nhistory records that will eventually become part of farmers\u0026rsquo; plots.\u003c/li\u003e\n\u003cli\u003eSharded plotting, which takes records from the global history and seals them in plots that include segments of the\nhistory of every shard of the system, and that will be used for the farming process.\u003c/li\u003e\n\u003cli\u003eAnd finally, merged farming, which is the protocol responsible for challenging farmer plots, and deriving the\ncorresponding winning tickets that elect block proposers in specific shards.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet me introduce the high-level operation behind each of these sub-protocols, while digging deep in the one that I\u0026rsquo;ve\nfocused the most on this week: sharded archiving.\u003c/p\u003e","title":"Merged Farming","type":"blog"},{"content":"The majority of last week I spent tinkering with Subspace codebase after importing it here in preparation for building an actual blockchain.\nAs mentioned in previous updates, transaction was the next logical step and after some preparation in PR 149 initial version landed in PR 151. It is quite basic and will likely require significant changes before used in an actual blockchain implementation, but there are just too many unknowns for now.\nOn a similar note, PR 156 added an initial version of the native token, which will be important for charging transaction fees. It is relatively simple for now, but in the spirit of what it will eventually end up being.\nWith that, Subspace codebase was imported in PR 152. This initial PR removed a bunch of features of the original codebase that will not be applicable here (domains, votes, etc.). There were many follow-up PRs that removed/refactored more things to reduce the feature set and reduce Substratisms from the codebase (PR 154, PR 155, PR 157, PR 158, PR 159). As the result the blocks can still be produced, but RPC is mostly gone (only supports what farmer needs) and many of the secondary features are gone too.\nBlockchains are hard # Now the challenge is figuring out where to go from there. Substrate is great because it gives a framework to work with, where an initial version of the blockchain may not be exactly what you want, but at least you have something working all the time. When bootstrapping from the ground up, it is easy to get lost since there are countless \u0026ldquo;chicken and egg\u0026rdquo; kinds of problems.\nThe plan right now is to first extract core consensus logic contained in the runtime into system contracts. After that to somehow get enough of node infrastructure up to get a single-node blockchain without P2P networking producing blocks.\nP2P networking for blockchain will be a major effort since while Distributed Storage Network (DSN) in Subspace is standalone, block and transaction propagation, PoT gossips are all built on top of Substrate and were never migrated off of it. Given our sharding requirements, it is likely that this (originally) DSN networking stack will have to evolve to support more things, but that is a big challenge.\nSo yeah, blockchains are hard to pull of the ground, but when building something truly different, you sometimes have to. Thankfully, some lessons were learned, and the logic already written and audited in Subspace codebase can be partially reused to speed up the process (hopefully).\nUpcoming plans # It is hard to predict how such open-ended projects will go, but one way or another I\u0026rsquo;ll be working on building the blockchain to allow us experimenting with sharding that Alfonso is designing. One thing for certain is that you can expect regular progress updates along the way.\nUntil next time ðŸ‘‹\n","date":"7 April 2025","externalUrl":null,"permalink":"/blog/2025-04-07-preparing-for-blockchain/","section":"Blog","summary":"\u003cp\u003eThe majority of last week I spent tinkering with Subspace codebase after importing it here in preparation for building\nan actual blockchain.\u003c/p\u003e","title":"Preparing for blockchain","type":"blog"},{"content":"The majority of the last two weeks I\u0026rsquo;ve been busy with the installation of the antivirus system update for my immune system. It was neither pleasant nor quick, but now that it is slowly approaching 100%, I\u0026rsquo;m back with another update of what I managed to do since the last update.\nI was able to conduct a few interviews with people of different background that helped to improve documentation (PR 134, PR 138, PR 141, PR 144). Overall good discussions, but no major issues were uncovered so far. Method context seems to be a more difficult concept to grasp that I don\u0026rsquo;t think can be avoided, but hopefully the latest revision of the book helps with understanding a bit.\nContracts as ELF files # I\u0026rsquo;ve spent a lot more time tinkering with ELF files and various options of the compiler and linker to see what would be the most ergonomic way to produce compact files without requiring user to learn some custom tooling. As a result, I came to the conclusion that a shared library is probably the best path going forward.\nContracts inherently behave like libraries that have multiple callable entrypoints, so a standard shared library is a good fit because it already contains all the necessary details about exported symbols. Another thing I really wanted to avoid is custom syscalls or other implicit \u0026ldquo;environment\u0026rdquo; for contracts to interact with. Using import symbols might be a good middle ground that avoids hardcoding a specific implementation, allowing VM to use whatever makes sense in its context.\nWith that in mind, PR 137 introduced addition CI jobs that build contracts against a custom no_std target. It is an x86-64 target for now, so I can play with its output locally, but eventually it will change to 64-bit RISC-V. While it is bare-metal-like and single-threaded, it does output a shared library in ELF format, which can be loaded with dlopen() and whose functions can be called directly. PR 136 and PR 142 introduced further tweaks to it.\nThe building process for ab-example-contract-ft in the workspace looks something like this (verbose, but completely standard):\ncargo rustc --crate-type cdylib -Z build-std=core \\ --package ab-example-contract-ft \\ --features ab-example-contract-ft/guest \\ --profile contract \\ --target x86_64-unknown-none-abundance.json Profile contract is custom and essentially enables LTO and strips symbols to produce a compact \u0026ldquo;production\u0026rdquo; output in the form of ab_example_contract_ft.contract file. -Z build-std=core will hopefully not be required at some point, similarly maybe --crate-type cdylib might be the default for the custom target somehow, so it doesn\u0026rsquo;t need to be specified explicitly.\nThe file still contains an annoying .comment section with Rust, LLVM and LLD versions, which can be stripped with:\nstrip -R .comment ab_example_contract_ft.contract I looked for ways to avoid it without this extra step, but didn\u0026rsquo;t find any.\nSo how big are the output files you might be wondering? Reasonably small actually, but ELF has relatively large 64-byte sections with a bunch of zeroes that increase the size a bit. For flipper example the size is ~1.8 kiB before manual stripping of .comment section and ~2,7 kiB for ft contract.\nThe good thing is that it compresses well with zstd, down to ~550 bytes for flipper and ~1100 bytes for ft contract (after .comment stripping), which is much closer to what I\u0026rsquo;d like to see. I think zstd compression is what will be used for contracts in practice since there is some repetition and a bunch of zeroes in the ELF file.\nThe huge benefit of this design is that it\u0026rsquo;d be possible to use all the standard tooling with these contracts. Regular Cargo/Rustc or even other compilers to produce an ELF file, no custom tooling is required (though it might be more convenient). I can also imagine a custom app that can load such a contract file plus encoded method call (dumped from block explorer, for example) to do step-by-step debugging in gdb/lldb. All standard disassembling and optimization tools should also work with these contracts.\nTransactions # I did some refactoring for transactions in preparation for a basic implementation of the transaction pool in PR 143, but ultimately didn\u0026rsquo;t have time to do anything significant there. I did spend a lot of time thinking about it and should have more progress in the coming weeks. A lot of open questions arise (like commitment schemes for things) since this is getting closer to the actual blockchain implementation (that we do not have), which is something I\u0026rsquo;ll be working on addressing in the coming weeks too.\nUpcoming plans # With basic execution environment, transactions and transaction pool, we\u0026rsquo;ll need blocks and a chain of blocks, meaning blockchain. The plan there right now is to use Subspace protocol reference implementation as the foundation initially (that I\u0026rsquo;m very familiar with). I\u0026rsquo;ll strip all the unnecessary bits there (everything related to domains, votes, probably most of the reward issuance logic, etc.) first to reduce the number of things to carry around. With that, we\u0026rsquo;ll not be using the Substrate framework either, so decoupling logic from Substrate and probably converting core consensus pieces into contracts will be the next logical step after that.\nThis will be a long-is process since Substrate provides networking stack for the blockchain, which will have to be replaced and networking stack used for DSN is not quite in the shape to replace that yet. With that, we\u0026rsquo;ll hopefully have a basic blockchain running a couple of system contracts one to two months later.\nOnce that is done, we\u0026rsquo;ll be able to compose instances of such blockchains into a sharded hierarchy and experimenting with how things fit together based on the research Alfonso is doing.\nThat is my small update for now, see you next time!\n","date":"31 March 2025","externalUrl":null,"permalink":"/blog/2025-03-31-building-contract-files/","section":"Blog","summary":"\u003cp\u003eThe majority of the last two weeks I\u0026rsquo;ve been busy with the installation of the antivirus system update for my immune\nsystem. It was neither pleasant nor quick, but now that it is slowly approaching 100%, I\u0026rsquo;m back with another update of\nwhat I managed to do since the last update.\u003c/p\u003e","title":"Building contract files","type":"blog"},{"content":"After a lot of thinking, this week I came to the realisation that a sharded architecture like the one we are trying to build can be designed leveraging the current design of the Subspace protocol and all its underlying mechanisms as a base. While this was the idea from the beginning, either for lack of familiarity with the protocol or plain ignorance, I was missing the big picture of how this could be done.\nUsing Subspace as the core protocol for our design has several advantages:\nWe can build upon the security and the robustness of the Subspace protocol. If we can assume the Subspace protocol as secure (which currently is the case), we can derive the security of our designs from the security analysis of Subspace. The team is already familiar with Subspace, this will help with its implementation and with reasoning about its correctness. We are building upon a consensus algorithm that is operating in production, and we can learn from all the improvements and mistakes made throughout its life (design and implementation-wise). Funnily, on our brainstorm sessions Nazar has already shared a few pointers to ideas that they already explored in the past and that are still relevant to us (which really helps when evaluating ideas for the design). High-level intuition for a multi-shard Subspace protocol # At this point you may be wondering already how does a multi-shard Subspace protocol would look like. I still don\u0026rsquo;t have the low-level details for the design, but at least I already have the really high-level intuition of how this could work.\nAs discussed in previous documents, ideally the protocol should be recursive and allow for infinite horizontal scaling by including deeper layers of shards The beacon chain (or main chain as referred to in some of the literature), is the main chain responsible for orchestrating the lifecycle and securing all the shards of the system. As such, all the population of farmers in the system participate from the system (see image below). By being part of the beacon chain, farmers are also implicitly proposing blocks from the underlying shards. High-level, the idea for the design is that the history buffer is populated with records belonging to the history of all shards in the system, and each of proof-of-time slot a new winning ticket will be drawn for each shard. The farmer encountering this winning ticket is responsible for sealing and broadcasting the newly proposed block for the shard they have the winning ticket for. Shards can be created in the beacon chain\u0026rsquo;s genesis, or later in the history of the chain (we will leave the specifics about this process to the future. New shards will be spin off through a network velvet forks [1]) The core idea for the system\u0026rsquo;s consensus algorithm is that consensus participants will be contributing to a global history buffer, that will be consequently archived and used for the block winner election in each shard. The beacon chain does not allow user transactions, it only accepts system transactions and block commitments from shards in the immediate layer below. Each shard has its own independent transaction pool with transactions initiated in that chain. Each Proof-of-Time (PoT) chain slot, the randomness beacon triggers a global farming process, where each farmer runs a protocol to extract a winning chunk. Ideally, there should be at least a winner for each shard. The way in which we determine if the winning ticket belongs to a specific shard is by checking to which shard the winning chunk belongs. When a farmer creates a block for a shard it broadcast the block to the corresponding shard and the header to the beacon chain. In this way, blocks in the beacon chain aggregate all the lower layer blocks that have been proposed so far. Blocks in the beacon chain are included in the history buffer, and as this blocks include the headers for the underlying shard blocks, the history buffer implicitly includes all shards blocks. The archiving is done over this global history, and when a new shard block is encountered in a genesis block, its content needs to be pulled to included in the archived history. Not all farmers need to keep the state of all shards (as it would deem the use of a sharded architecture useless). As such, every epoch (determined by a window of N slots) there is a random assignments for farmers to shards as \u0026quot; executors\u0026quot;. This assignment prevents potential collusion among farmers by keeping shard membership static. This epochs should be large enough to compensate for the \u0026ldquo;warm up\u0026rdquo; period between epoch changes where farmers may need to pull the latest state for their new shard if they don\u0026rsquo;t have it. From high-level ideas to low-level design # The high-level description shared above is great to gain an intuition of how a sharded version of the Subspace protocol could look like. Unfortunately, after a few brainstorm sessions with Nazar, we started to find holes and added complexity in how I imagined the protocol to work.\nHow can we ensure that the history buffer is populated with records belonging to the history of all shards in the system? What farmers are responsible for contributing blocks (or segments) from a shard to the history buffer if not all farmers have access to the full state of all of the shards? How are farmers assigned to specific shards and how can we prevent collusion among farmers? Which farmers are allowed to propose blocks in a specific shard? Every farmer independently of the shard? Only farmers assigned to that shard? How can we align incentives to prevent selfish farmers from just doing the least possible work to get the rewards? How can we balance the population of farmers among shards to avoid a big farmer trying to attack a shard and lead to power dilution? How should we recover from an attack in a shard? So you see that there are a lot of unanswered questions. With all of this in mind, we narrowed a bit more the design space coming up with the following ideas \u0026ndash;exploring these will be my focus on the coming week\u0026ndash;:\nInstead of having a global archiving protocol that requires every farmer to have knowledge about the state in every shard, there will be independent archiving in each shard. Shards will notify new segments to their parent chains and the beacon chain by submitting segment headers. New segments in shards are created with a local sequence ID. When the segments are committed to the beacon chain, they are assigned a global ID that sequences shard segments into a global history buffer (see figure below with a rough illustration of how this could work). Farmers are notified about new segments. If there are new pieces within their cache limit they try to go to the DHT and fetch a piece to include in their sector. Plots are tight to a specific shard. Farmers commit to farm in full branches of the hierarchical tree where they will be entitled to farm new blocks. In order to be able to do so, they\u0026rsquo;ll need to dedicate their storage to plotting on those shards (as it happens in single-chain Subspace). Along with keeping shard plots for all the shards in that branch of the tree, they will obviously also need to track the transaction pool for unverified transactions, and keep at least the most recent state for the shard. With this approach, farmers can self-assign themselves to shards, but they are required to perform some upfront work to be able to farm a block (preventing them from jumping from one shard to another with low effort). Even if they are committed to a specific branch of the tree, farmers can do light verification for other shards in the tree like validating the headers of the rest of the shards. We should introduce mechanisms, like a power threshold, used to identify when a shard is in a weak state to rebalance the population of farmers among shards. We can probably get some inspiration for this from Eigenlayer, where farmers can use their power to intervene in a shard and propose blocks to fix it. This same process should also be used to identify and recover from an attack in a shard. Finally, on top of all these mechanisms we can come up with a reward system that forces rational balancing of the farming population among shards. This will help us avoid collusion and a big farmer trying to attack a shard. Core subprotocols for the design # And I couldn\u0026rsquo;t close this weekly update without sharing a really interesting paper that Nazar brought to my attention throughout the week, and that ended up being extremely relevant to what we are doing: Scalable Multi-Chain Coordination via the Hierarchical Longest Chain Rule. This paper introduces BlockReduce, a PoW-based blockchain that achieves high-throughput by operating a hierarchy of merged mined parallel chains.\nThe paper presents a hierarchy of Nakamoto-based consensus chains like we have, and it introduces a lot of interesting concepts that reinforces or adds up to all of the ideas that we\u0026rsquo;ve been having in the past few weeks.\nThey introduce the concept of merge mining, where miners simultaneously mine multiple chains. This is similar to what we are trying to achieve with the Subspace protocol, where farmers will be able to farm multiple shards at the same time by choosing a branch of shards in the hierarchy. They propose the concept of coincident blocks, which are blocks that share the same PoW solution. This is a really interesting concept that we can use to have an implicit order of the different partitions (shards) through coincident blocks. They also propose the concept of cross-net transactions that need to be validated and can only be executed when the transactions are included in a coincident block. They use a burn-and-mint operation for cross-net transactions which really aligns with our idea of limiting cross-net transaction to the basic atomic operations. Even more, these transactions can only be executed when coincident blocks happen, as they are the ones that can attestate ordering among different shards. Finally, BlockReduce leverages a rational model, where each miner self-assigns itself to the hierarchical path that they consider more profitable for them (which is the model we are leaning towards after our latest brainstorms). What\u0026rsquo;s next? # We are starting to having a sense of how we can implement a hierarchical version of the Subspace protocol. The next step is to start breaking down the design into smaller pieces and start reasoning about their security and correctness. We need to identify what are the core subprotocols that would be needed to implement a hierarchical version of the Subspace protocol, which ones need to be adapted, and which ones can be reused unchanged.\nThis week my focus will be on trying to flesh out a first detailed spec for what I am calling the merged farming protocol, i.e. the construction of a global history buffer, a sharded archiving and plotting protocol, and sharded farming for shard block generation. On top of this, I\u0026rsquo;ll also explore alternatives for the clustering protocol, i.e. how are farmers assigned to specific shards (or partitions).\nSo with nothing else to add, see you next week!\n","date":"31 March 2025","externalUrl":null,"permalink":"/blog/2025-03-31-hierarchical-subspace-protocol/","section":"Blog","summary":"\u003cp\u003eAfter a lot of thinking, this week I came to the realisation that a sharded architecture like the one we are trying to\nbuild can be designed leveraging the current design of the Subspace protocol and all its underlying mechanisms as a\nbase. While this was the idea from the beginning, either for lack of familiarity with the protocol or plain ignorance, I\nwas missing the big picture of how this could be done.\u003c/p\u003e","title":"Hierarchical Subspace Protocol","type":"blog"},{"content":"Overall, I am really happy with the progress I\u0026rsquo;ve made this week. I\u0026rsquo;ve been mainly focused on unravelling one of the papers that I mentioned in my last update, Proof-of-Stake Sidechains. While is true that we don\u0026rsquo;t want our system to have anything to do with PoS, and there is no 1:1 matching of the concepts from the paper with what we are trying to build, the paper presents a framework that can come pretty handy to evaluate the correctness of our designs. This paper is from 2018, and after a first pass the first thing that I did is to check if there were any follow-up papers that built upon the abstractions of this paper. I came across Proof-of-Work Sidechains from the same authors, but without a doubt, the most complete proposal is the one that I started with. Let\u0026rsquo;s dive right into it.\nThinking in terms of sidechains. # What I liked the most about this work is that it presents a formal framework to reason about the interaction between different blockchain networks through the construction of sidechains. The paper focuses on PoS sidechains, but the concepts and the security model is general enough to be easily adaptable for other constructions like rollups, bridges, beacon chains and execution layers, or in our case, a hierarchical architecture of Nakamoto-based consensus networks.\nI would highly recommend everyone to read this paper if you are interested in the topic, but I will try to summarise the core primitives that I found more interesting and that I am planning to leverage (either for inspiration or to evaluate the correctness of our designs):\nA formalised security framework for sidechains This is whithout a doubt one of the most relevant contributions of the paper. They formalise the concept of sidechains, present formal concepts for many of the primitives that are used in the construction of sidechains, and present a security and adversarial models that can be used to reason about the security of the system. Merged staking v.s. independent staking: The paper presents two different models for the staking mechanism in sidechains. In the merged staking model, sidechains can leverage part of the staking power from the main chain to secure the network, i.e., the security of the sidechain is directly tied to the security of the main chain as some validators in the main chain also participate in the sidechain consensus. In the independent staking model, sidechains have their own staking mechanism, and their security is independent of the security of the main chain. The benefit of the merged staking model is that it helps preventing \u0026ldquo;goldfinger attacks\u0026rdquo; against sidechains with a small staking power. Direct observation v.s. certified-based observation: It also introduces two different models for the relationship between sidechain and the flow of verifiable information between them. In the direct observation model, the sidechain directly observes the main chain, i.e. it participates as a full node of the sidechain and stores and verifies every state update in that network; while in the certified-based observation model, participants of other chains do not store all the state of a sidechain, and they rely on succint proofs (i.e. certificates) to verify the state of the a sidechain. In the paper, they present what they call an ad-hoc threshold multisignatures (ATMS) construction to enable the certified-based observation model. Firewall property: The paper presents a formal definition of the firewall requirement that a sidechain must satisfy to ensure that the security of the main chain is not compromised by the sidechain. This property ensures that if a sidechain is compromised, the impact the attack can have over the main chain or other sidechains in the system is limited by the inflow of assets. The firewall property allows relying on an arbitrary definition of exactly how assets can correctly be moved back and forth between the two chains, we capture this by a so-called validity language. In case of failure, the firewall ensures that transfers from the sidechain into the main chain are rejected unless there exists a (not necessarily unique) plausible history of events on the sidechain that could, in case the sidechain was still secure, cause the particular transfers to take place. Merge operator: It also presents abstractly the use of a merge operator that allow to sequentialise a set of transactions of two independent chains. merge allows us to create a combined view of multiple ledgers, putting all of the transactions across multiple ledgers into a linear ordering. Finally, they show how their sidechain construction framework (i) supports safe cross-chain value transfers when the security assumptions of both chains are satisfied, namely that a majority of honest stake exists in both chains, and ( ii) in case of a one-sided failure, maintains the firewall property, thus containing the damage to the chains whose security conditions have been violated. What does all of this involve for design of our system? # Inspired by the concepts from above, and after doing another thorough review into Subspace\u0026rsquo;s consensus protocol (once again, thank you Nazar for clarifying the rationale behind the design of some of the parts of the protocol) I started to think about the different primitives that our design should have. These are still high-level ideas, but I am already working to flesh out the design.\nIf you recall from my previous update, our system will use an architecture composed by different layers of independent but interconnected chains (or shards), with the first layer being the global consensus (or main chain) responsible for orchestrating all the lower layers. We will adopt for our design an approach similar to the one presented for merged staking. All the farmers (we may also use consensus participants to refer to them more abstractly) dedicate their power to secure the global consensus, and they are also randomly assigned to participant and secure the consensus of the lower layers (by proposing and validating lower layer blocks). How I am planning to design this is that when a farmer audits its space to see if any of their chunks has a winning ticket to propose a block in the global consensus, they also check if they have a winning ticket to propose a block in any of the lower layers. To synchronise the different layers, I am planning to use a similar concept to that of epochs and slots. An epoch is a fixed period of time that is divided into slots. Each slot is assigned to a farmer that is responsible for proposing a block in the global consensus, and also for proposing a block in the lower layers. The global consensus will be responsible for orchestrating the assignment of slots to farmers in the lower layers, and for ensuring that the blocks proposed by the farmers in the lower layers are correctly included in the global consensus. Subspace already introduces an analogous approach through its proof-of-time that we may be able to leverage. We want Proof-of-Archival-Storage (PoAS) to apply to all the layers in the architecture, so that we can also use Proof-of-Space (PoSpace) for Sybil resistance. This is a key property that we want to maintain in our design, and one of the key things to figure out is how the protocol should work so that the history of all shards is archived and efficiently load balanced throughout the network to ensure their permanence and availability. In terms of chain observability, I expect that the lower layers will operate in a direct observation model, where they directly observe the global consensus, and they store and verify every state update in that network. We have an advantage here though, by using PoAS we make a the history of the shards available to all farmers, so that they can easily verify the state of the lower layers off-band if needed. Which brings me to the firewall property. In the paper when a sidechain fails, it is not recovering after that. The firewall property limits the impact of the attack, but there is no mechanism to make the sidechain operational again. In our case, the fact that we are archiving the history of all shards, and that we rely on a probabilistic consensus for their operation, will allow us to introduce a mechanism to recover from a failure of a shard. Thus, we want adopt the firewall property as described in the paper for our design and will try to come up with a stricter recovery property and mechanism. On top of all of this (and this is not something that I am currently considering as a high-priority) I am thinking about introducing the concept of a merge operator that allow to easily interleave the histories (or a subset of the history) between different shards. This operator will potentially be tightly coupled with the execution model that Nazar is working on, but I think that the modular approach that he is following can really help us to come up with a design that is flexible and that can be easily adapted to the different needs of the applications that will run on top of our system. Finally, the paper introduces the concepts of 2-way-pegs, cross-chain transactions, and the ability to perform atomic exchange of assets between chains. In our case, we are planning to limit the cross-shard operations to basic atomic primitives that allow to burn and mint tokens. This basic operation can be combined to construct more complex cross-chain operations by application developers. These operations will also be tightly coupled with the smart contract execution model (and already have many ideas on how to implement them), but I expect this to allow complex operation like the atomic execution of transactions involving states from two shards. In the scope of this, I had a few interesting discussions with Nazar about how addressing will work in our system, and how transactions would be queued for validations and routed throughout the hierarchy. As a side-note, I am leaving block rewards and the incentive model out of scope for this discussion for now, but I just want to let you know that we\u0026rsquo;ve also started thinking a little bit about this. What\u0026rsquo;s next? # First of all let me thank you for reading this far. This update ended up becoming a little bit longer than usual, but I wanted to share all the disconnected notes that I\u0026rsquo;ve been taking throughout the week. With all of these high-level ideas in mind, this week I am planning to focus on designing in detail the proposal of blocks in the main chain and lower level shards, and the archival of storage of the history of the shards. I think that with this we can start thinking about the implementation of the core protocol so we can surface issues or other things that need further design, while we work in parallel in detailing the rest of the system mechanics. Until next week!\n","date":"24 March 2025","externalUrl":null,"permalink":"/blog/2025-03-24-thinking-formally-in-terms-of-sidechains/","section":"Blog","summary":"\u003cp\u003eOverall, I am really happy with the progress I\u0026rsquo;ve made this week. I\u0026rsquo;ve been mainly focused on unravelling one of the\npapers that I mentioned in my last update, \u003ca\n  href=\"https://eprint.iacr.org/2018/1239.pdf\"\n    target=\"_blank\"\n  \u003eProof-of-Stake Sidechains\u003c/a\u003e. While is\ntrue that we don\u0026rsquo;t want our system to have anything to do with PoS, and there is no 1:1 matching of the concepts from\nthe paper with what we are trying to build, the paper presents a framework that can come pretty handy to evaluate the\ncorrectness of our designs. This paper is from 2018, and after a first pass the first thing that I did is to check if\nthere were any follow-up papers that built upon the abstractions of this paper. I came\nacross \u003ca\n  href=\"https://eprint.iacr.org/2018/1048.pdf\"\n    target=\"_blank\"\n  \u003eProof-of-Work Sidechains\u003c/a\u003e from the same authors, but without a doubt, the\nmost complete proposal is the one that I started with. Let\u0026rsquo;s dive right into it.\u003c/p\u003e","title":"Thinking formally in terms of sidechains","type":"blog"},{"content":"The plan was to get to transaction pool implementation, but it didn\u0026rsquo;t quite happen. I did a lot of investigation around performance though. For example, transaction processing was several orders of magnitude slower than direct method calls without a transaction, which concerned me, but after optimizations of last week the difference is ~10x. And it makes sense given how much more work the wallet has to do on top of the method call itself.\nSo last week we ended up with ~300 k transaction per second processing rate, while direct method calls were at ~13 M/s and transaction emulation (bypassing the wallet implementation) was at ~10.8 M/s. But why?\nThis turned out to be a combination of things, but the most impactful change was, funnily enough, probably a single stack-allocated variable that had a size 128x larger than expected. It was documented to be using byte units, but was actually expressing number of u128s. Decreasing it 128 times helped a lot. There was another data structure that was also 32 kiB in size, while needing less than one, all that and some more was corrected in PR 120.\nThat PR also introduced a constant MAX_TOTAL_METHOD_ARGS to define the maximum number of arguments the method can have. It was not constrained previously, which didn\u0026rsquo;t allow making assumptions about the max size of data structures and required heap allocations in executor. It is limited to 8, which is the number of bits in a byte and led to much anticipated rewrite of transaction payload encoding.\nThe encoding worked but was a bit less compact than I\u0026rsquo;d like it to be and didn\u0026rsquo;t support using outputs of previous method calls in the slots of the next one. Now that we know that there couldn\u0026rsquo;t be more than 8 arguments in a method, it is possible to use very compact bit flags to indicate what kind of value slot or input will use.\nPR 121 implemented the changes and fixed a few bugs. Specifically, it is now possible to create a transaction that in pseudocode looks like this:\nwallet_address = Code::deploy(wallet_code) Wallet::initialize(wallet_address, public_key) Token::send_to(wallet_address, amount) Note that in token transfer wallet_address might be a slot, and that is exactly what wasn\u0026rsquo;t supported previously. While only a simple sequence of method calls is supported by this reference transaction payload encoding, it already allows creating interesting workflows. For example, in Ethereum you often use approval in DeFi applications to allow spending your tokens by a contract. Here it would be possible to allow spending specific number of tokens just for the duration of the transaction itself:\nToken::allow_transfer_once_by(Context::Reset, contract_addres, amount) Defi::do_something(Context::Reset) Context wasn\u0026rsquo;t mentioned in examples before for simplicity, but it essentially is a notion similar to a \u0026ldquo;user\u0026rdquo; in operating systems, defining what things can be accessed. It is possible to inherit the context from a caller, override it with itself or reset to Address::NULL. Resetting means you don\u0026rsquo;t need to worry much about whatever contract you\u0026rsquo;re calling. They can\u0026rsquo;t do anything on behalf of the wallet due to context being Address::NULL already. However, by doing temporary approval, Defi contract gains an ability to do something with Token, but only within the limits of explicit approval. This is a much nicer security model that is easier to reason about IMO. And since all inputs and outputs are explicit, there is no danger of contracts messing up with wallet\u0026rsquo;s state by accident.\nSomething that Token::allow_transfer_once_by() above would use is #[tmp] argument for ephemeral storage (for duration of the transaction only), but implementation-wise it is abusing Address::NULL slots, which unfortunately were not cleaned up properly before. PR 115 finally extracted Slots data structure along with underlying aligned buffers into a separate crate, which will be needed for transaction pool implementation and PR 119 implemented cleanup for #[tmp] \u0026ldquo;slots\u0026rdquo;.\nPerformance # With transactions being much faster, I was exploring other kinds of overhead. This time the focus was on compactness and avoiding heap allocations, with PR 114, PR 124 and PR 125 performance looks roughly like this:\nflipper/direct time: [48.326 ns 48.469 ns 48.638 ns] thrpt: [20.560 Melem/s 20.632 Melem/s 20.693 Melem/s] flipper/transaction time: [57.682 ns 57.933 ns 58.219 ns] thrpt: [17.177 Melem/s 17.261 Melem/s 17.336 Melem/s] example-wallet/execute-only time: [535.60 ns 537.22 ns 539.44 ns] thrpt: [1.8538 Melem/s 1.8614 Melem/s 1.8671 Melem/s] A much smaller gap between direct method call and transaction emulation with a massive difference from last week. But more impressive is increase from ~300 k/s to 1.8 M/s for transactions that can be processed through the whole things, including the wallet. It is still bottlenecked by the signature verification cost, of course.\nThese can and will improve further. For example, metadata decoding is currently happening on every method call, but it can be cached to remove a significant amount of compute overhead from all benchmarks above.\nNo panic # There is an interesting crate in Rust ecosystem called no-panic. What it does is prevent code from compiling unless the compiler can be convinced that annotated method can\u0026rsquo;t possibly panic. This is a very nice property for assuring reliability and predictable code behavior. It is a rock-solid compiler guarantee that the API is what it appears to be. While an error handling story in Rust is already great, the fact that certain things can potentially panic is still really annoying.\nThe way it works is basically inserting a wrapper around user code that instantiates a struct that calls non-existing function in Drop implementation (that would cause linking error) before user code and removing instance after to prevent from Drop from actually running. The only way compiler would not eliminate that instance and its Drop it as dead code is if the user code can panic, in which case the Drop::drop() would need to be called during unwinding. Brilliant!\nIt is tricky to use, but with some effort can be applied to non-const (for now) methods. PR 109 implemented support for panic-free payload decoding in ab-system-contract-wallet-base and PR 118 extended this to a large portion of ab-contracts-common API. Really looking forward to trait support in const fn in Rust to be able to apply this to many const fn functions and methods in the codebase. I\u0026rsquo;ll also extend annotations to more crates over time.\nDocumentation # I need to work some more on documentation, and last week I added Transaction overview page to the book with some more details now that things have settled a bit.\nELF # I\u0026rsquo;ve spent some time researching on what format contracts should be packaged in. I thought about standard ELF rather than something custom for a long time, but didn\u0026rsquo;t know the specifics. It\u0026rsquo;d be great to use cargo build\u0026rsquo;s output, even if with custom configuration, without any manual post-processing. I\u0026rsquo;m still not 100% confident that it\u0026rsquo;ll be possible, but so far it doesn\u0026rsquo;t seem impossible either.\nFor a bit of context, what I\u0026rsquo;m ideally looking for is a 64-bit RISC-V ELF file that can be both uploaded to the blockchain as a contract and used as a \u0026ldquo;shared library\u0026rdquo; of sorts directly. By directly I mean with dlopen() or loading directly into gdb(), which will be an excellent debugging experience for developers and will eliminate the need to have custom compilers and debuggers, using industry standard tooling.\nIf it works out like I expect, it\u0026rsquo;d be possible to describe a method call in a format close to the pseudocode I have shown toward the beginning of this post and call it on a contract in gdb/lldb. Even IDE integration should work with precompiled contracts that way without the need to write special IDE-specific plugins, etc. It\u0026rsquo;ll also hopefully open the door for hardware acceleration on RISC-V hardware by running contracts in a VM, but I\u0026rsquo;m probably speculating too much at this point.\nI might start with something simpler and try to upload native x86-64 binaries in the format similar to the eventual RISC-V binaries, so I can get a feel of it and open them with dlopen() for now.\nI looked into VMs too, with PolkaVM looking interesting, but requiring PolkaVM-specific sections in ELF, which I\u0026rsquo;d really prefer to avoid. Embive looked interesting too, but has different design goals, and I\u0026rsquo;m not so sure about its performance. The impression right now is that either some PolkaVM changes would be needed to integrate it or a separate interpreter/VM will need to be designed instead.\nUpcoming plans # I\u0026rsquo;d like to get to transaction pool some time this week and to write more docs. There are also some developer interviews scheduled for later this week, which I hope will provide useful insights.\nI hope you don\u0026rsquo;t regret spending time to read this too much, see you in about a week.\n","date":"17 March 2025","externalUrl":null,"permalink":"/blog/2025-03-17-way-faster-transactions-and-no-panic/","section":"Blog","summary":"\u003cp\u003eThe plan was to get to transaction pool implementation, but it didn\u0026rsquo;t quite happen. I did a lot of investigation around\nperformance though. For example, transaction processing was several orders of magnitude slower than direct method calls\nwithout a transaction, which concerned me, but after optimizations of last week the difference is ~10x. And it makes\nsense given how much more work the wallet has to do on top of the method call itself.\u003c/p\u003e","title":"Way faster transactions and no-panic","type":"blog"},{"content":"I want to kick-off my first weekly update in the project thanking Nazar for the warm welcome and the opportunity to work with him on this exciting project. I was really pumped to see other teams actively working on a similar problem to the one I started researching more than three years ago. For several reasons, I wasn\u0026rsquo;t actively contributing to this problem any more, but this opportunity was the perfect excuse to get back to the game.\nIf you are curious about my previous work on the matter before joining Nazar, feel free to skim through this paper to get the gist of it.\nBackground # First things first, what exactly is this problem that I am referring to? At least the description of the problem is simple, \u0026ldquo;we want to design blockchain infrastructure that can scale to the size of the Internet\u0026rdquo;. The system should be able to host applications ranging from high-throughput media-intensive social networks and virtual worlds; to those that require more strict trust requirements and security guarantees, likeweb3-native and financial applications. Unfortunately, the implementation of a system like this is very challenging. Current blockchain designs are still extremely monolithic and require the execution of transactions to be performed by (almost) every node in the system, and for the state to also be replicated in each (or a great number) of them. All the innovations around L2s, rollups, and some next-gen blockchains are improving this, but no one is close to achieving a system that is able to operate at the scale of the Internet in a seamless (and if I may add, UX-friendly) way.\nThe architecture of the Internet # The best way to design a distributed system that is Internet-scale and that supports the workloads currently being executed in it is to build it from first principles, go to the source, and derive the architecture of our candidate system directly from the Internet.\nIf we look at how the Internet is structured today, we see the following layered architecture:\nA network of data centers holding the global state for all applications and the computational resources required to run applications. The interconnection of different Autonomous Systems that enables the exchange of information between different subnetworks through a routing system, logically merging all of the state and resources in the system. Local networks with a large number of devices that hang from an AS, and depend on it to interact with the rest of the network. A hierarchical DNS System that provides naming resolution through a distributed hierarchy, offering lessons for decentralised naming and discovery. How can we build a consensus algorithm that resembles this hierarchical architecture of the Internet, where there are different subnetworks that are globally orchestrated to operate as a common system? This has been the main focus for me this week, to think about how the high-level architecture of this consensus algorithm would look like considering the \u0026ldquo;wish list\u0026rdquo; set by Nazar here.\nA layered consensus # The first thing that we should acknowledge for our candidate designs is that there is a single \u0026ldquo;one-size-fits-all\u0026rdquo; consensus algorithm able to support any kind of application, but we want to come up with a design that offers the basic infrastructure that can be configured to make this possible. Depending on the application, the throughput and security guarantees are different. However, using a sound core consensus that can be tweaked and integrated as part of a bigger global consensus with the right primitives may be able to work around some of these trade-offs and achieve the desired result.\nWith this in mind, this is the high-level architecture that I\u0026rsquo;ve been tinkering with to guide my design:\nLayer 3: Local Area Consensus # Application-specific consensus run in local area subnetworks (LANs). They form dynamic clusters based on network proximity and transaction patterns Ideally, they run a consensus algorithm that allow for sub-second finality for local transactions. This consensus algorithm should be light and allow applications to configure its parameters for their needs. Provide strong local consistency with BFT guarantees (3f+1) Operate independently during network partitions Membership should be Subscription-based. They use a topic-based-like membership, where nodes that want to participate in a local are subnetwork can subscribe and unsubscribe dynamically (the subnetwork will be active as long as there are members). Proof-of-Archival should still be applied in local area networks to ensure that the history of the LAN is stored, and so we can leverage PoS for sybil resistance. The way in which I am considering this to be implemented is that LANs create fast microblocks that are broadcast to members subscribed to the LAN and the WAN (or WANs) that the LAN hangs from. Layer 2: Wide Area Consensus # Aggregates local clusters into wide-area regions Aggregate microblocks from underlying LANs and checkpoint their state through macroblocks (combining microblocks and WAN transactions). Handles cross-cluster transactions within a region Runs a Nakamoto-based consensus based on Subspace\u0026rsquo;s consensus basic primitives (i.e. employs erasure coding for data availability and Proof-of-Archival). All WANs are equal in terms of capabilities, and farmers are randomly assigned to more than one WAN (depending on the farmer population). The target block times of WANs should be similar (or better) to the ones that Autonomys currently have. WAN (as is the case for LANs) have full blockchain functionalities (transactions, smart contract executions, etc.). Layer 1: Global Consensus # Offers the higher level of security and data availability. The whole population of farmers in the system have to participate from the global consensus, as is the one orchestrating all the WANs (and implicitly LANs). There is no support for smart contract execution in the global network, and it serves exclusively as an anchor of trust, and a system-wide consensus network (with basic system functionality, like a global name system, account management, WAN farmer membership, synchrony, etc.). The global consensus run a global network analogous to the role that the beacon chain currently has in Ethereum. Provides probabilistic finality that strengthens over time. What\u0026rsquo;s next? # Obviously, many projects, from Polkadot to Optimism or Cosmos, have already realised that the best wayy to scale blockchains to support different kinds of applications is to deploy a set of subnetworks that are able to operate as a whole. So the architecture I described above doesn\u0026rsquo;t add much innovation in itself. However, I think that the key to achieve global scale is on the underlying mechanics. Using the system should be as seamless as using the Internet today.\nFor the first few strokes I will focus on designing Layers 1 and 2. This next week I want to focus on coming up with the membership protocol responsible for assigning, and the lifecycle of a transaction in the hierarchy i.e. how blocks are created, validated, and executed in the different layers, and the mechanism used to store and interleave the history of all the networks in the system.\nTo help me with this I am going to review again these papers for inspiration:\nBicomp: A Bilayer Scalable Nakamoto Consensus Protocol Close Latency-Security Trade-off for the Nakamoto Consensus Nakamoto consensus with VDFs Phantom GHOSTDAG Proof-of-Stake Sidechains (I really loved this one the first time I read it!). Narwhal and Tusk: A DAG-based mempool and efficient BFT Consensus And that concludes my first project update. I\u0026rsquo;ll try to get you a few juicy updates next week. Until then!\n","date":"16 March 2025","externalUrl":null,"permalink":"/blog/2025-03-16-drawing-inspiration-from-the-internets-architecture-to-scale-consensus/","section":"Blog","summary":"\u003cp\u003eI want to kick-off my first weekly update in the project thanking Nazar for the warm welcome and the opportunity to work with him on this exciting project. I was really pumped to see other teams actively working on a similar problem to the one I started researching more than three years ago. For several reasons, I wasn\u0026rsquo;t actively contributing to this problem any more, but this opportunity was the perfect excuse to get back to the game.\u003c/p\u003e","title":"Drawing inspiration from the Internet's architecture to scale consensus","type":"blog"},{"content":"The big change from the last update is that Alfonso de la Rocha has joined me as a part-time researcher to help with sharding designing. Code-wise, there were also a bunch of performance benchmarks and optimizations.\nAlfonso has worked extensively with blockchain-related tech and research, most recently at Protocol Labs. At PL he worked on Interplanetary Consensus (IPC) and a bunch of other things, that are all one way or another relevant to high-performance blockchains. Currently, he is learning how Subspace consensus works and prepares a framework for reasoning about sharding design options. Starting next week, there should be a section with updates from him too, so stay tuned.\nI\u0026rsquo;m very excited and I know he is too!\nTransactions # Last time I mentioned that a notion of transactions was added, but not yet integrated. I\u0026rsquo;m happy to report that it finally happened in PR 39!\nInstead of methods to manipulate environment directly, there are now dedicated methods for verification and execution of transactions using TxHandler interface that a wallet contract is supposed to implement. Crafting transactions for testing purposes might be tedious, so there is a transaction emulation API now for that purpose. NativeExecutor::env_ro() method was retained to be used for calling stateless methods, like when processing RPC requests, etc.\nExecutor creation was converted into a builder and storage container was extracted out of the executor instance, in the future it will have persistence APIs such that state can be persisted on disk and read back later. Just like you\u0026rsquo;d expect a normal blockchain node to do it, but we\u0026rsquo;re not quite there yet.\nThe same PR also introduced some test utilities in a separate crate, for example, a DummyWallet.\nPR 94, PR 100 and PR 101 updated transaction structure in preparation for work on the transaction pool, but no work on that has started yet, just some preliminary research.\nPerformance # In PR 92 I introduced benchmarking for example wallet implementation, discovering that signature verification massively dominates transaction processing time at ~14 Âµs ðŸ¤”. Not yet sure what to do with that, feels expensive, and it already takes advantage of AVX512 instructions on my Zen 4 CPU. Will have to think about that some more.\nEquipped with benchmarks, I spent a lot of time thinking and experimenting and came to the conclusion that while making multi-calls to leverage CPU concurrency is a sound idea in general, it needs to go. The reason is that instantiation cost is non-negligible and with a VM it will become even larger. At the same time, the amount of compute that can be done in a transaction in general isn\u0026rsquo;t so large that splitting it into multiple threads would be critical.\nNot having multi-calls simplified the design substantially, which after many trials and errors and long hours finally resulted in PR 103 and follow-ups PR 104 + PR 106.\nAlso learned something a bit surprising about Self and subtyping in Rust.\nThe latest numbers for execution environment for direct calls and transaction emulation overhead look like this:\nflipper/direct time: [76.389 ns 76.696 ns 77.047 ns] thrpt: [12.979 Melem/s 13.038 Melem/s 13.091 Melem/s] flipper/transaction time: [91.724 ns 91.936 ns 92.188 ns] thrpt: [10.847 Melem/s 10.877 Melem/s 10.902 Melem/s] 13 Million flips per second is A LOT more than 5 that I mentioned in previous updates, this is how much faster it became, very pleased with the results so far.\nAs for going through the while transaction processing pipeline for verification and execution of a well-formed transaction, the numbers look like this:\nexample-wallet/verify-only time: [18.268 Âµs 18.304 Âµs 18.345 Âµs] thrpt: [54.512 Kelem/s 54.634 Kelem/s 54.742 Kelem/s] example-wallet/execute-only time: [3.1408 Âµs 3.1470 Âµs 3.1545 Âµs] thrpt: [317.01 Kelem/s 317.76 Kelem/s 318.39 Kelem/s] Yeah, 54k transactions can be verified per second, while over 300k simple transactions can be executed per second. And this is all on a SINGLE CPU core. Even accounting for VM overhead, the ceiling is very high. The challenge will be to feed all these transactions at this rate (there is networking, disk access, lots of things that may slow this down).\nOther improvements # To increase the robustness of the features, PR 90 extended GitHub Actions workflows with more cases enabling various features and making sure they actually work properly.\nSince the number of crates increased, PR 98 rearranged crates into more subdirectories to help with navigation and ease discoverability of example and system contracts. Shamil will be pleased, I\u0026rsquo;m sure ðŸ˜‰. Some other of his feedback was regarding complexity and confusion regarding #[output] vs #[result]. After thinking about it and trying a few things, PR 83 and then PR 84 removed #[result], unifying the code in the process, which I hope will be not too convoluted and a bit easier to maintain. Also from him was a suggestion to support user-defined errors, which PR 82 implemented as well (with some further API improvements possible too).\nAnd a small but nice addition, PR 102 started copying original documentation (and linter attributes) from contract methods to generated extension trait methods.\nUpcoming plans # This has been a productive week, and I\u0026rsquo;m sure the next one will be too. I\u0026rsquo;m looking forward to transaction pool implementation, but not sure if I have sufficient infrastructure for that yet or this is the right time.\nI\u0026rsquo;ll be writing more documentation in the book about transactions and how they are processed now that it has solidified a bit.\nAnd I\u0026rsquo;d really like to have one or two developer interviews this week if possible to collect more feedback.\nAs usual, thank you for reading and until next time!\n","date":"9 March 2025","externalUrl":null,"permalink":"/blog/2025-03-09-there-is-two-of-us-now/","section":"Blog","summary":"\u003cp\u003eThe big change from the last update is that \u003ca\n  href=\"https://www.linkedin.com/in/adlrocha/\"\n    target=\"_blank\"\n  \u003eAlfonso de la Rocha\u003c/a\u003e has joined me\nas a part-time researcher to help with sharding designing. Code-wise, there were also a bunch of performance benchmarks\nand optimizations.\u003c/p\u003e","title":"There is two of us now","type":"blog"},{"content":"The most important progress from last week is initial work on transactions. I\u0026rsquo;ve spent quite some time thinking about the design and even implemented an initial wallet contract alongside with related infrastructure.\nAs mentioned at the end of last week, adding a notion of transactions and explicit slots were the next steps and the first part of that is now implemented in PR 79, but first a bit of context.\nAs mentioned in the book, \u0026ldquo;Everything is a contract\u0026rdquo; is the design philosophy for many things, and wallets are not an exception here. This basically means that there is no \u0026ldquo;system-wide\u0026rdquo; notion way of a signature scheme to use for transactions or even a way to serialize method calls into transaction payload. The wallet is just a contract that must conform to some fairly generic interface. This interface should be flexible for all kinds of wallets: from simple ones with a public key and nonce that checks a signature and supports simple transactions to complex multisig wallets, 2FA support, support for whitelisting/blacklisting transactions depending on signer and a lot of other things I probably can\u0026rsquo;t think of right now. At the same time, contracts should compile to compact RISC-V binary and not require heap allocation in most cases, ideally taking advantage of zero-copy mechanisms whenever possible.\nAs a result, I came with a trait that looks something like this (a bit simplified for this article):\npub struct TransactionHeader { pub genesis_hash: Blake3Hash, pub block_hash: Blake3Hash, pub gas_limit: Gas, pub contract: Address, } pub type TxHandlerPayload = [u128]; pub type TxHandlerSeal = [u8]; pub trait TxHandler { /// Verify a transaction #[view] fn authorize( env: \u0026amp;Env, header: \u0026amp;TransactionHeader, payload: \u0026amp;TxHandlerPayload, seal: \u0026amp;TxHandlerSeal, ) -\u0026gt; Result\u0026lt;(), ContractError\u0026gt;; /// Execute previously verified transaction #[update] fn execute( env: \u0026amp;mut Env\u0026lt;\u0026#39;_\u0026gt;, header: \u0026amp;TransactionHeader, payload: \u0026amp;TxHandlerPayload, seal: \u0026amp;TxHandlerSeal, ) -\u0026gt; Result\u0026lt;(), ContractError\u0026gt;; } TxHandler::authorize() takes transaction header, payload and seal and must make a decision whether to authorize transaction or not. Authorization implies that the cost of gas limit will be charged before calling TxHandler::execute() and the remainder will be returned after it.\nEssentially, what we have is an interface that a node (transaction pool and execution environment) will be aware of to statelessly verify the transaction for validity and stateful way to actually execute it. The contents of a transaction payload is opaque to the execution environment (but has to be aligned to 16 bytes) just like seal is.\nThe payload canonically contains serialized method calls. Each wallet is allowed to implement it, whichever way it wants, but there are some utilities in ab-system-contract-simple-wallet-base crate/contract that provide a reference implementation of what it might look like. Specifically, that crate supports sequences of transactions and the ability to reference outputs of previous transactions in transactions that follow, which is important for contract deployment, for example.\nThe payload is aligned to the maximum alignment supported by TrivialType that is used for I/O between host and guest with inputs, this way reference serialization/deserialization of method calls ensures all data structures are correctly aligned in memory. Aligned data structures mean they don\u0026rsquo;t need to be copied, the same bytes that were received from the networking stack could be passed around as pointers and sliced into smaller data structures without allocating any more memory dynamically.\nThe seal canonically contains something that authorizes the transaction, like a cryptographic signature and nonce to prevent transaction replaying. ab-system-contract-simple-wallet-base literally has those, but one can have more than one signature, some kind of one-time token instead of nonce and all kinds of other things imaginable.\nAuthorization here is a custom code and its execution is not guaranteed to be paid for, so how is it handled?\nWell, with transaction signatures being a notion of the blockchain node, the same issue exists. So the answer here is \u0026ldquo;it depends,\u0026rdquo; specifically node should be able to configure its own limit, but once the transaction is in the block, inability to pay for it will make block invalid. It is expected that initially some low-ish limit will be set that is enough to verify afew signatures, but it may be increased over time, including by node operator. This provides the ultimate flexibility for contract developers while reducing the complexity of the node implementation.\nab-system-contract-simple-wallet-base is also deployed as a system contract, containing the foundational logic, while I also added ab-contract-example-wallet that demonstrates how to take advantage of it to have a compact and efficient wallet contract.\nHardware wallets # I\u0026rsquo;d like to dedicate a whole separate section for hardware wallets, especially in the context of recent ByBit hack.\nOne of the first things in the design of contracts was the question of how to efficiently represent data structures in serialized form. On the one hand, it is desirable to be able to pass data structures in zero-copy manner as much as possible; on the other hand, there should be a way to make sense of them. This is why I initially reached to zerocopy crate, which had tooling for this, but didn\u0026rsquo;t have metadata generation utilities like SCALE has. I also looked at musli-zerocopy, which was another promising candidate, but required a git awkward wrappers and still didn\u0026rsquo;t solve the metadata generation/parsing issue.\nIn the end, TrivialType trait was born (implemented for types that can be treated as a bunch of bytes like u8, [u32; 4], etc.) and IoType that is implemented for TrivialType and a few custom data structures. TrivialType can be derived, and derived trait will contain const METADATA. This metadata can describe all kinds of data structure shapes that can be passed between host and guest environment as \u0026ldquo;bytes,\u0026rdquo; meaning no serialization code is necessary, just a pointer to existing memory.\n#[contract] macro also implements metadata, but this time for the all methods, which includes data structures involved too. As the result, all of this information is put into ELF section to be uploaded to the blockchain together with the code and can be read by various tools.\nThere is a bunch of places that read the metadata to make sense of the data structures various methods expect. Execution environment uses it to decode data structure received from one contract and to generate another data structure when calling another. Similarly ab-system-contract-simple-wallet-base uses it to serialize/deserialize method calls to/from payload bytes.\nGoing back to the hardware wallets and blind signing that lead to the hack, it would be possible to actually both display and verify in somewhat human-readable format the contents of every transaction right on the hardware wallet itself. Especially with wallets like Ledger Stax, there is plenty of space to do so.\nThis is how it can be done:\neach transaction header contains both genesis hash and block hash for which transaction is signed genesis hash is enough to know which blockchain transaction is signed for from block hash it is possible to generate proofs about metadata of all the contracts involved in a particular transaction since hardware wallet can confirm what contract it is signing transaction for, it can also decode, display and verify its contents, for example, with utilities provided by ab-system-contract-simple-wallet-base As a result, there is no blind signing, no need to trust the UI or machine that the wallet is connected to.\nFor now the wallet would have to know which kind of wallet contract is used or else it\u0026rsquo;ll not know how to deserialize opaque payload (which is a price to pay for utmost flexibility of transaction format).\nUpcoming plans # There was a lot of preparation work and lower-level API changes done that led to the transaction interface, but I will not bother readers with it this time because the blog post is fairly large as is.\nAs mentioned in the previous update, the next big step will be to integrate this into execution environment. And there are many conveniences to add and paper-cuts to eliminate here and there, after which I\u0026rsquo;ll be looking to do more developer interviews.\nSpeaking about interviews, I had a technical interview with one of the candidates last week and hoping to have good news to share next time.\nThis was one long blog post, if you made it till the end, thank you and see you next in about one more week!\n","date":"2 March 2025","externalUrl":null,"permalink":"/blog/2025-03-02-transactions/","section":"Blog","summary":"\u003cp\u003eThe most important progress from last week is initial work on transactions. I\u0026rsquo;ve spent quite some time thinking about\nthe design and even implemented an initial wallet contract alongside with related infrastructure.\u003c/p\u003e","title":"Transactions","type":"blog"},{"content":"It was a challenging week working on storage access checks for slots, but it is over, and I\u0026rsquo;m quite happy with how things are looking right now. Some extra refactoring also allowed running tests under Miri and spotted some things that violate the Rust safety rules.\nThe work from the previous week continued on reworking the way slots are managed by native execution environment to correctly handle recursive method calls and potential access violations. It finally concluded in PR 61 with some follow-up fixes in later PRs.\nThere were several challenges with it that step from the desire to achieve high performance, while retaining efficiency and maintainability. In the end, the following rules were established: a single recursive call can modify storage, but multiple calls dispatched at once (meant to be parallel, but aren\u0026rsquo;t right now) have only read-only view. This should fit the expected use cases nicely and help to constrain code complexity. There are a few paragraphs that explain goals and results in more detail in PR 61 if you\u0026rsquo;re interested to learn more.\nI\u0026rsquo;ve been thinking about address formats some more and decided that for a global system 44 bits for addresses is really not enough, and it should be way more than that. The addresses were also stored as [u8; 8] instead of u64 to reduce alignment requirements for data structures that might contain them, so the question became what should bigger address look like and how much bigger should it really be. I then looked at RISC-V (planned to be used for a VM) assembly for different operations on byte arrays. Comparing two addresses is the most common operation here, and turned out that byte arrays comparison generates way more assembly instructions to do the same job. This is both due to RISC nature of the ISA and the fact that alignment of the byte array is 1. x86-64 has powerful instructions to read unaligned byte ranges into XMM registers and do comparison for all bytes at once, while RISC-V assembly (at least the way it is generated by rustc for riscv64imac-unknown-none-elf) was comparing bytes one pair at a time.\nAs the result, I decided that u128 will be the address format, which might be relaxed to a pair of u64s that 64-bit to reduce alignment requirement from 16 bytes to 8 (RISC-V assembly is comparing 64-bit halves separately rather than full 128-bit value at once anyway). This, landed in PR 63, which also included some refactoring for slots management, given how large a pair of addresses (owner+contract are used to identify a slot) have become.\nBased on developer interview with Shamil I have clarified and expanded on documentation in PR 64, which I hope will make it easier to understand.\nI did some initial benchmarks with PR 61, turned out it is possible to create an environment instance and call Flipper::flip on it about four million per second on a single CPU core, which gives you a good perspective of how much overhead is happening in typical blockchain environments that can only do orders of magnitude fewer simple transactions per seconds. But after slot optimizations in PR 64 I got curious if it is possible to do better and squeezed another million calls per second in PR 65.\nFive million calls per second on a single CPU core, ~200 ns per call! I\u0026rsquo;m sure it is possible to get even lower while preserving necessary logic and overall architecture. That is basically the baseline, whatever cost above that is a waste and should be minimized. perf stats look something like this:\n1 122,47 msec task-clock:u # 1,000 CPUs utilized 0 context-switches:u # 0,000 /sec 0 cpu-migrations:u # 0,000 /sec 167 page-faults:u # 148,780 /sec 5 459 682 279 cycles:u # 4,864 GHz 74 201 852 stalled-cycles-frontend:u # 1,36% frontend cycles idle 14 406 036 797 instructions:u # 2,64 insn per cycle # 0,01 stalled cycles per insn 2 470 197 650 branches:u # 2,201 G/sec 14 684 branch-misses:u # 0,00% of all branches With storage taken care of for now, there was a small problem that bothered me for a while: inability to run tests under Miri. Writing unsafe code in Rust is more challenging than in languages like C, and there is quite a bit of unsafe code due to FFI and performance reasons in the native execution environment right now. So running under Miri was very desirable, but unfortunately not possible with inventory crate that was used to make execution environment aware of all the contracts available, so implicit use of inventory had to go away.\nI still wanted to have an ergonomic API though, and that proved to be its own challenge due to the need to register both contracts themselves and traits that they implement, but traits as such aren\u0026rsquo;t types. The best thing I came up with was to instead use dyn ContractTrait as a type, but then I discovered that associated constants just like other generics make traits not object safe. I found several discussions and summarized the conclusion with some links on Rust forum. And shared in the next post an unstable (and incomplete!) feature that allows to have associated constants in traits that are object safe, but it doesn\u0026rsquo;t look likely that it\u0026rsquo;ll be stabilized any time soon. Ultimately, I had to split associated constants into a separate trait (implemented on dyn ContractTrait) and remove : Contract bound on the ContractTrait itself, but it seemed like a price worth paying. In the end, PR 66 landed a decent API that explicitly registers contracts to be used in the native execution environment (system contracts are registered internally automatically), looks something like this:\n#[test] fn basic() { let shard_index = ShardIndex::from_u32(1).unwrap(); let mut executor = NativeExecutor::in_memory_empty(shard_index) .with_contract::\u0026lt;Flipper\u0026gt;() .build() .unwrap(); // ... } That also meant tests are finally running under Miri ðŸ˜±\nYeah, Miri wasn\u0026rsquo;t too happy initially ðŸ˜…. It took a lot more reading and some help from the Rust community to figure out why, but eventually I was able to make it work in PR 67, which also added Miri tests to CI ðŸ˜Š.\nThat was the bulk of the things I got done, with some random research and WIP stuff in a local branch that I will talk about next time. Unfortunately, there were no interviews this week, but hopefully next time!\nUpcoming plans # The next steps related to execution environment will be to add a notion of a transaction. So far it was just calling methods on contracts, but the actual blockchain will have inputs serialized into a transaction. While serialization/deserialization is already happening when doing calls from contract methods, the API that developers can use externally wasn\u0026rsquo;t that. With transaction support and more explicit slots handling (ability to provide them as input and extract afterward for persistence) the workflow will be partially complete and sufficient for further interation into a bigger system with things like transaction pool. Transaction pool, of course, doesn\u0026rsquo;t exist yet (just like most other things), but it can be fixed ðŸ˜‰.\nBased on developer feedback, I would also like to simplify contract API a bit, specifically remove #[result] and make it a special case of #[output], which will remove some code duplication in execution environment and procedural macro and will be easier to explain.\nOnce those are done, I will probably conduct more developer interviews with more people (will try to hunt down some ink! maintainers or users initially). If there is someone I should definitely talk to, let me know.\nAlso, hopefully more hiring interviews this time.\nSee you in about a week with more updates!\n","date":"21 February 2025","externalUrl":null,"permalink":"/blog/2025-02-21-5-million-flips/","section":"Blog","summary":"\u003cp\u003eIt was a challenging week working on storage access checks for slots, but it is over, and I\u0026rsquo;m quite happy with how\nthings are looking right now. Some extra refactoring also allowed running tests under Miri and spotted some things that\nviolate the Rust safety rules.\u003c/p\u003e","title":"5 million flips","type":"blog"},{"content":"Last week felt a bit less productive with a lot of time spent thinking about how to approach slots conflict resolution in the native execution environment, but still managed to land a few improvements, especially on the documentation side. Also conducted four separate interviews.\nThe fist developer interview was with my past colleague Liu-Cheng, from which I\u0026rsquo;ve collected a bunch of notes. As a result, I have updated documentation on Contracts overview page in PR 57, expanding on some topics and providing more analogies for developers to connect with. Then PR 58 renamed existing \u0026ldquo;example\u0026rdquo; contract into \u0026ldquo;playground\u0026rdquo; because it was really there to try all kinds of advanced APIs that are not used in most cases, which made it hard to understand. To make things easier to comprehend, two contracts were introduced: flipper and a fungible token.\nFipper is one of the simplest contracts possible that simply flip a boolean value stored in the state to the opposite value. It doesn\u0026rsquo;t deal with slots and is there as a gentle introduction, accompanied by an integration test.\nFungible token is more advanced, supports minting and transferring tokens with balances stored in slots. It also showcases how to work with traits by implementing Fungible trait and including examples of how to use it in integration test.\nI also looked at the API provided by the execution environment and simplified a few things prior to the second developer interview with another past colleague Shamil. Shamil had a bunch more comments about various things from documentation to APIs that will take some time to incorporate into the code, but it will make things better relatively soon.\nFor example, it became clear from the interview with Shamil that the distinction between #[result], #[output] and return type is blurry, which I agree with. It\u0026rsquo;d be nice to unify all 3 into a single concept of an \u0026ldquo;output.\u0026rdquo; The primary place where it plays a significant role is #[init] method that is expected to return initial state of the contract, but I think it is possible to simply treat the last output as the state for this purpose, regardless of whether it is a return type or an explicit argument.\nAlso default return types provided by the ab-contracts-common may not be enough regardless of how extensive the selection is and developers would want to provide a custom variants that are application-specific. This might be a bit of a challenge for metadata generation, but likely worth doing anyway.\nHuge thanks to both Liu-Cheng and Shamil for spending almost 2 hours with me and going through the very early version of something completely new at the very early stages of development. The key takeaway so far is that the whole concept of \u0026ldquo;slots\u0026rdquo; is quite different from other execution environments even if justified and will be a learning curve to people. Developers typically like working with dynamic data structures whenever possible and thinking how to model the system with (ideally) fixed size slots is a slightly hostile environment. I believe the majority of it is needed for optimal performance and code size, but there is certainly room for simplification and documentation improvements. Like with fungible token example that demonstrates that one can live without a global hashmap for balances in a fairly ergonomic way.\nAs mentioned in the last update, the check for calling #[update] or #[init] from #[view] methods on execution environment level, which was implemented in PR 59. It also included a bunch of refactoring as preparation for handling of conflicting storage updates, but the handling itself wasn\u0026rsquo;t ready.\nIn fact, I have spent quite some time thinking and trying a few approaches to efficiently handle update conflicts. After many failed attempts that quickly blew up in terms of complexity, making it hard to read, and I think I found one that should work, need to do yet another attempt to implement it ðŸ¤ž.\nSpent some time looking into RISC-V specs trying to understand how it will fit into the architecture, asked a bunch of questions at PolkaVM repository. I must admit, I do not 100% like what I understood about PolkaVM so far. Not sure what the linking cost is, but my understanding right now is that linker result is what is supposed to be used as a potential contract rather than \u0026ldquo;linking\u0026rdquo; it on the fly. The problem with that is that PolkaVM linker produces something that isn\u0026rsquo;t quite vanilla RISC-V, it has some custom instructions. This isn\u0026rsquo;t inherently bad, but I imagined a system where something like a minimal ELF file is uploaded as a contract with minimal (ideally none) API that is not the standard RISC-V instruction set produced by Rust/LLVM.\nFor example, PolkaVM uses custom ecalli instruction for host function calls instead of the standard ecall instruction (which is explicitly forbidden) due to the need to do static analysis of the program, which I do not see as a requirement for this project. I\u0026rsquo;d be ideal to maybe use the same syscalls as Linux for memory allocation and add a single one for host method calls (or find something from Linux syscalls that matches). Then it would be possible to literally take a normal static C library compiled for Linux target and use it as a contract. This is not a blocker and instead more of an exploration and education for me. I hope that there will be a way to take advantage of PolkaVM in a more generic way and collaborate with them on the project.\nLastly, I had two more researcher interviews with some really nice folks, who unfortunately (for me) are not available for full-time engagement right now. So the search continues, but I believe there will be opportunities to engage them at some point of the project.\nUpcoming plans # I\u0026rsquo;m planning to finally crack the conflicting storage updates situation, further improve update documentation and, hopefully, simply API based on received developer feedback.\nAlso, hopefully more hiring interviews.\nSee you in about a week with more updates!\n","date":"14 February 2025","externalUrl":null,"permalink":"/blog/2025-02-14-initial-developer-feedback/","section":"Blog","summary":"\u003cp\u003eLast week felt a bit less productive with a lot of time spent thinking about how to approach slots conflict resolution\nin the native execution environment, but still managed to land a few improvements, especially on the documentation side.\nAlso conducted four separate interviews.\u003c/p\u003e","title":"Initial developer feedback","type":"blog"},{"content":"After a lot of refactoring and preparation, native execution environment is finally functional and can be used for purposes like writing tests and debugging.\nWhile the previous update mentioned metadata decoding for methods, turns out full metadata decoding was necessary for native execution environment and was implemented in PR 45 with some fixes and optimizations landing in PR 46 shortly afterward. One important change there was avoiding #[slot] and #[tmp] type repetition in metadata, instead it is stored in the main contract metadata right next to the state metadata, which is part of the reason why full metadata decoding was needed at this stage.\nAfter that, PR 47 made one of the last changes to FFI interface, moving size and capacity as pointers next to the data pointer in both InternalArgs and ExternalArgs for easier processing by the host. It really made things a lot simpler to handle than before and since they are pointers now, there are no issues with alignment of fields that have different types in those data structures.\nWith that, PR 50 finally introduced a native execution environment alongside some basic tests of the example contract. PR 55 further simplified things a bit, and as of right now, a contract test looks something like this. As you can see, it supports deploying system contracts first, deploying user contracts, various calls into contracts both from outside execution environment and cross-contract calls, both directly and through trait interfaces like Fungible trait for pseudo-token (implemented by example contract). A massive step forward overall!\nI mentioned InternalArgs and ExternalArgs before, but what are they? Great question indeed! With FFI interface being a bit more stable now, I have expanded contract macro documentation with details about what code it actually generates and how it is supposed to be used in PR 52. I know it is a lot of text, but it should still be a bit easier to comprehend than trying to infer what it does and why from the macros source code (even though I tried to document it as well).\nIn the process of doing that, I was more and more annoyed by the fact that there are raw pointers in Rust that can express whether the pointer can be used for writes (*mut T vs *const T) and there is a pointer that is guaranteed to be not null (NonNull), but there is no NonNullConst and NonNullMut pair or similar. Discussion on Rust Internals indicates I\u0026rsquo;m not alone, so hopefully things will improve in the future and make FFI interfaces even better.\nOutside of code changes, there was one researcher interview and there are a few more leads that will hopefully turn into more interviews later, thanks Michelle! Two developer interviews are also planned for next week; I\u0026rsquo;ll share how those went hopefully in the next update.\nUpcoming plans # While the native execution environment is there and even supports recursive calls into other contracts, it unfortunately doesn\u0026rsquo;t perform a couple of important checks. First it doesn\u0026rsquo;t reject #[update] or #[init] calls from #[view] methods, but it really should despite generated extension traits making it impossible to compile when using high-level APIs. Second, conflicting state modifications in recursive calls are not prohibited, which are trivial to do in contracts to the first issue. I\u0026rsquo;ll work on fixing these next.\nAnd interviews I\u0026rsquo;m sure will lead to some interesting feedback to reflect on. If you know someone with smart contact or blockchain development experience that would be good to talk to, let me know.\n","date":"7 February 2025","externalUrl":null,"permalink":"/blog/2025-02-07-contracts-are-actually-running/","section":"Blog","summary":"\u003cp\u003eAfter a lot of refactoring and preparation, native execution environment is finally functional and can be used for\npurposes like writing tests and debugging.\u003c/p\u003e","title":"Contracts are actually running","type":"blog"},{"content":"Last week was busy with refactoring with the primary goal of being able to run contracts in test execution environment. The environment is not quite ready yet, but a lot of progress was done, and it\u0026rsquo;ll hopefully be ready next week.\nThe plan is to have several execution environments. Naturally, the blockchain environment will run a VM with gas metering, etc. But it is less convenient to debug, which is why test/native execution environment will be available as well that can run things exactly the same way, but with access to usual debugging and other tools. The data flow will still happen through the same FFI functions as guest environment in a VM, which means there will be as few actual code path differences as possible.\nTo improve certainly in code, especially because there is a decent amount of unsafe, some of which is auto-generated, safety is important and Miri is a great tool for this purpose. Since execution environment is generic and doesn\u0026rsquo;t really know types it deals with at compile time, I initiated a discussion on Rust forum to make sure Strict Provenance is possible even in this unusual situation.\nContract and method metadata in particular was introduced a while ago to describe to the host and to avoid higher-level tools what contract contains in terms of its FFI surface, but it was far from complete.\nPR 35 addressed function fingerprint only having a non-functional stub implementation. Function signature is somewhat similar to function selector in EVM, except it uses cryptographically secure hashing function (due to the need of being able to compute in const function const-sha1 crate is used, but I also opened a PR a few weeks ago to make blake3 work in const functions too, but it is not merged yet) and is supposed to uniquely represent signature of a method when making external method calls (call will fail in case of fingerprint mismatch).\nMetadata compaction was also implemented as part of PR 35 that strips data structures of unnecessary details. This allows implementations to change argument, field and data structure names as long as the shape of the data is exactly the same, without affecting fingerprint.\nGetting closer to execution environment implementation and striving to great developer experience, storing of function pointers was introduced in PR 40 (and improved in PR 42), which implements a global registry of all methods provided by all contracts that are being linked into the binary using linkme crate. This means that no explicit actions are necessary for developers beyond adding crate to dependencies, which they would have to do anyway to be able to deploy a contracts, access its helper methods, etc. I do not like that it is so implicit too much, but I think the context and usability win justifies it.\nTo be able to dynamically and \u0026ldquo;manually\u0026rdquo; construct data structures of correct shape, it is necessary to process metadata in various ways, which is tedious and error-prone. PR 43 introduced utilities to decode method metadata and return something that is more convenient to use for creating and reading internal (host â†’ guest) and external (guest â†’ host) data structures properly. It also verifies metadata as it processes it, rejecting invalid contents (which is another \u0026ldquo;free\u0026rdquo; test case for metadata generation).\nWith those and some more minor improvements and refactoring (which as always, you\u0026rsquo;re free to check out in individual PRs), native execution is almost here. A bit more work still remains that will have to wait until the next update before contracts are actually running for real.\nUpcoming plans # Just like last time, the plan is to work on execution environment to be able to run contracts together and have a better understanding of what it feels like as a whole. Once that is done, I\u0026rsquo;ll be doing developer interviews collecting unfiltered feedback from other developers about the system before moving much further. It is important to understand if anyone actually wants something like this or not before investing too much effort into it.\nAdditionally, interviews will start with potential candidates that might help with sharding design research, which I could use a lot of help with, especially with math.\n","date":"30 January 2025","externalUrl":null,"permalink":"/blog/2025-01-30-contracts-are-almost-running/","section":"Blog","summary":"\u003cp\u003eLast week was busy with refactoring with the primary goal of being able to run contracts in test execution environment.\nThe environment is not quite ready yet, but a lot of progress was done, and it\u0026rsquo;ll hopefully be ready next week.\u003c/p\u003e","title":"Contracts are almost running","type":"blog"},{"content":"Last week was busy on various improvements for contracts infrastructure, trying to clarify existing API and ensuring everything that might be built is actually possible. First system contracts were introduced, trait support was added and more, below is a recap of key updates.\nEnvironment improvements changed the way calls into extension traits are made, statically guaranteeing that #[view] methods can\u0026rsquo;t recursively call methods that potentially #[update] slot contents, this is because #[view] methods are supposed to not modify anything and be callable from non-block context, so it\u0026rsquo;d be strange for them to be able to access API that is capable of altering persistent data. Now this is expressed on type system level.\n#[tmp] arguments were introduced as a bit of ephemeral state that only lives for a duration of a single transaction processing. This will allow to, for example, approve transfer of a specific amount of a specific token just for the duration of a single transaction and nothing else, which makes it possible to make the least privileged contract calls instead of allowing to do everything on behalf of the user by default. Also, safe contructors for VariableBytes and MaybeData were added that make calls into smart contracts more convenient.\nContacts overview (rendered) was added to the book with some diagrams detailing how storage is organized and \u0026ldquo;Everything is a contract\u0026rdquo; approach that is being tested right now. This was followed up in PR 19 and PR 27 with introduction of system contracts ab-system-contract-address-allocator, ab-system-contract-code and ab-system-contract-state that implement some core fundamental capabilities. This means that in contrast to older revisions of the code base \u0026ldquo;code\u0026rdquo; and \u0026ldquo;state\u0026rdquo; are no longer separate types of storage, they are just slots stored by system contracts, which happens to be known by the host so it can read/write those when needed. Address allocator is the contract which allocates addresses to contracts that are about to be deployed and is used by code contract during deployment of new contracts. There is still a lot more work left around system contracts, but so far the concept \u0026quot; Everything is a contract\u0026quot; seems to be working reasonably well.\nThere were some metadata improvements (PR 20, PR 21, PR 22) that massaged metadata information about the contract and its methods, followed by more #[contract] macro refactoring (PR 23, PR 24) that finally led to trait support being implemented. Trait support allows to define things like fungible token as a trait and for other contract to implement it. Then contracts can rely on just trait definition to be able to interact with any contract that has implemented that trait. Simple Fungible trait was added to the codebase just to demonstrate how it could work. More traits will be added over time, for example it is likely that some kind of \u0026ldquo;Wallet\u0026rdquo; trait will be defined to unify interation with contracts from the user side.\nThere were some smaller changes here and there as well, but if you\u0026rsquo;re interested in that you better go read numerous PRs directly instead.\nUpcoming plans # The next steps will involve implementing some kind of test environment for contract execution, such that it is possible to combine a couple of contracts, deploy them and see them interacting with each other. This will be an important milestone in showcasing developer experience and will hopefully help to collect some developer feedback.\n","date":"21 January 2025","externalUrl":null,"permalink":"/blog/2025-01-21-system-contracts-trait-support-and-more/","section":"Blog","summary":"\u003cp\u003eLast week was busy on various improvements for contracts infrastructure, trying to clarify existing API and ensuring\neverything that might be built is actually possible. First system contracts were introduced, trait support was added and\nmore, below is a recap of key updates.\u003c/p\u003e","title":"System contracts, trait support and more","type":"blog"},{"content":"Hello, world ðŸ‘‹!\nThis is the beginning of hopefully successful thing I call \u0026ldquo;Project Abundance\u0026rdquo;.\nAfter writing initial set of constraints and thinking about it for quite a while, it is finally time to dedicate all my time to it and see where it leads us.\nDuring last couple of weeks new repository was created with some initial CI infrastructure for building this website, a book and Rust docs of the code.\nA book was started that tries to provide some details about the project and will accumulate many technical details over time.\nSmart contracts initial dump was merged, which contains some infrastructure for building smart contracts that would run in (currently) imaginary execution environment. It is just an early prototype, but should give an idea of what things might look and feel.\nI have learned way too much about working writing Rust for const environment, about procedural macros and various static site generators.\nNext I\u0026rsquo;ll be thinking about how to add a notion of capabilities to smart contracts (imagine allowing to withdraw certain amount of certain token only) and documenting the architecture of execution environment with diagrams.\n","date":"13 January 2025","externalUrl":null,"permalink":"/blog/2025-01-13-welcome/","section":"Blog","summary":"\u003cp\u003eHello, world ðŸ‘‹!\u003c/p\u003e\n\u003cp\u003eThis is the beginning of hopefully successful thing I call \u0026ldquo;Project Abundance\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eAfter \u003ca\n  href=\"https://gist.github.com/nazar-pc/760505c5ad7d56c20b2c75c1484e672f\"\n    target=\"_blank\"\n  \u003ewriting initial set of constraints\u003c/a\u003e and thinking about it for quite a while, it is finally time to dedicate all\nmy time to it and see where it leads us.\u003c/p\u003e","title":"Welcome!","type":"blog"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"categories","summary":"","title":"categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"series","summary":"","title":"series","type":"series"}]